**Appication on a large Vision-Language Models**

CLIP_collect_acts.ipynb: Contains code to extract activations from CLIP's VIT encoder. Inputs are images of truck and cars respectively. The labelling is done based on the zero-shot classification of CLIP.

ProphecyTool_with_CLIP.ipynb: Demonstrates application of Prophecy to extract rules using the fingerprints and zero-shot classification labels of a CLIP model trained on the RIVAL-10 dataset. 
