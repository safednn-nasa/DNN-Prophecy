{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg') # If we don't want saved images printed on output\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "\n",
    "from Models.loss import smoothL1\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Input, InputLayer\n",
    "from tensorflow.keras.layers import Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "#from tensorflow.compat.v1 import ConfigProto\n",
    "#from tensorflow.compat.v1 import InteractiveSession\n",
    "#config = ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "#session = InteractiveSession(config=config)\n",
    "\n",
    "import math\n",
    "import io\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "from sklearn import tree\n",
    "from tqdm import tqdm\n",
    "import operator\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "import glob\n",
    "\n",
    "#from vis.visualization import visualize_cam\n",
    "#import cv2\n",
    "#FIXIT Needs old Scipy version (1.1.0), need to check integration with Conda and Pip\n",
    "\n",
    "#import tensorflow as tf\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 GPUs\n"
     ]
    }
   ],
   "source": [
    "from tf_keras_vis.utils import num_of_gpus\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tf_keras_vis.saliency import Saliency\n",
    "from tf_keras_vis.utils import normalize\n",
    "\n",
    "from matplotlib import cm\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.gradcam import GradcamPlusPlus\n",
    "\n",
    "_, gpus = num_of_gpus()\n",
    "print('{} GPUs'.format(gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Augmentation System\n",
    "By: Tyler Staudinger\n",
    "Using CPU based augmentation from a dataset loaded into memory incurs about 10% additional per epoch \n",
    "processing time\n",
    "Copyright 2018 The Boeing Company\n",
    "\"\"\"\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "#Apply various augmentations from imgaug\n",
    "def blur():\n",
    "    blurer=[iaa.GaussianBlur((0, 3.0)),iaa.AverageBlur(k=(2, 7)),iaa.MedianBlur(k=(3, 11))]\n",
    "    return blurer[np.random.randint(0,3)]\n",
    "\n",
    "#Dropout may not be appropriate, so its not currently used\n",
    "def dropout():\n",
    "    dropper=[iaa.Dropout((0.01, 0.1), per_channel=0.5), iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2)]\n",
    "    return dropper[np.random.randint(0,2)]\n",
    "\n",
    "def noise():\n",
    "    return iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5)\n",
    "\n",
    "def contrast():\n",
    "    return iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5)\n",
    "\n",
    "def greyscale():\n",
    "    return iaa.Grayscale(alpha=(0.0, 1.0))\n",
    "\n",
    "def invert():\n",
    "    return iaa.Invert(0.05, per_channel=True)\n",
    "\n",
    "def hue():\n",
    "    return iaa.AddToHueAndSaturation((-20, 20))\n",
    "\n",
    "def add(): \n",
    "    return iaa.Add((-10, 10), per_channel=0.5)\n",
    "\n",
    "def multiply():\n",
    "    return iaa.Multiply((0.5, 1.5), per_channel=0.5)\n",
    "\n",
    "def sharpen(): \n",
    "    return iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5))\n",
    "\n",
    "def emboss(): \n",
    "    return iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0))\n",
    "   \n",
    "def translate():\n",
    "    return iaa.Affine(translate_percent={\"x\": (0.0, 0.0), \"y\": (-0.03, 0.03)},mode='edge')\n",
    "    \n",
    "def rotate():\n",
    "    return iaa.Affine(rotate=(-3, 3),mode='edge')    \n",
    "\n",
    "   \n",
    "def augment(imgs,max_augs,affine=False,debug=False):\n",
    "    \"\"\"Applys a series of augmentations to an image\n",
    "    # Arguments\n",
    "        imgs: the input images\n",
    "        max_augs: the maximum number of augmentations to apply\n",
    "    # Returns\n",
    "        Augmented images\n",
    "    \"\"\"\n",
    "    augmented=[]\n",
    "    for img in imgs:\n",
    "        if affine:\n",
    "            affine_augs=[translate(),rotate()]            \n",
    "            seq_affine=iaa.Sequential(affine_augs)\n",
    "            img=seq_affine.augment_image(img)\n",
    "        #Choose a number of augmentations to use\n",
    "        num_aug=np.random.randint(0,max_augs+1)\n",
    "        #set of augmentations to use\n",
    "        augs=[blur(),noise(),contrast(),greyscale(),invert(),hue(),add(),multiply(),sharpen(),emboss()]\n",
    "        #apply augmentations\n",
    "        seq=iaa.Sequential(list(np.random.choice(augs,num_aug)))\n",
    "        aug_img=seq.augment_image(img)\n",
    "        \n",
    "        #For Debugging of augmentations\n",
    "        if debug:\n",
    "            cv2.imshow('frame',aug_img)\n",
    "            cv2.waitKey(1000)\n",
    "        augmented.append(aug_img)\n",
    "        \n",
    "    return np.asarray(augmented)\n",
    "    \n",
    "    \n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y, batch_size, noaugs=False, num_aug=5, affine=False, height=None, width=None): \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.noaugs     = noaugs\n",
    "        self.num_aug    = num_aug\n",
    "        self.affine     = affine\n",
    "        self.height     = height\n",
    "        self.width      = width\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #indecies=np.random.randint(0,self.X.shape[0],self.batch_size)\n",
    "            \n",
    "        if self.noaugs:\n",
    "            images=self.X[idx * self.batch_size:(idx + 1) *self.batch_size]/255.0\n",
    "            target=self.y[idx * self.batch_size:(idx + 1) *self.batch_size]\n",
    "            #Normalize cte with max magnitude of 8 meters\n",
    "            target[:,0]/=8.0 #16.0\n",
    "            #Normalize heading with max magnitude of 35 degrees\n",
    "            target[:,1]/=35.0 #30.0\n",
    "            return images,target\n",
    "        imgs=augment(self.X[idx * self.batch_size:(idx + 1) *self.batch_size],self.num_aug,self.affine)/255.0\n",
    "        \n",
    "        target=self.y[idx * self.batch_size:(idx + 1) *self.batch_size]\n",
    "        #Normalize cte with max magnitude of 16 meters\n",
    "        target[:,0]/=8.0\n",
    "        #Normalize heading with max magnitude of 35 degrees\n",
    "        target[:,1]/=35.0\n",
    "        return imgs,target\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.X) / self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER = 4\n",
    "if LAYER == 1:\n",
    "    layer = 'dense_1'\n",
    "elif LAYER == 2:\n",
    "    layer = 'dense_2'\n",
    "elif LAYER == 3:\n",
    "    layer = 'dense_3'\n",
    "else:\n",
    "    layer = 'ALL'\n",
    "    \n",
    "PLOT_DATA_BOEING = False\n",
    "PLOT_DATA_KJ = False\n",
    "\n",
    "BOEING_TRAIN = True\n",
    "BOEING_TEST = False\n",
    "\n",
    "MODEL = 'SimpleModel'\n",
    "#MODEL = 'FullyConnectedModel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (13885, 200, 360, 3)\n",
      "Shape of X test: (2777, 200, 360, 3)\n"
     ]
    }
   ],
   "source": [
    "## Loading the data\n",
    "eval_folder = '../20200630_TaxiNet_data/'\n",
    "eval_folder_kjtrain = '/home/dgopinat/Work/KJ-TaxinetArtifacts/data_train/'\n",
    "eval_folder_kjtest = '/home/dgopinat/Work/KJ-TaxinetArtifacts/data_val/'\n",
    "\n",
    "#Loading the data\n",
    "if BOEING_TRAIN:\n",
    "    x=np.load(eval_folder+'X_train.npy')\n",
    "    y=np.load(eval_folder+'Y_train.npy')\n",
    "    x_test=np.load(eval_folder+'X_test.npy')\n",
    "    y_test=np.load(eval_folder+'Y_test.npy')\n",
    "\n",
    "elif BOEING_TEST:\n",
    "    x=np.load(eval_folder+'X_small_test.npy')\n",
    "    y=np.load(eval_folder+'Y_small_test.npy')\n",
    "    x_train = np.load(eval_folder+'X_train_upd_lay1.npy')\n",
    "    y_train = np.load(eval_folder+'Y_train_upd_lay1.npy')\n",
    "    print('X TRAIN SHAPE:', x_train.shape)\n",
    "    print('Y TRAIN SHAPE:', y_train.shape)\n",
    "\n",
    "print(\"Shape of X:\", x.shape)\n",
    "print(\"Shape of X test:\", x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the ORIG model and printing the summary:\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 98, 178, 24)       1800      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 98, 178, 24)       96        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 98, 178, 24)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 47, 87, 36)        21600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 47, 87, 36)        144       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 47, 87, 36)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 22, 42, 48)        43200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 22, 42, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 22, 42, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 40, 64)        27648     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 20, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 20, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 18, 38, 64)        36864     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 18, 38, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 18, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 43776)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 43776)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               4377700   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 4,515,338\n",
      "Trainable params: 4,514,866\n",
      "Non-trainable params: 472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Loading the model\n",
    "if MODEL == 'SimpleModel':\n",
    "    model_path = './Training_Runs/TaxiNet-Simple/SimpleModel.h5'\n",
    "elif MODEL == 'MobileNetModel':\n",
    "    model_path = './Training_Runs/TaxiNet-MobileNet/MobileNetV2.h5'\n",
    "else:\n",
    "    model_path = './Training_Runs/TaxiNet-FullyConnected/FullyConnectedModel.h5'\n",
    "\n",
    "print('Loading the ORIG model and printing the summary:')\n",
    "model=load_model(model_path,custom_objects={'smoothL1':smoothL1}) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299, 5)\n",
      "18.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "delimiter = ','\n",
    "try:\n",
    "        data = np.genfromtxt('Boeing_Taxinet_annotations.txt', delimiter=delimiter)#, dtype='|S128')\n",
    "except:\n",
    "        import pandas\n",
    "        data = pandas.read_csv('Boeing_Taxinet_annotations.txt',\n",
    "                               header=None,\n",
    "                               delimiter=delimiter,\n",
    "                               na_filter=True,\n",
    "                               dtype=str).fillna(fill_na).values\n",
    "        \n",
    "print(data.shape)\n",
    "dataset = list(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Inputs with Boeing-TaxiNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ORIGINAL MODEL\n",
      "Cross Track Error\n",
      "MeanAbsoluteError: 0.36629044055199295\n",
      "MedianAbsoluteError: 0.2837369384839286\n",
      "StandardDeviation: 0.32818068833292025\n",
      "MeanInferenceTime: 60.7564324858878\n",
      "MaxAbsoluteError: 3.8953489884122146\n",
      "Heading Error\n",
      "MeanAbsoluteError: 1.6450901653917946\n",
      "MedianAbsoluteError: 1.4151208355401792\n",
      "StandardDeviation: 1.270872181246601\n",
      "MaxAbsoluteError: 9.824763055083636\n"
     ]
    }
   ],
   "source": [
    "outputs_test   = []\n",
    "inference_time = []\n",
    "\n",
    "for img in x_test:\n",
    "    t=time.time()\n",
    "    outputs_test.append(model.predict(np.expand_dims(img/255.0,axis=0)))\n",
    "    inference_time.append((1/(time.time()-t)))\n",
    "\n",
    "outputs_test=np.squeeze(np.asarray(outputs_test))\n",
    "inference_time=np.squeeze(np.asarray(inference_time))\n",
    "\n",
    "#Scale the predictions\n",
    "outputs_cte=outputs_test[:,0]*8.0\n",
    "outputs_heading=outputs_test[:,1]*35.0\n",
    "\n",
    "error_test=np.squeeze(y_test[:,0])-outputs_cte\n",
    "    \n",
    "print(\" ORIGINAL MODEL\")\n",
    "print(\"Cross Track Error\")\n",
    "abs_error_test=np.abs(error_test)\n",
    "print('MeanAbsoluteError: '+str(np.mean(abs_error_test)))\n",
    "print('MedianAbsoluteError: '+str(np.median(abs_error_test)))\n",
    "print('StandardDeviation: '+str(np.std(abs_error_test)))\n",
    "print('MeanInferenceTime: '+str(np.mean(inference_time)))\n",
    "print('MaxAbsoluteError: '+str(np.max(abs_error_test)))\n",
    "\n",
    "error_test=np.squeeze(y_test[:,1])-outputs_heading\n",
    "    \n",
    "print(\"Heading Error\")\n",
    "#Print error metrics\n",
    "abs_error_test=np.abs(error_test)\n",
    "print('MeanAbsoluteError: '+str(np.mean(abs_error_test)))\n",
    "print('MedianAbsoluteError: '+str(np.median(abs_error_test)))\n",
    "print('StandardDeviation: '+str(np.std(abs_error_test)))\n",
    "print('MaxAbsoluteError: '+str(np.max(abs_error_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the actual and ideal outputs \n",
    "# When Only Feature = True, all inputs within Threshold 1 and Thereshold 2 are labelled '1', '0' otherwise\n",
    "# When Only Prop = True, inputs are labelled '1' if they satisfy the correctness constraints, '0' otherwise\n",
    "# When Both Feature and Prop are True, inputs which are not within intersection and sat correct constraint -> label 0, \n",
    "# inputs which are within intersection and sat correct constraint -> label 1, inputs which are not within intersection \n",
    "# and dont sat correct constraint -> label 2, and inputs which are within intersection and dont sat correct constraint -> label3\n",
    "\n",
    "def get_prediction(inputs, tensor = None, LAB=y_test, Feature=False, Prop=False,TEST=False,THRESHOLD1=None,THRESHOLD2=None):\n",
    "    if ((Feature == False) and (Prop==True)):\n",
    "        if tensor == \"ALL\":\n",
    "            full_list = []\n",
    "            for tensor in ['dense_1', 'dense_2', 'dense_3']:\n",
    "                layer_name = tensor\n",
    "                layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "                layer_outputs_test = []\n",
    "                for img in inputs:\n",
    "                    layer_result = (layer_model.predict(np.expand_dims(img/255.0,axis=0))>0).astype('int')\n",
    "                    #layer_output = layer_model.predict(np.expand_dims(img/255.0,axis=0))[0]\n",
    "                    layer_outputs_test.append(layer_result[0])\n",
    "\n",
    "                full_list.append(layer_outputs_test)\n",
    "\n",
    "            new_list = [list(full_list[0][i])+list(full_list[1][i])+list(full_list[2][i]) for i in range(len(inputs))]\n",
    "\n",
    "            layer_outputs_test = np.asarray(new_list)\n",
    "            return layer_outputs_test\n",
    "\n",
    "        if tensor is None or tensor == 'dense_4':\n",
    "            outputs_test = []\n",
    "\n",
    "            for img in inputs:\n",
    "                outputs_test.append(model.predict(np.expand_dims(img/255.0,axis=0)))\n",
    "\n",
    "            outputs_test    = np.squeeze(np.asarray(outputs_test))\n",
    "\n",
    "            #Scale the predictions\n",
    "            outputs_cte     = outputs_test[:,0]*8.0\n",
    "            outputs_heading = outputs_test[:,1]*35.0\n",
    "\n",
    "             #Absolute value of the predictions\n",
    "            abs_outputs_cte = np.abs(outputs_cte)\n",
    "            abs_outputs_heading = np.abs(outputs_heading)\n",
    "\n",
    "            print(np.shape(inputs))\n",
    "            print(np.shape(LAB))\n",
    "            print(np.shape(outputs_cte))\n",
    "           \n",
    "            #Absolute Error between Ground Truth and Prediction for Cross Track Error\n",
    "            if (TEST == False):\n",
    "                error_cte=np.squeeze(y[:,0])-outputs_cte\n",
    "            else:\n",
    "                error_cte=np.squeeze(LAB[:,0])-outputs_cte\n",
    "\n",
    "            abs_error_cte=np.abs(error_cte)\n",
    "\n",
    "            #Absolute Error between Ground Truth and Prediction for Heading Error\n",
    "            if (TEST == False):\n",
    "                error_heading     = np.squeeze(y[:,1])-outputs_heading\n",
    "            else:\n",
    "                error_heading     = np.squeeze(LAB[:,1])-outputs_heading\n",
    "\n",
    "            abs_error_heading = np.abs(error_heading)\n",
    "\n",
    "            ret = []\n",
    "            for i in range(0,len(abs_outputs_cte)):\n",
    "                #if (abs_outputs_cte[i] <= 10.0 and abs_error_cte[i] <=1.0):# TEST\n",
    "                #if (abs_error_cte[i] < 0.4): # TEST\n",
    "                if ((abs_error_cte[i] < 0.5) and (abs_error_heading[i] < 2.0)): \n",
    "               # if (abs_error_heading[i] <= 5.0): \n",
    "                    ret.append(1)\n",
    "                else:\n",
    "                    ret.append(0)\n",
    "\n",
    "            return abs_outputs_cte, ret\n",
    "            #return (abs_error_heading<=10.0).astype('int')\n",
    "            #return (abs_error_cte<=1.5).astype('int')\n",
    "        else:\n",
    "            layer_name = tensor\n",
    "            layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "            layer_outputs_test = []\n",
    "            for img in inputs:\n",
    "                layer_result = (layer_model.predict(np.expand_dims(img/255.0,axis=0))>0).astype('int')\n",
    "                #layer_output = layer_model.predict(np.expand_dims(img/255.0,axis=0))[0]\n",
    "                layer_outputs_test.append(layer_result[0])\n",
    "\n",
    "            layer_outputs_test = np.asarray(layer_outputs_test)\n",
    "\n",
    "            #Scale the predictions        \n",
    "            return layer_outputs_test\n",
    "        \n",
    "    if ((Feature == True) and (Prop==False)):\n",
    "        \n",
    "        if tensor is None or tensor == 'dense_4':\n",
    "            \n",
    "            outputs_test = []\n",
    "\n",
    "            for img in inputs:\n",
    "                outputs_test.append(model.predict(np.expand_dims(img/255.0,axis=0)))\n",
    "\n",
    "            outputs_test    = np.squeeze(np.asarray(outputs_test))\n",
    "\n",
    "            #Scale the predictions\n",
    "            outputs_cte     = outputs_test[:,0]*8.0\n",
    "            outputs_heading = outputs_test[:,1]*35.0\n",
    "\n",
    "             #Absolute value of the predictions\n",
    "            abs_outputs_cte = np.abs(outputs_cte)\n",
    "            abs_outputs_heading = np.abs(outputs_heading)\n",
    "            \n",
    "            print(np.shape(inputs))\n",
    "            print(np.shape(LAB))\n",
    "            print(np.shape(outputs_cte))\n",
    "           \n",
    "\n",
    "            #Absolute Error between Ground Truth and Prediction for Cross Track Error\n",
    "            if (TEST == False):\n",
    "                error_cte=np.squeeze(y[:,0])-outputs_cte\n",
    "            else:\n",
    "                error_cte=np.squeeze(LAB[:,0])-outputs_cte\n",
    "\n",
    "            abs_error_cte=np.abs(error_cte)\n",
    "\n",
    "            #Absolute Error between Ground Truth and Prediction for Heading Error\n",
    "            if (TEST == False):\n",
    "                error_heading     = np.squeeze(y[:,1])-outputs_heading\n",
    "            else:\n",
    "                error_heading     = np.squeeze(LAB[:,1])-outputs_heading\n",
    "\n",
    "            abs_error_heading = np.abs(error_heading)\n",
    "\n",
    "            ret = []\n",
    "            for indx in range(0,len(abs_outputs_cte)):\n",
    "                if (indx <= THRESHOLD1) or (indx > THRESHOLD2):\n",
    "                    ret.append(0)\n",
    "                else:\n",
    "                    ret.append(1)\n",
    "\n",
    "            return abs_outputs_cte, ret\n",
    "        \n",
    "        \n",
    "    if ((Feature == True) and (Prop==True)):\n",
    "\n",
    "        if tensor is None or tensor == 'dense_4':\n",
    "         \n",
    "            outputs_test = []\n",
    "\n",
    "            for img in inputs:\n",
    "                outputs_test.append(model.predict(np.expand_dims(img/255.0,axis=0)))\n",
    "\n",
    "            outputs_test    = np.squeeze(np.asarray(outputs_test))\n",
    "\n",
    "            #Scale the predictions\n",
    "            outputs_cte     = outputs_test[:,0]*8.0\n",
    "            outputs_heading = outputs_test[:,1]*35.0\n",
    "\n",
    "             #Absolute value of the predictions\n",
    "            abs_outputs_cte = np.abs(outputs_cte)\n",
    "            abs_outputs_heading = np.abs(outputs_heading)\n",
    "\n",
    "            print(np.shape(inputs))\n",
    "            print(np.shape(LAB))\n",
    "            print(np.shape(outputs_cte))\n",
    "            \n",
    "            #Absolute Error between Ground Truth and Prediction for Cross Track Error\n",
    "            if (TEST == False):\n",
    "                error_cte=np.squeeze(y[:,0])-outputs_cte\n",
    "            else:\n",
    "                error_cte=np.squeeze(LAB[:,0])-outputs_cte\n",
    "\n",
    "            abs_error_cte=np.abs(error_cte)\n",
    "            \n",
    "            \n",
    "\n",
    "            #Absolute Error between Ground Truth and Prediction for Heading Error\n",
    "            if (TEST == False):\n",
    "                error_heading     = np.squeeze(y[:,1])-outputs_heading\n",
    "            else:\n",
    "                error_heading     = np.squeeze(LAB[:,1])-outputs_heading\n",
    "\n",
    "            abs_error_heading = np.abs(error_heading)\n",
    "\n",
    "            ret = []\n",
    "            for indx in range(0,len(abs_outputs_cte)):\n",
    "                #if (abs_outputs_cte[i] <= 10.0 and abs_error_cte[i] <=1.0):# TEST\n",
    "                #if (abs_error_cte[i] < 0.4): # TEST\n",
    "                if ((abs_error_cte[indx] <= 0.5) and (abs_error_heading[indx] <= 2.0)): \n",
    "               # if (abs_error_cte[indx] <= 5.0): \n",
    "                    if ((indx <= THRESHOLD1) or (indx > THRESHOLD2)):\n",
    "                            ret.append(0)\n",
    "                    else:\n",
    "                            ret.append(1)\n",
    "                else:\n",
    "                    if ((indx <= THRESHOLD1) or (indx > THRESHOLD2)):\n",
    "                            ret.append(2)\n",
    "                    else:\n",
    "                            ret.append(3)\n",
    "\n",
    "\n",
    "            return abs_outputs_cte, ret\n",
    "         \n",
    "\n",
    "def fingerprint_suffix(inps,ten):\n",
    "    return (get_prediction(inps, tensor=ten)>0.0).astype('int')\n",
    "\n",
    "\n",
    "\n",
    "def get_prediction_vals(inputs, tensor = None, TEST=False):\n",
    "    \n",
    "    if tensor == \"ALL\":\n",
    "        full_list = []\n",
    "        for tensor in ['dense_1', 'dense_2', 'dense_3']:\n",
    "            layer_name = tensor\n",
    "            layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "            layer_outputs_test = []\n",
    "            for img in inputs:\n",
    "                layer_result = (layer_model.predict(np.expand_dims(img/255.0,axis=0)))\n",
    "                layer_outputs_test.append(layer_result[0])\n",
    "\n",
    "            full_list.append(layer_outputs_test)\n",
    "        \n",
    "        new_list = [list(full_list[0][i])+list(full_list[1][i])+list(full_list[2][i]) for i in range(len(inputs))]\n",
    "            \n",
    "        layer_outputs_test = np.asarray(new_list)\n",
    "        return layer_outputs_test\n",
    "    \n",
    "    if tensor is None or tensor == 'dense_4':\n",
    "        outputs_test = []\n",
    "        \n",
    "        for img in inputs:\n",
    "            outputs_test.append(model.predict(np.expand_dims(img/255.0,axis=0)))\n",
    "\n",
    "        outputs_test    = np.squeeze(np.asarray(outputs_test))\n",
    "\n",
    "        #Scale the predictions\n",
    "        outputs_cte     = outputs_test[:,0]*8.0\n",
    "        outputs_heading = outputs_test[:,1]*35.0\n",
    "        \n",
    "         #Absolute value of the predictions\n",
    "        abs_outputs_cte = np.abs(outputs_cte)\n",
    "        abs_outputs_heading = np.abs(outputs_heading)\n",
    "        \n",
    "        #Absolute Error between Ground Truth and Prediction for Cross Track Error\n",
    "        if (TEST == False):\n",
    "            error_cte=np.squeeze(y[:,0])-outputs_cte\n",
    "        else:\n",
    "            error_cte=np.squeeze(y_test[:,0])-outputs_cte\n",
    "            \n",
    "        abs_error_cte=np.abs(error_cte)\n",
    "\n",
    "        #Absolute Error between Ground Truth and Prediction for Heading Error\n",
    "        if (TEST == False):\n",
    "            error_heading     = np.squeeze(y[:,1])-outputs_heading\n",
    "        else:\n",
    "            error_heading     = np.squeeze(y_test[:,1])-outputs_heading\n",
    "            \n",
    "        abs_error_heading = np.abs(error_heading)\n",
    "        \n",
    "        ret = []\n",
    "        for i in range(0,len(abs_outputs_cte)):\n",
    "            #if (abs_outputs_cte[i] <= 10.0 and abs_error_cte[i] <=1.0):# TEST\n",
    "            #if (abs_error_cte[i] < 0.4): # TEST\n",
    "           # if ((abs_error_cte[i] <= 1.0)):# and (abs_error_heading[i] <= 5.0)): \n",
    "            if (abs_error_heading[i] <= 5.0): \n",
    "                ret.append(1)\n",
    "            else:\n",
    "                ret.append(0)\n",
    "                \n",
    "        return abs_outputs_cte, ret\n",
    "        #return (abs_error_heading<=10.0).astype('int')\n",
    "        #return (abs_error_cte<=1.5).astype('int')\n",
    "    else:\n",
    "        layer_name = tensor\n",
    "        layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "        layer_outputs_test = []\n",
    "        for img in inputs:\n",
    "            layer_result = (layer_model.predict(np.expand_dims(img/255.0,axis=0)))\n",
    "            layer_outputs_test.append(layer_result[0])\n",
    "\n",
    "        layer_outputs_test = np.asarray(layer_outputs_test)\n",
    "\n",
    "        #Scale the predictions        \n",
    "        return layer_outputs_test\n",
    "\n",
    "def fingerprint_suffix_vals(inps,ten):\n",
    "    return (get_prediction_vals(inps, tensor=ten))\n",
    "\n",
    "def fingerprint_signature(inps, layer_name = 'dense_1'):\n",
    "    layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "    layer_result = (layer_model.predict(np.expand_dims(img/255.0,axis=0))>0).astype('int')[0]\n",
    "    return layer_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `output` variable refer to the output of the model,\n",
    "# so, in this case, `output` shape is `(1, 2)` i.e., (samples, classes).\n",
    "\n",
    "#Loss and Model Modifier for middle layers\n",
    "def loss_gen_sum(node_list):\n",
    "    def loss(output):\n",
    "        loss_val = sum([output[0][i] for i in node_list])/len(node_list)\n",
    "        return loss_val\n",
    "    return loss\n",
    "\n",
    "def loss_gen_avg_sep(node):\n",
    "    def loss(output):\n",
    "        loss_val = output[0][node]\n",
    "        return loss_val\n",
    "    return loss\n",
    "\n",
    "def loss_gen_separate(node_list):\n",
    "    def loss(output):\n",
    "        loss_val = tuple([output[ind][i] for ind,i in enumerate(node_list)])\n",
    "        return loss_val\n",
    "    return loss\n",
    "\n",
    "def model_modifier_mid_layer(current_model):\n",
    "    target_layer = current_model.get_layer(name=layer)\n",
    "    new_model = tf.keras.Model(inputs=current_model.inputs,\n",
    "                               outputs=target_layer.output)\n",
    "    new_model.layers[-1].activation = tf.keras.activations.linear\n",
    "    return new_model\n",
    "\n",
    "def model_modifier_last_layer(m):\n",
    "    m.layers[-1].activation = tf.keras.activations.linear\n",
    "    return m\n",
    "\n",
    "def loss_crosstrack(output):\n",
    "    return (output[0][0])\n",
    "\n",
    "def loss_heading(output):\n",
    "    return (output[0][1])\n",
    "\n",
    "def no_loss(output):\n",
    "    return (output[0][0] * 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FONT_PATH  = '/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed.ttf'\n",
    "IMAGE_SIZE = 28\n",
    "\n",
    "def mnist_to_rgb(mnist_img):\n",
    "  \"\"\"\n",
    "  Transformsn an MNIST image (shape: <784>) to a grayscale\n",
    "  RGB image (shape: <28,28,3>)\n",
    "  \"\"\"\n",
    "  return mnist_img\n",
    "  #pixel_array = mnist_img.reshape(200, 360, 3)  # shape: 28,28\n",
    "  #rgb_image = np.transpose([pixel_array,pixel_array,pixel_array], axes=[1,2,0])\n",
    "  #return rgb_image\n",
    "\n",
    "def pil_img(a):\n",
    "  '''Returns a PIL image created from the provided RGB array.\n",
    "  '''\n",
    "  a = np.uint8(a)\n",
    "  return PIL.Image.fromarray(a)\n",
    "\n",
    "def mnist_to_pil_img(inp):\n",
    "  rgb_inp = 255*inp#255*mnist_to_rgb(inp)\n",
    "  vis_inp = pil_img(rgb_inp)\n",
    "  return vis_inp  \n",
    "\n",
    "def pil_fig(fig):\n",
    "  # Returns a PIL image obtained from the provided PLT figure.\n",
    "  buf = io.BytesIO()\n",
    "  fig.savefig(buf, format='png')\n",
    "  plt.close(fig)\n",
    "  buf.seek(0)\n",
    "  img = PIL.Image.open(buf)\n",
    "  return img\n",
    "\n",
    "def show_img(img, fmt='jpeg'):\n",
    "  '''Displays the provided PIL image\n",
    "  '''\n",
    "  #f = StringIO()\n",
    "  f = BytesIO()\n",
    "  img.save(f, fmt)\n",
    "  display(Image(data=f.getvalue()))\n",
    " \n",
    "def show_mnist_img(mnist_img):\n",
    "  show_img(pil_img(255*mnist_to_rgb(mnist_img)))\n",
    "  \n",
    "def gray_scale(img):\n",
    "  '''Converts the provided RGB image to gray scale.\n",
    "  '''\n",
    "  img = np.average(img, axis=2)\n",
    "  return np.transpose([img, img, img], axes=[1,2,0])\n",
    "\n",
    "def normalize(attrs, ptile=99):\n",
    "  '''Normalize the provided attributions so that they fall between\n",
    "     -1.0 and 1.0.\n",
    "  '''\n",
    "  h = np.percentile(attrs, ptile)\n",
    "  l = np.percentile(attrs, 100-ptile)\n",
    "  if (max(abs(h), abs(l))) == 0.0:\n",
    "    return np.clip(attrs/1.0, -1.0, 1.0)  \n",
    "  else:\n",
    "    return np.clip(attrs/max(abs(h), abs(l)), -1.0, 1.0)  \n",
    "\n",
    "\n",
    "def pil_text(strs, shape, start_h=10, start_w=10, font_size=18, color=(0, 0, 0)):\n",
    "  # Returns a PIL image with the provided text.\n",
    "  img = pil_img(255*np.ones(shape))\n",
    "  draw = PIL.ImageDraw.Draw(img)\n",
    "  font = PIL.ImageFont.truetype(FONT_PATH, font_size)\n",
    "  h = start_h\n",
    "  for s in strs: \n",
    "    draw.text((start_w,h), s, fill=color, font=font)\n",
    "    h = h + 30\n",
    "  return img\n",
    "\n",
    "def combine(imgs, horizontal=True):\n",
    "  # Combines the provided PIL Images horizontally or veritically\n",
    "  if horizontal:\n",
    "    w = np.sum([img.size[0]+10 for img in imgs])\n",
    "    h = np.max([img.size[1] for img in imgs])\n",
    "  else:\n",
    "    w = np.max([img.size[0] for img in imgs])\n",
    "    h = np.sum([img.size[1]+10 for img in imgs])\n",
    "  final_img = PIL.Image.new('RGB', (w, h), color='white')\n",
    "  pos = 0\n",
    "  for img in imgs:\n",
    "    if horizontal:\n",
    "      final_img.paste(im=img, box=(pos,0))\n",
    "      pos = pos+img.size[0]+10\n",
    "    else:\n",
    "      final_img.paste(im=img, box=(0,pos))\n",
    "      pos = pos+img.size[1]+10\n",
    "  return final_img\n",
    "\n",
    "def visualize_attrs(img, attrs, ptile=99):\n",
    "  '''Visualizes the provided attributions by first aggregating them\n",
    "    along the color channel to obtain per-pixel attributions and then\n",
    "    scaling the intensities of the pixels in the original image in\n",
    "    proportion to absolute value of these attributions.\n",
    "\n",
    "    The provided image and attributions must of shape (224, 224, 3).\n",
    "  '''\n",
    "  if np.sum(attrs) == 0.0:\n",
    "    # print \"Attributions are all ZERO\"\n",
    "    return pil_img(0*img)\n",
    "  attrs = gray_scale(attrs)\n",
    "  attrs = abs(attrs)\n",
    "  attrs = np.clip(attrs/np.percentile(attrs, ptile), 0,1)\n",
    "  vis = img*attrs\n",
    "  return pil_img(vis)\n",
    "  \n",
    "  \n",
    "R=np.array([255,0,0])\n",
    "G=np.array([0,255,0])\n",
    "B=np.array([0,0,255])\n",
    "\n",
    "def visualize_attrs2(img, attrs, pos_ch=G, neg_ch=R, ptile=99):\n",
    "  '''Visualizes the provided attributions by first aggregating them\n",
    "     along the color channel and then overlaying the positive attributions\n",
    "     along pos_ch, and negative attributions along neg_ch.\n",
    "\n",
    "     The provided image and attributions must of shape (224, 224, 3).\n",
    "  '''\n",
    " \n",
    "  if np.sum(attrs) == 0.0:\n",
    "    # print \"Attributions are all ZERO\"\n",
    "    return pil_img(0*img)\n",
    " \n",
    "  attrs = gray_scale(attrs)\n",
    "  attrs = normalize(attrs, ptile)   \n",
    "  \n",
    "  pos_attrs = attrs * (attrs >= 0.0)\n",
    "  #pos_attrs = pos_attrs1 * (abs(pos_attrs1) >= threshold)\n",
    "  neg_attrs = -1.0 * attrs * (attrs < 0.0)\n",
    "  #neg_attrs = -1.0 * neg_attrs1 * (abs(neg_attrs1) >= threshold)\n",
    "  attrs_mask = pos_attrs*pos_ch + neg_attrs*neg_ch\n",
    "  vis = 0.3*gray_scale(img) + 0.7*attrs_mask\n",
    "  return pil_img(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_decision_path(estimator, inp):\n",
    "  # Extract the decision path taken by an input as an ordered list of indices\n",
    "  # of the neurons that were evaluated.\n",
    "  # See: http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
    "  n_nodes = estimator.tree_.node_count\n",
    "  feature = estimator.tree_.feature\n",
    "\n",
    "  # First let's retrieve the decision path of each sample. The decision_path\n",
    "  # method allows to retrieve the node indicator functions. A non zero element of\n",
    "  # indicator matrix at the position (i, j) indicates that the sample i goes\n",
    "  # through the node j.\n",
    "  X_test = [inp]\n",
    "  node_indicator = estimator.decision_path(X_test)\n",
    "  # Similarly, we can also have the leaves ids reached by each sample.\n",
    "  leaf_id = estimator.apply(X_test)\n",
    "  # Now, it's possible to get the tests that were used to predict a sample or\n",
    "  # a group of samples. First, let's make it for the sample.\n",
    "  node_index = node_indicator.indices[node_indicator.indptr[0]:\n",
    "                                      node_indicator.indptr[1]]\n",
    "  neuron_ids = []\n",
    "  for node_id in node_index:\n",
    "    if leaf_id[0] == node_id:\n",
    "        continue\n",
    "    neuron_ids.append(feature[node_id])\n",
    "  return neuron_ids\n",
    "\n",
    "def get_suffix_cluster(neuron_ids, neuron_sig,suffixes):\n",
    "  # Get the cluster of inputs that such that all inputs in the cluster\n",
    "  # have provided on/off signature for the provided neurons.\n",
    "  #\n",
    "  # The returned cluster is an array of indices (into mnist.train.images).\n",
    "  return np.where((suffixes[:, neuron_ids] == neuron_sig).all(axis=1))[0]\n",
    "\n",
    "def get_suffix_cluster_vals(neuron_ids, neuron_sig,suffixes):\n",
    "  # Get the cluster of inputs that such that all inputs in the cluster\n",
    "  # have provided on/off signature for the provided neurons.\n",
    "  #\n",
    "  # The returned cluster is an array of indices (into mnist.train.images).\n",
    "  cls_list = []\n",
    "  for inp_indx in range(0,len(suffixes)):\n",
    "    val = suffixes[inp_indx]\n",
    "    neu_indx = 0\n",
    "    fnd = 1\n",
    "    for indx in range(0,len(neuron_sig)):\n",
    "        if (indx %2 == 0):\n",
    "            oper = neuron_sig[indx]\n",
    "            continue\n",
    "        thres = neuron_sig[indx]\n",
    "        neu = neuron_ids[neu_indx]\n",
    "        neu_indx = neu_indx + 1\n",
    "        if (((oper =='<=') and (val[neu] > thres)) or ((oper == '>') and (val[neu] <= thres))):\n",
    "            fnd = 0\n",
    "            break\n",
    "            \n",
    "    if (fnd == 1):\n",
    "        cls_list.append(inp_indx)\n",
    "            \n",
    "\n",
    "  return cls_list\n",
    "  \n",
    "\n",
    "def is_consistent_cluster(cluster, predictions):\n",
    "  # Check if all inputs within the cluster have the same prediction.\n",
    "  # 'cluster' is an array of input ids.\n",
    "  pred = predictions[cluster[0]]\n",
    "  for i in cluster:\n",
    "    if predictions[i] != pred:\n",
    "      return False\n",
    "  return True\n",
    "\n",
    "def is_misclassified(i):\n",
    "  #print(train_predictions[i])\n",
    "  #print(y[i][0])\n",
    "  return False\n",
    "  #return (1 if train_predictions[i]>0.0 else 0) != (1 if y[i][0]>0.0 else 0)\n",
    "\n",
    "def visualize_conductances(img, label, layer, neuron_ids, only_on=False):\n",
    "  # Visualize the conductances for the provided image.\n",
    "  # Args:\n",
    "  # - img: the provided mnist image\n",
    "  # - label: prediction label w.r.t. conductance must be computed\n",
    "  # - neuron_ids: list of neurons indices from the suffix tensor for which\n",
    "  #    conductances must be computed.\n",
    "  # - only_on: If True then conductance is computed only for those neurons\n",
    "  #    that are on for the given image. \n",
    "  vis = [mnist_to_pil_img(img)]\n",
    "  suffix = fingerprint_suffix([img], layer)\n",
    "  sumigc = 0.0\n",
    "  for i, id in enumerate(neuron_ids):\n",
    "    if only_on and suffix[i] != 1:\n",
    "      continue  \n",
    "    igc = conductance(img, label, neuron_id=id)\n",
    "    # igc = conductances[id]\n",
    "    sumigc = sumigc + igc\n",
    "  \n",
    "  \n",
    "  avgigc = sumigc / len(neuron_ids)\n",
    "  maxval = abs(max(avgigc, key=abs))\n",
    "  minval = abs(min(avgigc, key=abs))\n",
    "  threshold = (maxval - minval)/2.0\n",
    "  print(\"MAX ATR:\", maxval, \"MIN ATR:\", minval, \"THRESH:\", threshold)\n",
    "  avgigc = 1.0 * avgigc * (abs(avgigc) >= threshold)\n",
    "  \n",
    "  \n",
    "  vis.append(visualize_attrs2(255*mnist_to_rgb(img), mnist_to_rgb(avgigc)))\n",
    "  return combine(vis)\n",
    "\n",
    "def get_invariant_inp(estimator, ref_id, suffixes):\n",
    "  # Returns an invariant found w.r.t. the provided reference input\n",
    "  # Args\n",
    "  #  - inp: reference input, shape <784,>\n",
    "  # Returns:\n",
    "  #  - cluster: Indices of training inputs that satisfy the invariant\n",
    "  #  - neuron_id: A list of neurons such that all inputs that agree with\n",
    "  #    the reference input on the on/off status of these neurons have the\n",
    "  #    same prediction as the reference input.\n",
    "  ref_img = mnist_inp_images[ref_id]\n",
    "  ref_suffix = suffixes[ref_id]\n",
    "  print('PREFIX',ref_suffix)\n",
    "  neuron_ids = get_decision_path(estimator, ref_suffix)\n",
    "  print('NEURON IDS',neuron_ids)\n",
    "  neuron_sig = ref_suffix[neuron_ids]\n",
    "  print('NEURON SIGNATURE',neuron_sig)\n",
    "  cluster = get_suffix_cluster(neuron_ids, neuron_sig,suffixes)\n",
    "  imgs = []\n",
    "  cnt = 0\n",
    "  for indx1 in range(0,len(cluster)):\n",
    "    img = mnist.train.images(cluster[indx1])\n",
    "    fnd = 1\n",
    "    for i in range(0,len(img)):\n",
    "      if (ref_img[i] != img[i]):\n",
    "        fnd = 0\n",
    "        break\n",
    "    if (fnd == 1):\n",
    "        ref_id = cnt\n",
    "    cnt = cnt + 1\n",
    "    imgs.append(img)\n",
    "    \n",
    "  imgs_suffixes = fingerprint_signature(imgs,'dense_1')\n",
    "  ref_suffix = imgs_suffixes[ref_id]\n",
    "  print('PREFIX',ref_suffix)\n",
    "  neuron_ids = get_decision_path(estimator, ref_suffix)\n",
    "  print('NEURON IDS',neuron_ids)\n",
    "  neuron_sig = ref_suffix[neuron_ids]\n",
    "  print('NEURON SIGNATURE',neuron_sig)\n",
    "  cluster = get_suffix_cluster(neuron_ids, neuron_sig, imgs_suffixes)\n",
    "    \n",
    "  return cluster, neuron_ids, neuron_sig\n",
    "\n",
    "def get_invariant(estimator, ref_id, suffixes):\n",
    "  # Returns an invariant found w.r.t. the provided reference input\n",
    "  # Args\n",
    "  #  - ref_id: Index (into mnist.train.images) of the reference input\n",
    "  # Returns:\n",
    "  #  - cluster: Indices of training inputs that satisfy the invariant\n",
    "  #  - neuron_id: A list of neurons such that all inputs that agree with\n",
    "  #    the reference input on the on/off status of these neurons have the\n",
    "  #    same prediction as the reference input.\n",
    "  ref_img = mnist.train.images[ref_id]\n",
    "  ref_suffix = suffixes[ref_id]\n",
    "  neuron_ids = get_decision_path(estimator, ref_suffix)\n",
    "  neuron_sig = ref_suffix[neuron_ids]\n",
    "  cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
    "  return cluster, neuron_ids, neuron_sig\n",
    "\n",
    "\n",
    "def get_all_invariants(estimator):\n",
    "  # Returns a dictionary mapping each decision tree prediction class\n",
    "  # to a list of invariants. Each invariant is specified as a triple:\n",
    "  # - neuron ids\n",
    "  # - neuron signature (for the neuron ids)\n",
    "  # - number of training samples that hit it\n",
    "  # The neuron ids and neuron signature can be supplied to get_suffix_cluster\n",
    "  # to obtain the cluster of training instances that hit the invariant.\n",
    "  def is_leaf(node):\n",
    "    return estimator.tree_.children_left[node] == estimator.tree_.children_right[node]\n",
    "\n",
    "  def left_child(node):\n",
    "    return estimator.tree_.children_left[node]\n",
    "\n",
    "  def right_child(node):\n",
    "    return estimator.tree_.children_right[node]\n",
    "  \n",
    "  def get_all_paths_rec(node):\n",
    "    # Returns a list of triples corresponding to paths\n",
    "    # in the decision tree. Each triple consists of\n",
    "    # - neurons encountered along the path\n",
    "    # - signature along the path\n",
    "    # - prediction class at the leaf\n",
    "    # - number of training samples that hit the path\n",
    "    # The prediction class and number of training samples\n",
    "    # are set to -1 when the leaf is \"impure\".\n",
    "    feature = estimator.tree_.feature\n",
    "    if is_leaf(node):\n",
    "      values = estimator.tree_.value[node][0]\n",
    "      if len(np.where(values != 0)[0]) == 1:\n",
    "        cl = estimator.classes_[np.where(values != 0)[0][0]]\n",
    "        nsamples = estimator.tree_.n_node_samples[node]\n",
    "      else:\n",
    "        # impure node\n",
    "        cl = -1\n",
    "        nsamples = -1\n",
    "      return [[[], [], cl, nsamples]]\n",
    "    # If it is not a leaf both left and right childs must exist\n",
    "    paths = [[[feature[node]] + p[0], [0] + p[1], p[2], p[3]] for p in get_all_paths_rec(left_child(node))]\n",
    "    paths += [[[feature[node]] + p[0], [1] + p[1], p[2], p[3]] for p in get_all_paths_rec(right_child(node))]\n",
    "    return paths\n",
    "  paths =  get_all_paths_rec(0)\n",
    "  print(\"Obtained all paths\")\n",
    "  invariants = {}\n",
    "  for p in tqdm(paths):\n",
    "    neuron_ids, neuron_sig, cl, nsamples = p\n",
    "    if cl not in invariants:\n",
    "      invariants[cl] = []\n",
    "    # cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
    "    invariants[cl].append([neuron_ids, neuron_sig, nsamples])\n",
    "  for cl in invariants.keys():\n",
    "    invariants[cl] = sorted(invariants[cl], key=operator.itemgetter(2), reverse=True)\n",
    "  return invariants\n",
    "\n",
    "\n",
    "def get_all_invariants_vals(estimator):\n",
    "  # Returns a dictionary mapping each decision tree prediction class\n",
    "  # to a list of invariants. Each invariant is specified as a triple:\n",
    "  # - neuron ids\n",
    "  # - neuron signature (for the neuron ids)\n",
    "  # - number of training samples that hit it\n",
    "  # The neuron ids and neuron signature can be supplied to get_suffix_cluster\n",
    "  # to obtain the cluster of training instances that hit the invariant.\n",
    "  def is_leaf(node):\n",
    "    return estimator.tree_.children_left[node] == estimator.tree_.children_right[node]\n",
    "\n",
    "  def left_child(node):\n",
    "    return estimator.tree_.children_left[node]\n",
    "\n",
    "  def right_child(node):\n",
    "    return estimator.tree_.children_right[node]\n",
    "  \n",
    "  def get_all_paths_rec(node):\n",
    "    # Returns a list of triples corresponding to paths\n",
    "    # in the decision tree. Each triple consists of\n",
    "    # - neurons encountered along the path\n",
    "    # - signature along the path\n",
    "    # - prediction class at the leaf\n",
    "    # - number of training samples that hit the path\n",
    "    # The prediction class and number of training samples\n",
    "    # are set to -1 when the leaf is \"impure\".\n",
    "    feature = estimator.tree_.feature\n",
    "    threshold = estimator.tree_.threshold\n",
    "    if is_leaf(node):\n",
    "      values = estimator.tree_.value[node][0]\n",
    "      if len(np.where(values != 0)[0]) == 1:\n",
    "        cl = estimator.classes_[np.where(values != 0)[0][0]]\n",
    "        nsamples = estimator.tree_.n_node_samples[node]\n",
    "      else:\n",
    "        # impure node\n",
    "        cl = -1\n",
    "        nsamples = -1\n",
    "      return [[[], [], cl, nsamples]]\n",
    "    # If it is not a leaf both left and right childs must exist\n",
    "    #paths = [[[feature[node]] + p[0], [0] + p[1], p[2], p[3]] for p in get_all_paths_rec(left_child(node))]\n",
    "    #paths += [[[feature[node]] + p[0], [1] + p[1], p[2], p[3]] for p in get_all_paths_rec(right_child(node))]\n",
    "    paths = [[[feature[node]] + p[0],['<='] + [threshold[node]] + p[1], p[2], p[3]] for p in get_all_paths_rec(left_child(node))]\n",
    "    paths += [[[feature[node]] + p[0],['>'] + [threshold[node]] + p[1], p[2], p[3]] for p in get_all_paths_rec(right_child(node))]\n",
    "    return paths\n",
    "  paths =  get_all_paths_rec(0)\n",
    "  print(\"Obtained all paths\")\n",
    "  invariants = {}\n",
    "  for p in tqdm(paths):\n",
    "    neuron_ids, neuron_sig, cl, nsamples = p\n",
    "    if cl not in invariants:\n",
    "      invariants[cl] = []\n",
    "    # cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
    "    invariants[cl].append([neuron_ids, neuron_sig, nsamples])\n",
    "  for cl in invariants.keys():\n",
    "    invariants[cl] = sorted(invariants[cl], key=operator.itemgetter(2), reverse=True)\n",
    "  return invariants\n",
    "\n",
    "\n",
    "def describe_cluster(cluster, neuron_ids, suffixes, show_samples=False):\n",
    "  neuron_sig = suffixes[cluster[0]][neuron_ids]\n",
    "  print(\"Num neurons in invariant\", len(neuron_ids))\n",
    "  print(\"Neuron id and signature\")\n",
    "  \n",
    "  for i in range(0,len(neuron_ids)):\n",
    "    print(\"id:\", neuron_ids[i], \"sig:\", neuron_sig[i])\n",
    "  \n",
    "  print(\"Cluster size: \", len(cluster))\n",
    "  print(\"Num misclassified\", len([i for i in cluster if is_misclassified(i)]))\n",
    "  if show_samples:\n",
    "    for i in range(10):\n",
    "      images = []\n",
    "      for j in range(10):\n",
    "        if 10*i + j >= len(cluster):\n",
    "          break\n",
    "        images.append(mnist_to_pil_img(mnist.train.images[cluster[10*i+j]]))\n",
    "      if len(images) > 0:\n",
    "        show_img(combine(images))\n",
    "  \n",
    "\n",
    "def describe_invariants_all_labels1(all_invariants,prevlayer,layer,suffixes, COMMON=False, DEC_PREFX= False):\n",
    "  print(\"PRINTING PURE RULES WITH SUPPORT MORE THAN 1000 FOR EVERY LABEL:\");\n",
    "  for cl, invs in all_invariants.items():\n",
    "    if (cl == -1):\n",
    "      continue\n",
    "    \n",
    "    for indx in range (0, 5):\n",
    "    #len(invs)):\n",
    "      inv = invs[indx]\n",
    "      cls = get_suffix_cluster(inv[0],inv[1],suffixes)\n",
    "      \n",
    "      neurons = inv[0]\n",
    "      signature = inv[1]\n",
    "\n",
    "      if (len(cls) <= 1000):\n",
    "        continue\n",
    "      print(\"Class:\", cl, \", Rule:(neurons:\",inv[0],\",signature:\",inv[1],\"), Support:\",inv[2],\", Num misclassified\", len([i for i in cls if is_misclassified(i)]));\n",
    "\n",
    "      ##invoke_marabou_chk(LAYER,neurons,signature,cl)\n",
    "\n",
    "      if (COMMON == True):\n",
    "          common_nodes(cls,suffixes)\n",
    "\n",
    "      if (DEC_PREFX == True):\n",
    "          decision_prefs(cls,suffixes)\n",
    "\n",
    "  return\n",
    "  \n",
    "def common_nodes(cls,suffixes):\n",
    "    cnt = 0\n",
    "    common = np.zeros(10,dtype=int)\n",
    "    prev = np.zeros(10,dtype=int)\n",
    "    \n",
    "    for indx in range(0, len(cls)):\n",
    "        i = cls[indx]\n",
    "        cnt = cnt + 1\n",
    "        for j in range(0,len(suffixes[i])):\n",
    "          if (common[j] == -1):\n",
    "             continue\n",
    "          if ((indx != 0) and (suffixes[i][j] != prev[j])):\n",
    "             common[j] = -1\n",
    "          else:\n",
    "             common[j] = suffixes[i][j]\n",
    "          prev[j] = suffixes[i][j]\n",
    "\n",
    "\n",
    "    print('COMMON NODES IN CLUSTER for CLASS:',cl,cnt)\n",
    "    com = []\n",
    "    for k in range(0,len(common)):\n",
    "        if (common[k] != -1):\n",
    "           com.append((k,common[k]))\n",
    "    print(com)\n",
    "\n",
    "    return\n",
    "    \n",
    "def decision_prefs(cls,suffixes):\n",
    "    images = mnist.train.images\n",
    "    imgsCom = []\n",
    "    imgs = []\n",
    "    for indx in range(0, len(cls)):\n",
    "        print('IMG:')\n",
    "        print(list(zip(images[cls[indx]])))\n",
    "        imgs.append(images[cls[indx]])\n",
    "        imgsCom.append(images[cls[indx]])\n",
    "            \n",
    "    dec_prefixes= fingerprint_signature(imgs,layer)\n",
    "    prefixes = []\n",
    "    for indx in range(0,len(dec_prefixes)):\n",
    "       dec_pref = dec_prefixes[indx]\n",
    "    \n",
    "       match = 0\n",
    "       for indx1 in range(0, len(prefixes)):\n",
    "          match = 1\n",
    "          for i in range(0,len(prefixes[indx1])):\n",
    "             if (dec_pref[i] != prefixes[indx1][i]):\n",
    "                match = 0\n",
    "                break\n",
    "          if (match == 1):\n",
    "             break\n",
    "    \n",
    "       if (match == 0):\n",
    "          prefixes.append(dec_pref)\n",
    "    \n",
    "    print('DECISION PREFIXES IN CLUSTER for CLASS:',cl,cnt)\n",
    "    for k in range(0,len(prefixes)):\n",
    "      print(prefixes[k])\n",
    "\n",
    "    return\n",
    "\n",
    "def describe_all_invariants(all_invariants):\n",
    "  df = []\n",
    "  for cl, invs in all_invariants.iteritems(): \n",
    "    inv = invs[0]\n",
    "    clus = get_suffix_cluster(inv[0],inv[1])\n",
    "    #print(len(clus))\n",
    "    misCl = 0\n",
    "    for i in range(0,len(clus)):\n",
    "      indx = clus[i]\n",
    "      if (is_misclassified(indx) == True):\n",
    "        misCl = misCl + 1\n",
    "    print('class:',cl,',masSup:',inv[2],',#misCl:',misCl)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = 'dense_1'\n",
    "layer2 = 'dense_2'\n",
    "layer3 = 'dense_3'\n",
    "\n",
    "### TEST SET\n",
    "# Get the fingerprint (neuron values) after dense_1 and the output labels for correctness property\n",
    "print(\"LAYER 1:\")\n",
    "test_suffixes1 = fingerprint_suffix_vals(x_test, layer1)\n",
    "print(np.shape(x_test),np.shape(y_test))\n",
    "print(\"Suffixes computed for all test data\")\n",
    "actual_preds1, test_predictions1 = get_prediction(x_test,Feature=False,Prop=True,TEST=True)\n",
    "print(\"Predictions computed for all test data\")\n",
    "\n",
    "print(\"Class 1:\", list(test_predictions1).count(1))\n",
    "print(\"Class 0:\", list(test_predictions1).count(0))\n",
    "\n",
    "x_test_full_seq = []\n",
    "y_test_full_seq = []\n",
    "x_test_seq = []\n",
    "y_test_seq = []\n",
    "x_val_seq = []\n",
    "y_val_seq = []\n",
    "\n",
    "cnt = 0\n",
    "threshold1 = -1\n",
    "threshold2 = -1\n",
    "for ix in range(0, len(x_test)):\n",
    "    num = ix/100.0\n",
    "    if (num > 19 and num < 24):\n",
    "        x_test_full_seq.append(x_test[ix])\n",
    "        y_test_full_seq.append(y_test[ix])\n",
    "        if (num == 21.54):\n",
    "            threshold1 = cnt\n",
    "        if (num == 22.54):\n",
    "            threshold2 = cnt\n",
    "        cnt =cnt + 1\n",
    "        \n",
    "import random\n",
    "length = len(x_test_full_seq)\n",
    "ind1 = 0\n",
    "valinds = []\n",
    "while (ind1 < (length)/2):\n",
    "    ix1 = random.randint(0,length)\n",
    "    while(ix1 in valinds):\n",
    "        ix1 = random.randint(0,length)\n",
    "    valinds.append(ix1)\n",
    "    ind1 = ind1 + 1\n",
    "\n",
    "feat_val_nums = []\n",
    "feat_test_nums = []\n",
    "for ind1 in range(0,length):\n",
    "    if (ind1 in valinds):\n",
    "        x_val_seq.append(x_test_full_seq[ind1])\n",
    "        y_val_seq.append(y_test_full_seq[ind1])\n",
    "        if (ind1 >= threshold1 and ind1 <= threshold2):\n",
    "            feat_val_nums.append(len(x_val_seq)-1)\n",
    "    else:\n",
    "        x_test_seq.append(x_test_full_seq[ind1])\n",
    "        y_test_seq.append(y_test_full_seq[ind1])\n",
    "        if (ind1 >= threshold1 and ind1 <= threshold2):\n",
    "            feat_test_nums.append(len(x_test_seq)-1)\n",
    "\n",
    "x_val_seq = np.array(x_val_seq)\n",
    "y_val_seq = np.array(y_val_seq)\n",
    "x_test_seq = np.array(x_test_seq)\n",
    "y_test_seq = np.array(y_test_seq)\n",
    "\n",
    "feat_val_nums.sort()\n",
    "feat_test_nums.sort()\n",
    "print(\"VAL NUM:\", len(x_val_seq), \"TEST NUM:\", len(x_test_seq))\n",
    "print(\"FEATURE VAL NUM:\", feat_val_nums, \"FEATURE TEST NUM:\", feat_test_nums)\n",
    "\n",
    "feat_val_nums = np.array(feat_val_nums)\n",
    "feat_test_nums = np.array(feat_test_nums)\n",
    "\n",
    "threshold1 = feat_test_nums[0]\n",
    "feat_len = len(feat_test_nums)-1\n",
    "threshold2 = feat_test_nums[feat_len]\n",
    "\n",
    "Vthreshold1 = feat_val_nums[0]\n",
    "feat_len = len(feat_val_nums)-1\n",
    "Vthreshold2 = feat_val_nums[feat_len]\n",
    "\n",
    "\n",
    "# Get the fingerprint (neuron values) after dense_1 and the output labels for Feature Intersection\n",
    "test_suffixes1F = fingerprint_suffix_vals(x_test_seq, layer1)\n",
    "print(\"FEATURE Suffixes computed for all test data\")\n",
    "actual_preds1F, test_predictions1F = get_prediction(x_test_seq,LAB=np.array(y_test_seq),Feature=True,Prop=False,TEST=True,THRESHOLD1=threshold1,THRESHOLD2=threshold2)\n",
    "print(\"FEATURE Predictions computed for all test data\")\n",
    "\n",
    "print(\"Class 1 F:\", list(test_predictions1F).count(1))\n",
    "print(\"Class 0 F:\", list(test_predictions1F).count(0))\n",
    "\n",
    "# Get the fingerprint (neuron values) after dense_1 and the output labels for Feature Intersection and correctness prop \n",
    "test_suffixes1FT = fingerprint_suffix_vals(x_test_seq, layer1)\n",
    "print(\"FEATURE AND PROP Suffixes computed for all test data\")\n",
    "actual_preds1FT, test_predictions1FT = get_prediction(x_test_seq,LAB=np.array(y_test_seq),Feature=True,Prop=True,TEST=True,THRESHOLD1=threshold1,THRESHOLD2=threshold2)\n",
    "print(\"FEATURE AND PROP Predictions computed for all test data\")\n",
    "\n",
    "print(\"Class 0 FT:\", list(test_predictions1FT).count(0))\n",
    "print(\"Class 1 FT:\", list(test_predictions1FT).count(1))\n",
    "print(\"Class 2 FT:\", list(test_predictions1FT).count(2))\n",
    "print(\"Class 3 FT:\", list(test_predictions1FT).count(3))\n",
    "\n",
    "# Get the fingerprint (neuron values) after dense_1 and the output labels for correctness property\n",
    "test_suffixes1CI = fingerprint_suffix_vals(x_test_seq, layer1)\n",
    "print(\"PROP Suffixes computed for all test data\")\n",
    "actual_preds1CI, test_predictions1CI = get_prediction(x_test_seq,LAB =np.array(y_test_seq),Feature=False,Prop=True,TEST=True)\n",
    "print(\"PROP Predictions computed for all test data\")\n",
    "print(\"Class 1:\", list(test_predictions1CI).count(1))\n",
    "print(\"Class 0:\", list(test_predictions1CI).count(0))\n",
    "\n",
    "### VALIDATION SET\n",
    "\n",
    "val_suffixes1F = fingerprint_suffix_vals(x_val_seq, layer1)\n",
    "print(\"FEATURE Suffixes computed for all val data\")\n",
    "Vactual_preds1F, val_predictions1F = get_prediction(x_val_seq,LAB=np.array(y_val_seq),Feature=True,Prop=False,TEST=True,THRESHOLD1=Vthreshold1,THRESHOLD2=Vthreshold2)\n",
    "print(\"FEATURE Predictions computed for all test data\")\n",
    "\n",
    "print(\"Class 1 F:\", list(val_predictions1F).count(1))\n",
    "print(\"Class 0 F:\", list(val_predictions1F).count(0))\n",
    "\n",
    "\n",
    "val_suffixes1CI = fingerprint_suffix_vals(x_val_seq, layer1)\n",
    "print(\"PROP Suffixes computed for all val data\")\n",
    "Vactual_preds1CI, val_predictions1CI = get_prediction(x_val_seq,LAB =np.array(y_val_seq),Feature=False,Prop=True,TEST=True)\n",
    "print(\"PROP Predictions computed for all test data\")\n",
    "print(\"Class 1:\", list(val_predictions1CI).count(1))\n",
    "print(\"Class 0:\", list(val_predictions1CI).count(0))\n",
    "\n",
    "# ###################################################################\n",
    "print(\"LAYER 2:\")\n",
    "test_suffixes2 = fingerprint_suffix_vals(x_test, layer2)\n",
    "print(\"Suffixes computed for all test data\")\n",
    "print(np.shape(x_test),np.shape(y_test))\n",
    "actual_preds2 = actual_preds1\n",
    "test_predictions2 = test_predictions1\n",
    "print(\"Predictions computed for all test data\")\n",
    "\n",
    "print(\"Class 1:\", list(test_predictions2).count(1))\n",
    "print(\"Class 0:\", list(test_predictions2).count(0))\n",
    "\n",
    "test_suffixes2F = fingerprint_suffix_vals(x_test_seq, layer2)\n",
    "print(\"FEATURE Suffixes computed for all test data\")\n",
    "actual_preds2F = actual_preds1F\n",
    "test_predictions2F = test_predictions1F\n",
    "print(\"FEATURE Predictions computed for all test data\")\n",
    "\n",
    "print(\"Class 1 F:\", list(test_predictions2F).count(1))\n",
    "print(\"Class 0 F:\", list(test_predictions2F).count(0))\n",
    "\n",
    "test_suffixes2FT = fingerprint_suffix_vals(x_test_seq, layer2)\n",
    "print(\"FEATURE AND PROP Suffixes computed for all test data\")\n",
    "actual_preds2FT = actual_preds1FT\n",
    "test_predictions2FT = test_predictions1FT\n",
    "print(\"FEATURE AND PROP Predictions computed for all test data\")\n",
    "\n",
    "print(\"Class 0 ft:\", list(test_predictions2FT).count(0))\n",
    "print(\"Class 1 ft:\", list(test_predictions2FT).count(1))\n",
    "print(\"Class 2 ft:\", list(test_predictions2FT).count(2))\n",
    "print(\"Class 3 ft:\", list(test_predictions2FT).count(3))\n",
    "\n",
    "test_suffixes2CI = fingerprint_suffix_vals(x_test_seq, layer2)\n",
    "print(\"PROP Suffixes computed for all test data\")\n",
    "actual_preds2CI, test_predictions2CI = get_prediction(x_test_seq,LAB =np.array(y_test_seq),Feature=False,Prop=True,TEST=True)\n",
    "print(\"PROP Predictions computed for all test data\")\n",
    "print(\"Class 1:\", list(test_predictions2CI).count(1))\n",
    "print(\"Class 0:\", list(test_predictions2CI).count(0))\n",
    "\n",
    "val_suffixes2F = fingerprint_suffix_vals(x_val_seq, layer2)\n",
    "print(\"FEATURE Suffixes computed for all val data\")\n",
    "Vactual_preds2F, val_predictions2F = get_prediction(x_val_seq,LAB=np.array(y_val_seq),Feature=True,Prop=False,TEST=True,THRESHOLD1=Vthreshold1,THRESHOLD2=Vthreshold2)\n",
    "print(\"FEATURE Predictions computed for all test data\")\n",
    "\n",
    "print(\"Class 1 F:\", list(val_predictions2F).count(1))\n",
    "print(\"Class 0 F:\", list(val_predictions2F).count(0))\n",
    "\n",
    "\n",
    "val_suffixes2CI = fingerprint_suffix_vals(x_val_seq, layer2)\n",
    "print(\"PROP Suffixes computed for all val data\")\n",
    "Vactual_preds2CI, val_predictions2CI = get_prediction(x_val_seq,LAB =np.array(y_val_seq),Feature=False,Prop=True,TEST=True)\n",
    "print(\"PROP Predictions computed for all test data\")\n",
    "print(\"Class 1:\", list(val_predictions2CI).count(1))\n",
    "print(\"Class 0:\", list(val_predictions2CI).count(0))\n",
    "# ###########################################################################\n",
    "print(\"LAYER 3\")\n",
    "test_suffixes3 = fingerprint_suffix_vals(x_test, layer3)\n",
    "print(\"Suffixes computed for all test data\")\n",
    "print(np.shape(x_test),np.shape(y_test))\n",
    "actual_preds3 = actual_preds1\n",
    "test_predictions3 = test_predictions1\n",
    "print(\"Predictions computed for all test data\")\n",
    "\n",
    "print(\"Class 1:\", list(test_predictions3).count(1))\n",
    "print(\"Class 0:\", list(test_predictions3).count(0))\n",
    "\n",
    "test_suffixes3F = fingerprint_suffix_vals(x_test_seq, layer3)\n",
    "print(\"FEATURE Suffixes computed for all test data\")\n",
    "actual_preds3F = actual_preds1F\n",
    "test_predictions3F = test_predictions1F\n",
    "print(\"FEATURE Predictions computed for all test data\")\n",
    "\n",
    "print(\"Class 1 F:\", list(test_predictions3F).count(1))\n",
    "print(\"Class 0 F:\", list(test_predictions3F).count(0))\n",
    "\n",
    "test_suffixes3FT = fingerprint_suffix_vals(x_test_seq, layer3)\n",
    "print(\"FEATURE AND PROP Suffixes computed for all test data\")\n",
    "actual_preds3FT = actual_preds1FT\n",
    "test_predictions3FT = test_predictions1FT\n",
    "print(\"FEATURE AND PROP Predictions computed for all test data\")\n",
    "\n",
    "print(\"Class 0 ft:\", list(test_predictions3FT).count(0))\n",
    "print(\"Class 1 ft:\", list(test_predictions3FT).count(1))\n",
    "print(\"Class 2 ft:\", list(test_predictions3FT).count(2))\n",
    "print(\"Class 3 ft:\", list(test_predictions3FT).count(3))\n",
    "\n",
    "test_suffixes3CI = fingerprint_suffix_vals(x_test_seq, layer3)\n",
    "print(\"PROP Suffixes computed for all test data\")\n",
    "actual_preds3CI, test_predictions3CI = get_prediction(x_test_seq,LAB =np.array(y_test_seq),Feature=False,Prop=True,TEST=True)\n",
    "print(\"PROP Predictions computed for all test data\")\n",
    "print(\"Class 1:\", list(test_predictions3CI).count(1))\n",
    "print(\"Class 0:\", list(test_predictions3CI).count(0))\n",
    "\n",
    "val_suffixes3F = fingerprint_suffix_vals(x_val_seq, layer3)\n",
    "print(\"FEATURE Suffixes computed for all val data\")\n",
    "Vactual_preds3F, val_predictions3F = get_prediction(x_val_seq,LAB=np.array(y_val_seq),Feature=True,Prop=False,TEST=True,THRESHOLD1=Vthreshold1,THRESHOLD2=Vthreshold2)\n",
    "print(\"FEATURE Predictions computed for all val data\")\n",
    "\n",
    "print(\"Class 1 F:\", list(val_predictions3F).count(1))\n",
    "print(\"Class 0 F:\", list(val_predictions3F).count(0))\n",
    "\n",
    "\n",
    "val_suffixes3CI = fingerprint_suffix_vals(x_val_seq, layer3)\n",
    "print(\"PRP Suffixes computed for all val data\")\n",
    "Vactual_preds3CI, val_predictions3CI = get_prediction(x_val_seq,LAB =np.array(y_val_seq),Feature=False,Prop=True,TEST=True)\n",
    "print(\"PROP Predictions computed for all val data\")\n",
    "print(\"Class 1:\", list(val_predictions3CI).count(1))\n",
    "print(\"Class 0:\", list(val_predictions3CI).count(0))\n",
    "\n",
    "##################################################################################\n",
    "print(\"ALL\")\n",
    "test_suffixesA = fingerprint_suffix_vals(x_test, \"ALL\")\n",
    "print(\"Suffixes computed for all test data\")\n",
    "print(np.shape(x_test),np.shape(y_test))\n",
    "actual_predsA = actual_preds1\n",
    "test_predictionsA = test_predictions1\n",
    "print(\"Predictions computed for all test data\")\n",
    "\n",
    "test_suffixesAF = fingerprint_suffix_vals(x_test_seq, \"ALL\")\n",
    "print(\"FEATURE Suffixes computed for all test data\")\n",
    "actual_predsAF, test_predictionsAF = get_prediction(x_test_seq,LAB=np.array(y_test_seq),Feature=True,Prop=False,TEST=True,THRESHOLD1=threshold1,THRESHOLD2=threshold2)\n",
    "print(\"FEATURE Predictions computed for all test data\")\n",
    "\n",
    "print(\"Class 1 F:\", list(test_predictionsAF).count(1))\n",
    "print(\"Class 0 F:\", list(test_predictionsAF).count(0))\n",
    "\n",
    "\n",
    "test_suffixesAFT = fingerprint_suffix_vals(x_test_seq, \"ALL\")\n",
    "print(\"FEATURE AND PROP Suffixes computed for all test data\")\n",
    "actual_predsAFT, test_predictionsAFT = get_prediction(x_test_seq,LAB=np.array(y_test_seq),Feature=True,Prop=True,TEST=True,THRESHOLD1=threshold1,THRESHOLD2=threshold2)\n",
    "print(\"FEATURE AND PROP Predictions computed for all test data\")\n",
    "\n",
    "print(\"Class 0 FT:\", list(test_predictionsAFT).count(0))\n",
    "print(\"Class 1 FT:\", list(test_predictionsAFT).count(1))\n",
    "print(\"Class 2 FT:\", list(test_predictionsAFT).count(2))\n",
    "print(\"Class 3 FT:\", list(test_predictionsAFT).count(3))\n",
    "\n",
    "test_suffixesACI = fingerprint_suffix_vals(x_test_seq, \"ALL\")\n",
    "print(\"PROP Suffixes computed for all test data\")\n",
    "actual_predsACI, test_predictionsACI = get_prediction(x_test_seq,LAB =np.array(y_test_seq),Feature=False,Prop=True,TEST=True)\n",
    "print(\"PROP Predictions computed for all test data\")\n",
    "print(\"Class 1:\", list(test_predictionsACI).count(1))\n",
    "print(\"Class 0:\", list(test_predictionsACI).count(0))\n",
    "\n",
    "val_suffixesAF = fingerprint_suffix_vals(x_val_seq, \"ALL\")\n",
    "print(\"FEATURE Suffixes computed for all val data\")\n",
    "Vactual_predsAF, val_predictionsAF = get_prediction(x_val_seq,LAB=np.array(y_val_seq),Feature=True,Prop=False,TEST=True,THRESHOLD1=Vthreshold1,THRESHOLD2=Vthreshold2)\n",
    "print(\"FEATURE Predictions computed for all test data\")\n",
    "\n",
    "print(\"Class 1 F:\", list(val_predictionsAF).count(1))\n",
    "print(\"Class 0 F:\", list(val_predictionsAF).count(0))\n",
    "\n",
    "\n",
    "val_suffixesACI = fingerprint_suffix_vals(x_val_seq, \"ALL\")\n",
    "print(\"PROP Suffixes computed for all val data\")\n",
    "Vactual_predsACI, val_predictionsACI = get_prediction(x_val_seq,LAB =np.array(y_val_seq),Feature=False,Prop=True,TEST=True)\n",
    "print(\"PROP Predictions computed for all test data\")\n",
    "print(\"Class 1:\", list(val_predictionsACI).count(1))\n",
    "print(\"Class 0:\", list(val_predictionsACI).count(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Decision Trees \n",
    "basic_estimator1 = tree.DecisionTreeClassifier()\n",
    "basic_estimator1.fit(test_suffixes1, test_predictions1)\n",
    "\n",
    "basic_estimator1F = tree.DecisionTreeClassifier()\n",
    "basic_estimator1F.fit(test_suffixes1F, test_predictions1F)\n",
    "\n",
    "basic_estimator1FT = tree.DecisionTreeClassifier()\n",
    "basic_estimator1FT.fit(test_suffixes1FT, test_predictions1FT)\n",
    "\n",
    "basic_estimator1CI = tree.DecisionTreeClassifier()\n",
    "basic_estimator1CI.fit(test_suffixes1CI, test_predictions1CI)\n",
    "# # Basic decision tree\n",
    "basic_estimator2 = tree.DecisionTreeClassifier()\n",
    "basic_estimator2.fit(test_suffixes2, test_predictions2)\n",
    "\n",
    "basic_estimator2F = tree.DecisionTreeClassifier()\n",
    "basic_estimator2F.fit(test_suffixes2F, test_predictions2F)\n",
    "\n",
    "basic_estimator2FT = tree.DecisionTreeClassifier()\n",
    "basic_estimator2FT.fit(test_suffixes2FT, test_predictions2FT)\n",
    "\n",
    "\n",
    "basic_estimator2CI = tree.DecisionTreeClassifier()\n",
    "basic_estimator2CI.fit(test_suffixes2CI, test_predictions2CI)\n",
    "\n",
    "basic_estimator3 = tree.DecisionTreeClassifier()\n",
    "basic_estimator3.fit(test_suffixes3, test_predictions3)\n",
    "\n",
    "basic_estimator3F = tree.DecisionTreeClassifier()\n",
    "basic_estimator3F.fit(test_suffixes3F, test_predictions3F)\n",
    "\n",
    "basic_estimator3FT = tree.DecisionTreeClassifier()\n",
    "basic_estimator3FT.fit(test_suffixes3FT, test_predictions3FT)\n",
    "\n",
    "\n",
    "basic_estimator3CI = tree.DecisionTreeClassifier()\n",
    "basic_estimator3CI.fit(test_suffixes3CI, test_predictions3CI)\n",
    "\n",
    "basic_estimatorA = tree.DecisionTreeClassifier()\n",
    "basic_estimatorA.fit(test_suffixesA, test_predictionsA)\n",
    "\n",
    "basic_estimatorAF = tree.DecisionTreeClassifier()\n",
    "basic_estimatorAF.fit(test_suffixesAF, test_predictionsAF)\n",
    "\n",
    "basic_estimatorAFT = tree.DecisionTreeClassifier()\n",
    "basic_estimatorAFT.fit(test_suffixesAFT, test_predictionsAFT)\n",
    "\n",
    "\n",
    "basic_estimatorACI = tree.DecisionTreeClassifier()\n",
    "basic_estimatorACI.fit(test_suffixesACI, test_predictionsACI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.gradcam import GradcamPlusPlus\n",
    "\n",
    "# The `output` variable refer to the output of the model,\n",
    "# so, in this case, `output` shape is `(1, 2)` i.e., (samples, classes).\n",
    "\n",
    "#Loss and Model Modifier for middle layers\n",
    "def loss_gen_sum(node_list):\n",
    "    def loss(output):\n",
    "        loss_val = sum([output[0][i] for i in node_list])/len(node_list)\n",
    "        return loss_val\n",
    "    return loss\n",
    "\n",
    "def loss_gen_avg_sep(node):\n",
    "    def loss(output):\n",
    "        loss_val = output[0][node]\n",
    "        return loss_val\n",
    "    return loss\n",
    "\n",
    "def loss_gen_separate(node_list):\n",
    "    def loss(output):\n",
    "        loss_val = tuple([output[ind][i] for ind,i in enumerate(node_list)])\n",
    "        return loss_val\n",
    "    return loss\n",
    "\n",
    "def model_modifier_layer1(current_model):\n",
    "    target_layer = current_model.get_layer(name=\"dense_1\")\n",
    "    new_model = tf.keras.Model(inputs=current_model.inputs,\n",
    "                               outputs=target_layer.output)\n",
    "    new_model.layers[-1].activation = tf.keras.activations.linear\n",
    "    return new_model\n",
    "\n",
    "def model_modifier_layer2(current_model):\n",
    "    target_layer = current_model.get_layer(name=\"dense_2\")\n",
    "    new_model = tf.keras.Model(inputs=current_model.inputs,\n",
    "                               outputs=target_layer.output)\n",
    "    new_model.layers[-1].activation = tf.keras.activations.linear\n",
    "    return new_model\n",
    "\n",
    "def model_modifier_layer3(current_model):\n",
    "    target_layer = current_model.get_layer(name=\"dense_3\")\n",
    "    new_model = tf.keras.Model(inputs=current_model.inputs,\n",
    "                               outputs=target_layer.output)\n",
    "    new_model.layers[-1].activation = tf.keras.activations.linear\n",
    "    return new_model\n",
    "\n",
    "def model_modifier_last_layer(m):\n",
    "    m.layers[-1].activation = tf.keras.activations.linear\n",
    "    return m\n",
    "\n",
    "def loss_crosstrack(output):\n",
    "    return (output[0][0])\n",
    "\n",
    "def loss_heading(output):\n",
    "    return (output[0][1])\n",
    "\n",
    "def no_loss(output):\n",
    "    return (output[0][0] * 0)\n",
    "\n",
    "\n",
    "gradcam1 = GradcamPlusPlus(model,\n",
    "                    model_modifier=model_modifier_layer1,\n",
    "                    clone=True)\n",
    "gradcam2 = GradcamPlusPlus(model,\n",
    "                    model_modifier=model_modifier_layer2,\n",
    "                    clone=True)\n",
    "gradcam3 = GradcamPlusPlus(model,\n",
    "                    model_modifier=model_modifier_layer3,\n",
    "                    clone=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_invariants_all_labels_vals(all_invariants,prevlayer,layer,suffixes,label):\n",
    "\n",
    "    min_support = 0\n",
    "    \n",
    "    subplot_args = { 'nrows': 1, 'ncols': 4, 'figsize': (12, 12),\n",
    "                 'subplot_kw': {'xticks': [], 'yticks': []} }\n",
    "    subplot_args2 = { 'nrows': 1, 'ncols': 3, 'figsize': (12, 12),\n",
    "                 'subplot_kw': {'xticks': [], 'yticks': []} }\n",
    "\n",
    "    print(\"\\n PRINTING PURE RULES WITH SUPPORT MORE THAN {}:\".format(min_support));\n",
    "    #Pstr = \"\"\n",
    "    for cl, invs in all_invariants.items():\n",
    "       \n",
    "        if ((cl == -1)): # (label == -1) and \n",
    "            continue\n",
    "    \n",
    "        for indx in range (0, len(invs)):\n",
    "          \n",
    "            inv = invs[indx]\n",
    "           \n",
    "            ## GET THE LIST OF INDICES OF INPUTS THAT SATISFY THE RULE\n",
    "            cls = get_suffix_cluster_vals(inv[0],inv[1],suffixes)\n",
    "      \n",
    "            neurons = inv[0]\n",
    "            signature = inv[1]\n",
    "\n",
    "                \n",
    "            Pstr = \"\"\n",
    "            Pstr = Pstr + str(\"Class:\" + str(cl) + \", Rule:(neurons:\" + str(inv[0])  + \",signature:\" + str(inv[1]) +\"), Support:\" + str(inv[2]));\n",
    "            ## PRINT DETAILS REG THE RULE\n",
    "            print(Pstr)\n",
    "            \n",
    "            ### FOR RULES FOR CLASS 1 (FEATURE PRESENT) , HIGHLIGHT PIXELS THAT IMPACT THE RULE USING GRADCAM.\n",
    "            ### THIS IS A STEP TO VISUALLY VALIDATE IF THE RULE TRULY CORRESPONDS TO THE FEATURE\n",
    "            if (cl != 1 or layer == \"ALL\"):\n",
    "                continue\n",
    "            heatmap_sequence = list()\n",
    "            sum_cam = np.zeros((1,200,360),dtype=float)\n",
    "            cnt_cam = 0\n",
    "            for cls_indx in range(0,len(cls)):\n",
    "                img_indx = cls[cls_indx]\n",
    "                img = x_test_seq[img_indx]\n",
    "                ops = y_test_seq[img_indx]\n",
    "                img_mod = np.expand_dims(img/255.0,axis=0)\n",
    "                \n",
    "                if (layer == \"dense_1\"):\n",
    "                    cam = gradcam1(loss_gen_sum(inv[0]),img_mod,penultimate_layer=-1)\n",
    "                if (layer == \"dense_2\"):\n",
    "                    cam = gradcam1(loss_gen_sum(inv[0]),img_mod,penultimate_layer=-1)\n",
    "                if (layer == \"dense_3\"):\n",
    "                    cam = gradcam1(loss_gen_sum(inv[0]),img_mod,penultimate_layer=-1)\n",
    "                    \n",
    "                cam = normalize(cam)\n",
    "                sum_cam = sum_cam + cam\n",
    "                cnt_cam = cnt_cam + 1\n",
    "                ## PRINT THE SALIENCY MAP FOR EVERY 5TH IMAGE\n",
    "                if (cls_indx % 5 == 0):\n",
    "                    act_op = model.predict(np.expand_dims(img/255.0,axis=0))\n",
    "                    \n",
    "                    err_cte = np.squeeze((ops[0])-(act_op[0][0]*8.0))\n",
    "                    err_he = np.squeeze((ops[1])-(act_op[0][1]*35.0))\n",
    "                    #print(\"IDEAL - ACT OPS:\", np.abs(np.round(err_cte,decimals=2)), np.abs(np.round(err_he,decimals=2)) )\n",
    "                    \n",
    "                    f1, ax1 = plt.subplots(**subplot_args)\n",
    "                    ax1[0].set_title('IMAGE'+str(indx)+'_'+str(cls_indx))\n",
    "                    ax1[0].imshow(img)\n",
    "                 #   ax1[1].set_title('CAM '+str(np.abs(np.round(err_cte,decimals=2))) + '_' + str(np.abs(np.round(err_he,decimals=2))))\n",
    "                    ax1[1].set_title('CAM ')\n",
    "                    ax1[1].imshow(cam[0], cmap='jet', alpha=0.5)\n",
    "                    \n",
    "                \n",
    "                    \n",
    "                    \n",
    "            avg_cam = sum_cam/float(cnt_cam)\n",
    "            f2, ax2 = plt.subplots(**subplot_args)\n",
    "            ax2[0].set_title('AVG CAM FOR RULE '+str(indx))\n",
    "            ax2[0].imshow(avg_cam[0], cmap='jet', alpha=0.5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## PRINT DETAILS OF THE EXTRACTED RULES AFTER DENSE_1\n",
    "###################################################\n",
    "\n",
    "\n",
    "#invariants1 = get_all_invariants_vals(basic_estimator1)\n",
    "#layer = 'dense_1'\n",
    "#print(\"TEST DATA RULES FOR PROP \")\n",
    "#describe_invariants_all_labels_vals(invariants1, None, layer, suffixes = test_suffixes1, label = test_predictions1)\n",
    "\n",
    "invariants1F = get_all_invariants_vals(basic_estimator1F)\n",
    "layer = 'dense_1'\n",
    "print(\"TEST DATA RULES FOR FEATURE  \")\n",
    "describe_invariants_all_labels_vals(invariants1F, None, layer, suffixes = test_suffixes1F, label = test_predictions1F)\n",
    "\n",
    "#invariants1FT = get_all_invariants_vals(basic_estimator1FT)\n",
    "#layer = 'dense_1'\n",
    "#print(\"TEST DATA RULES FOR FEATURE AND PROP  \")\n",
    "#describe_invariants_all_labels_vals(invariants1FT, None, layer, suffixes = test_suffixes1FT, label = test_predictions1FT)\n",
    "\n",
    "#invariants1CI = get_all_invariants_vals(basic_estimator1CI)\n",
    "#layer = 'dense_1'\n",
    "#print(\"TEST DATA RULES FOR PROP ON SMALLER SET \")\n",
    "#describe_invariants_all_labels_vals(invariants1CI, None, layer, suffixes = test_suffixes1CI, label = test_predictions1CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRINT DETAILS OF THE EXTRACTED RULES AFTER DENSE_2\n",
    "###################################################\n",
    "\n",
    "#invariants2 = get_all_invariants_vals(basic_estimator2)\n",
    "#layer = 'dense_2'\n",
    "#print(\"TEST DATA RULES FOR PROP \")\n",
    "#describe_invariants_all_labels_vals(invariants2, None, layer, suffixes = test_suffixes2, label = test_predictions2)\n",
    "\n",
    "invariants2F = get_all_invariants_vals(basic_estimator2F)\n",
    "layer = 'dense_2'\n",
    "print(\"TEST DATA RULES FOR FEATURE  \")\n",
    "describe_invariants_all_labels_vals(invariants2F, None, layer, suffixes = test_suffixes2F, label = test_predictions2F)\n",
    "\n",
    "#invariants2FT = get_all_invariants_vals(basic_estimator2FT)\n",
    "#layer = 'dense_2'\n",
    "#print(\"TEST DATA RULES FOR FEATURE AND PROP  \")\n",
    "#describe_invariants_all_labels_vals(invariants2FT, None, layer, suffixes = test_suffixes2FT, label = test_predictions2FT)\n",
    "\n",
    "#invariants2CI = get_all_invariants_vals(basic_estimator2CI)\n",
    "#layer = 'dense_2'\n",
    "#print(\"TEST DATA RULES FOR PROP ON SMALLER SET \")\n",
    "#describe_invariants_all_labels_vals(invariants2CI, None, layer, suffixes = test_suffixes2CI, label = test_predictions2CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRINT DETAILS OF THE EXTRACTED RULES AFTER DENSE_3\n",
    "###################################################\n",
    "\n",
    "#invariants3 = get_all_invariants_vals(basic_estimator3)\n",
    "#layer = 'dense_3'\n",
    "#print(\"TEST DATA RULES FOR PROP \")\n",
    "#describe_invariants_all_labels_vals(invariants3, None, layer, suffixes = test_suffixes3, label = test_predictions3)\n",
    "\n",
    "invariants3F = get_all_invariants_vals(basic_estimator3F)\n",
    "layer = 'dense_3'\n",
    "print(\"TEST DATA RULES FOR FEATURE  \")\n",
    "describe_invariants_all_labels_vals(invariants3F, None, layer, suffixes = test_suffixes3F, label = test_predictions3F)\n",
    "\n",
    "#invariants3FT = get_all_invariants_vals(basic_estimator3FT)\n",
    "#layer = 'dense_3'\n",
    "#print(\"TEST DATA RULES FOR FEATURE AND PROP  \")\n",
    "#describe_invariants_all_labels_vals(invariants3FT, None, layer, suffixes = test_suffixes3FT, label = test_predictions3FT)\n",
    "\n",
    "#invariants3CI = get_all_invariants_vals(basic_estimator3CI)\n",
    "#layer = 'dense_3'\n",
    "#print(\"TEST DATA RULES FOR PROP ON SMALLER SET \")\n",
    "#describe_invariants_all_labels_vals(invariants3CI, None, layer, suffixes = test_suffixes3CI, label = test_predictions3CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRINT DETAILS OF THE EXTRACTED RULES USING ALL LAYERS\n",
    "###################################################\n",
    "\n",
    "#invariantsA = get_all_invariants_vals(basic_estimatorA)\n",
    "#layer = 'ALL'\n",
    "#print(\"TEST DATA RULES FOR PROP \")\n",
    "#describe_invariants_all_labels_vals(invariantsA, None, layer, suffixes = test_suffixesA, label = test_predictionsA)\n",
    "\n",
    "invariantsAF = get_all_invariants_vals(basic_estimatorAF)\n",
    "layer = 'ALL'\n",
    "print(\"TEST DATA RULES FOR FEATURE  \")\n",
    "describe_invariants_all_labels_vals(invariantsAF, None, layer, suffixes = test_suffixesAF, label = test_predictionsAF)\n",
    "\n",
    "#invariantsAFT = get_all_invariants_vals(basic_estimatorAFT)\n",
    "#layer = 'ALL'\n",
    "#print(\"TEST DATA RULES FOR FEATURE AND PROP  \")\n",
    "#describe_invariants_all_labels_vals(invariantsAFT, None, layer, suffixes = test_suffixesAFT, label = test_predictionsAFT)\n",
    "\n",
    "#invariantsACI = get_all_invariants_vals(basic_estimatorACI)\n",
    "#layer = 'ALL'\n",
    "#print(\"TEST DATA RULES FOR PROP ON SMALLER SET \")\n",
    "#describe_invariants_all_labels_vals(invariantsACI, None, layer, suffixes = test_suffixesACI, label = test_predictionsACI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pattern_inter(layer_vals,suff,neuron_ids,neuron_sig,VAL = True,ALL=False):\n",
    "   \n",
    "   if (VAL == False):\n",
    "     if ((suff[:,neuron_ids][0] == neuron_sig).all(axis=0)):\n",
    "        return True\n",
    "     else:\n",
    "        return False\n",
    "\n",
    "   \n",
    "   found = True\n",
    "   oper = -1\n",
    "   layer_vals = (layer_vals).flatten()\n",
    "   \n",
    "   for ix in range(0,len(neuron_ids)):\n",
    "     found = True\n",
    "     for ind in range(0,len(neuron_ids[ix])):\n",
    "       if (ind % 2 == 0):\n",
    "         op = neuron_sig[ix][ind]\n",
    "         if (op == '<='):\n",
    "           oper = 0\n",
    "         else:\n",
    "           oper = 1\n",
    "       else:\n",
    "         v = neuron_ids[ix][ind]\n",
    "         vsig = neuron_sig[ix][ind]\n",
    "         val = layer_vals[v]\n",
    "         #print(oper, v, vsig, val)\n",
    "         if (oper == 0):\n",
    "           if (val > vsig):\n",
    "             found = False\n",
    "             break\n",
    "         else:\n",
    "           if (val <= vsig):\n",
    "            found = False\n",
    "            break\n",
    "         oper = -1\n",
    "     if (found == True):\n",
    "        break \n",
    "\n",
    "   if (found == False):\n",
    "      return -1\n",
    "   else:\n",
    "     return ix \n",
    "    \n",
    "def correlate(invariants1,clas1,invariants2,clas2,val_suffixes,val_preds_prop,val_preds_feat):\n",
    "  \n",
    "    neurons_list1 = []\n",
    "    neurons_sig1 = []\n",
    "    for cl, invs in invariants1.items():\n",
    "        if ((cl == -1) or (clas1 != cl)): # (label == -1) and \n",
    "            continue\n",
    "       \n",
    "      \n",
    "        for indx in range (0, len(invs)):\n",
    "           \n",
    "            inv = invs[indx]    \n",
    "            neurons = []\n",
    "            for indx in range(0,len(inv[0])):\n",
    "                neurons.append(-1)\n",
    "                neurons.append(inv[0][indx])\n",
    "\n",
    "            neurons_list1.append(neurons)\n",
    "            neurons_sig1.append(inv[1])\n",
    "        \n",
    "        \n",
    "    neurons_list2= []\n",
    "    neurons_sig2= []\n",
    "    for cl, invs in invariants2.items():\n",
    "        if ((cl == -1) or (clas2!= cl)): # (label == -1) and \n",
    "            continue\n",
    "       \n",
    "      \n",
    "        for indx in range (0, len(invs)):\n",
    "           \n",
    "            inv = invs[indx]    \n",
    "            neurons = []\n",
    "            for indx in range(0,len(inv[0])):\n",
    "                neurons.append(-1)\n",
    "                neurons.append(inv[0][indx])\n",
    "\n",
    "            neurons_list2.append(neurons)\n",
    "            neurons_sig2.append(inv[1])\n",
    "    \n",
    "    print(len(neurons_list1),len(neurons_list2))\n",
    "    feature_cnt = 0\n",
    "    prop_cnt = 0\n",
    "    for indx1 in range(0,len(val_suffixes)):\n",
    "            prop = val_preds_prop[indx1]\n",
    "            feat = val_preds_feat[indx1]\n",
    "            match1 = check_pattern_inter(val_suffixes[indx1],val_suffixes[indx1],neurons_list1,neurons_sig1)\n",
    "            match2 = check_pattern_inter(val_suffixes[indx1],val_suffixes[indx1],neurons_list2,neurons_sig2)\n",
    "\n",
    "            if (match1 != -1):\n",
    "                    if (feat == clas1):\n",
    "                        feature_cnt = feature_cnt + 1\n",
    "            \n",
    "                        if (match2 != -1):\n",
    "                            if (prop == clas2):\n",
    "                                prop_cnt = prop_cnt + 1\n",
    "         \n",
    "    if (feature_cnt == 0):\n",
    "        print(\"% of Feature CNT in Prop CNT:\", 0)\n",
    "    else:\n",
    "        print(\"% of Feature CNT in Prop CNT:\", float(prop_cnt/feature_cnt)*100)\n",
    "                \n",
    "         \n",
    "\n",
    "def validate(invariants,clas,val_suffixes,val_preds_prop,val_preds_feat,Feature=-1,Prop=-1):\n",
    "  \n",
    "   \n",
    "    for cl, invs in invariants.items():\n",
    "        if ((cl == -1) or (clas != cl)): # (label == -1) and \n",
    "            continue\n",
    "       \n",
    "      \n",
    "        for indx in range (0, len(invs)):\n",
    "           \n",
    "            inv = invs[indx]    \n",
    "            neurons = []\n",
    "            for indx in range(0,len(inv[0])):\n",
    "                neurons.append(-1)\n",
    "                neurons.append(inv[0][indx])\n",
    "\n",
    "    \n",
    "            recall = 0\n",
    "            prec = 0\n",
    "            for indx1 in range(0,len(val_suffixes)):\n",
    "                match = check_pattern_inter(val_suffixes[indx1],val_suffixes[indx1],[neurons],[inv[1]])\n",
    "                if (match != -1):\n",
    "                    recall = recall + 1\n",
    "                    prop = val_preds_prop[indx1]\n",
    "                    feat = val_preds_feat[indx1]\n",
    "                    if (Feature == 1):\n",
    "                        if (feat != 1):\n",
    "                            if (Prop == -1):\n",
    "                                prec = prec + 1\n",
    "                            else:\n",
    "                                if (Prop == 1):\n",
    "                                    if (prop == 1):\n",
    "                                        prec= prec + 1\n",
    "                                else:\n",
    "                                    if (prop == 0):\n",
    "                                        prec = prec + 1\n",
    "                        continue\n",
    "                                    \n",
    "                    if (Feature == 0):\n",
    "                        if (feat != 0):\n",
    "                            if (Prop == -1):\n",
    "                                prec = prec + 1\n",
    "                            else:\n",
    "                                if (Prop == 1):\n",
    "                                    if (prop == 1):\n",
    "                                        prec= prec+ 1\n",
    "                                else:\n",
    "                                    if (prop == 0):\n",
    "                                        prec = prec+ 1\n",
    "\n",
    "                        continue\n",
    "                                    \n",
    "                    if (Prop == 1):\n",
    "                        if (prop != 0):\n",
    "                            if (Feature == -1):\n",
    "                                prec = prec + 1\n",
    "                            else:\n",
    "                                if (Feature == 1):\n",
    "                                    if (feat == 0):\n",
    "                                        prec = prec + 1\n",
    "                                else:\n",
    "                                    if (feat == 1):\n",
    "                                        prec = prec + 1\n",
    "                        continue\n",
    "                    \n",
    "                    if (Prop == 0):\n",
    "                        if (prop != 1):\n",
    "                            if (Feature == -1):\n",
    "                                prec = prec + 1\n",
    "                            else:\n",
    "                                if (Feature == 1):\n",
    "                                    if (feat == 0):\n",
    "                                        prec = prec + 1\n",
    "                                else:\n",
    "                                    if (feat == 1):\n",
    "                                        prec = prec + 1\n",
    "                        continue\n",
    "                                    \n",
    "            \n",
    "            if (recall == 0):\n",
    "                print(\"CLASS;\", cl, inv, \";Precision:\",0,\";Recall:\",0)\n",
    "            else:\n",
    "                print(\"CLASS;\", cl, inv, \";Precision:\",float(prec/recall)*100,\";Recall:\",float(recall/len(val_suffixes))*100)\n",
    "            \n",
    "\n",
    "print(\"### LAYER 1\")\n",
    "print(\"Validate Invariants for Feature SAT\")       \n",
    "print(\"-----------\")\n",
    "validate(invariants1F,0,val_suffixes1F,val_predictions1CI,val_predictions1F,Feature=1,Prop=-1)      \n",
    "print(\"-----------\")\n",
    "print(\"Validate Invariants for Feature UNSAT\")     \n",
    "print(\"-----------\")\n",
    "validate(invariants1F,1,val_suffixes1F,val_predictions1CI,val_predictions1F,Feature=0,Prop=-1) \n",
    "\n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Prop SAT\")  \n",
    "#print(\"-----------\")\n",
    "#validate(invariants1CI,1,val_suffixes1F,val_predictions1CI,val_predictions1F,Feature=-1,Prop=1)      \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Prop UNSAT\")     \n",
    "#print(\"-----------\")\n",
    "#validate(invariants1CI,0,val_suffixes1F,val_predictions1CI,val_predictions1F,Feature=-1,Prop=0) \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature SAT and Prop SAT\")     \n",
    "#print(\"-----------\")\n",
    "#validate(invariants1FT,0,val_suffixes1F,val_predictions1CI,val_predictions1F,Feature=1,Prop=1)      \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature UNSAT and Prop SAT\")      \n",
    "#print(\"-----------\")\n",
    "#validate(invariants1FT,1,val_suffixes1F,val_predictions1CI,val_predictions1F,Feature=0,Prop=1) \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature SAT and Prop UNSAT\")      \n",
    "#print(\"-----------\")\n",
    "#validate(invariants1FT,2,val_suffixes1F,val_predictions1CI,val_predictions1F,Feature=1,Prop=0) \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature UNSAT and Prop UNSAT\") \n",
    "#print(\"-----------\")\n",
    "#validate(invariants1FT,3,val_suffixes1F,val_predictions1CI,val_predictions1F,Feature=0,Prop=0) \n",
    "#print(\"-----------\")\n",
    "\n",
    "print(\"#### LAYER 2\")\n",
    "print(\"Validate Invariants for Feature SAT\")       \n",
    "print(\"-----------\")\n",
    "validate(invariants2F,0,val_suffixes2F,val_predictions2CI,val_predictions2F,Feature=1,Prop=-1)      \n",
    "print(\"-----------\")\n",
    "print(\"Validate Invariants for Feature UNSAT\")     \n",
    "print(\"-----------\")\n",
    "validate(invariants2F,1,val_suffixes2F,val_predictions2CI,val_predictions2F,Feature=0,Prop=-1) \n",
    "\n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Prop SAT\")  \n",
    "#print(\"-----------\")\n",
    "#validate(invariants2CI,1,val_suffixes2F,val_predictions2CI,val_predictions2F,Feature=-1,Prop=1)      \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Prop UNSAT\")     \n",
    "#print(\"-----------\")\n",
    "#validate(invariants2CI,0,val_suffixes2F,val_predictions2CI,val_predictions2F,Feature=-1,Prop=0) \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature SAT and Prop SAT\")     \n",
    "#print(\"-----------\")\n",
    "#validate(invariants2FT,0,val_suffixes2F,val_predictions2CI,val_predictions2F,Feature=1,Prop=1)      \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature UNSAT and Prop SAT\")      \n",
    "#print(\"-----------\")\n",
    "#validate(invariants2FT,1,val_suffixes2F,val_predictions2CI,val_predictions2F,Feature=0,Prop=1) \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature SAT and Prop UNSAT\")      \n",
    "#print(\"-----------\")\n",
    "#validate(invariants2FT,2,val_suffixes2F,val_predictions2CI,val_predictions2F,Feature=1,Prop=0) \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature UNSAT and Prop UNSAT\") \n",
    "#print(\"-----------\")\n",
    "#validate(invariants2FT,3,val_suffixes2F,val_predictions2CI,val_predictions2F,Feature=0,Prop=0) \n",
    "#print(\"-----------\")\n",
    "\n",
    "print(\"### LAYER 3\")\n",
    "print(\"Validate Invariants for Feature SAT\")       \n",
    "print(\"-----------\")\n",
    "validate(invariants3F,0,val_suffixes3F,val_predictions3CI,val_predictions3F,Feature=1,Prop=-1)      \n",
    "print(\"-----------\")\n",
    "print(\"Validate Invariants for Feature UNSAT\")     \n",
    "print(\"-----------\")\n",
    "validate(invariants3F,1,val_suffixes3F,val_predictions3CI,val_predictions3F,Feature=0,Prop=-1) \n",
    "\n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Prop SAT\")  \n",
    "#print(\"-----------\")\n",
    "#validate(invariants3CI,1,val_suffixes3F,val_predictions3CI,val_predictions3F,Feature=-1,Prop=1)      \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Prop UNSAT\")     \n",
    "#print(\"-----------\")\n",
    "#validate(invariants3CI,0,val_suffixes3F,val_predictions3CI,val_predictions3F,Feature=-1,Prop=0) \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature SAT and Prop SAT\")     \n",
    "#print(\"-----------\")\n",
    "#validate(invariants3FT,0,val_suffixes3F,val_predictions3CI,val_predictions3F,Feature=1,Prop=1)      \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature UNSAT and Prop SAT\")      \n",
    "#print(\"-----------\")\n",
    "#validate(invariants3FT,1,val_suffixes3F,val_predictions3CI,val_predictions3F,Feature=0,Prop=1) \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature SAT and Prop UNSAT\")      \n",
    "#print(\"-----------\")\n",
    "#validate(invariants3FT,2,val_suffixes3F,val_predictions3CI,val_predictions3F,Feature=1,Prop=0) \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature UNSAT and Prop UNSAT\") \n",
    "#print(\"-----------\")\n",
    "#validate(invariants3FT,3,val_suffixes3F,val_predictions3CI,val_predictions3F,Feature=0,Prop=0) \n",
    "#print(\"-----------\")\n",
    "\n",
    "print(\"### LAYER ALL\")\n",
    "print(\"Validate Invariants for Feature SAT\")       \n",
    "print(\"-----------\")\n",
    "validate(invariantsAF,0,val_suffixesAF,val_predictionsACI,val_predictionsAF,Feature=1,Prop=-1)      \n",
    "print(\"-----------\")\n",
    "print(\"Validate Invariants for Feature UNSAT\")     \n",
    "print(\"-----------\")\n",
    "validate(invariantsAF,1,val_suffixesAF,val_predictionsACI,val_predictionsAF,Feature=0,Prop=-1) \n",
    "\n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Prop SAT\")  \n",
    "#print(\"-----------\")\n",
    "#validate(invariantsACI,1,val_suffixesAF,val_predictionsACI,val_predictionsAF,Feature=-1,Prop=1)      \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Prop UNSAT\")     \n",
    "#print(\"-----------\")\n",
    "#validate(invariantsACI,0,val_suffixesAF,val_predictionsACI,val_predictionsAF,Feature=-1,Prop=0) \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature SAT and Prop SAT\")     \n",
    "#print(\"-----------\")\n",
    "#validate(invariantsAFT,0,val_suffixesAF,val_predictionsACI,val_predictionsAF,Feature=1,Prop=1)      \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature UNSAT and Prop SAT\")      \n",
    "#print(\"-----------\")\n",
    "#validate(invariantsAFT,1,val_suffixesAF,val_predictionsACI,val_predictionsAF,Feature=0,Prop=1) \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature SAT and Prop UNSAT\")      \n",
    "#print(\"-----------\")\n",
    "#validate(invariantsAFT,2,val_suffixesAF,val_predictionsACI,val_predictionsAF,Feature=1,Prop=0) \n",
    "#print(\"-----------\")\n",
    "#print(\"Validate Invariants for Feature UNSAT and Prop UNSAT\") \n",
    "#print(\"-----------\")\n",
    "#validate(invariantsAFT,3,val_suffixesAF,val_predictionsACI,val_predictionsAF,Feature=0,Prop=0) \n",
    "#print(\"-----------\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:div-env]",
   "language": "python",
   "name": "conda-env-div-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
