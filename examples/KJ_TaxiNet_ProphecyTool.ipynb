{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IARS2WHWJWTo"
      },
      "source": [
        "**INSTALLATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bKcRdNC2sBR",
        "outputId": "433df8d5-27b0-4107-922a-41b267b15cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ProphecyPlus' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone --recurse-submodules https://github.com/safednn-nasa/ProphecyPlus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_mx5eC_3XLz",
        "outputId": "6cf8cf5e-0de2-41a0-88a8-e203aa2ee56f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ProphecyPlus\n",
            "/content/ProphecyPlus\n",
            "total 108\n",
            "-rw-r--r-- 1 root root  4902 Dec 30 16:22 KJ_TinyTaxiNet.onnx\n",
            "-rw-r--r-- 1 root root     1 Dec 30 16:22 readme.txt\n",
            "-rw-r--r-- 1 root root  5377 Dec 30 16:22 saved_model.pb\n",
            "-rw-r--r-- 1 root root 45896 Dec 30 16:22 KJ_Taxinet.h5\n",
            "-rw-r--r-- 1 root root 30274 Dec 30 16:22 KJ_TaxiNet.nnet\n",
            "-rw-r--r-- 1 root root  5377 Dec 30 16:22 KJ_TaxiNet.pb\n"
          ]
        }
      ],
      "source": [
        "%cd ./ProphecyPlus\n",
        "!pwd\n",
        "!ls -lt /content/ProphecyPlus/dataset_models/kj_tiny_taxinet/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aD6Pxkus2x1w",
        "outputId": "103a46d2-48e0-4c91-89fd-8c50e78773c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy~=1.23.5 (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting pandas~=1.3.5 (from -r requirements.txt (line 2))\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting scikit-learn~=1.3.0 (from -r requirements.txt (line 3))\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy~=1.9.3 (from -r requirements.txt (line 4))\n",
            "  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow~=2.14.0 (from -r requirements.txt (line 5))\n",
            "  Downloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting keras~=2.14.0 (from -r requirements.txt (line 6))\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting pybind11~=2.11.1 (from -r requirements.txt (line 7))\n",
            "  Downloading pybind11-2.11.2-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting tqdm~=4.66.1 (from -r requirements.txt (line 8))\n",
            "  Downloading tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas~=1.3.5->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas~=1.3.5->-r requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn~=1.3.0->-r requirements.txt (line 3)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn~=1.3.0->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (18.1.1)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow~=2.14.0->-r requirements.txt (line 5))\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow~=2.14.0->-r requirements.txt (line 5))\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->-r requirements.txt (line 5)) (1.68.1)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow~=2.14.0->-r requirements.txt (line 5))\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow~=2.14.0->-r requirements.txt (line 5))\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.14.0->-r requirements.txt (line 5)) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5))\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->-r requirements.txt (line 5)) (3.2.2)\n",
            "Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.11.2-py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.3/229.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: wrapt, tqdm, tensorflow-estimator, pybind11, numpy, keras, scipy, pandas, ml-dtypes, scikit-learn, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.0\n",
            "    Uninstalling wrapt-1.17.0:\n",
            "      Successfully uninstalled wrapt-1.17.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.0\n",
            "    Uninstalling scikit-learn-1.6.0:\n",
            "      Successfully uninstalled scikit-learn-1.6.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 1.4.20 requires scipy>=1.10.0, but you have scipy 1.9.3 which is incompatible.\n",
            "arviz 0.20.0 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "bigframes 1.29.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.29.0 requires pandas>=1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "chex 0.1.88 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.3.5 which is incompatible.\n",
            "geopandas 1.0.1 requires pandas>=1.4.0, but you have pandas 1.3.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.3.5 which is incompatible.\n",
            "ibis-framework 9.2.0 requires pandas<3,>=1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.4.33 requires scipy>=1.10, but you have scipy 1.9.3 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.4.33 requires scipy>=1.10, but you have scipy 1.9.3 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.3.5 which is incompatible.\n",
            "plotnine 0.14.4 requires pandas>=2.2.0, but you have pandas 1.3.5 which is incompatible.\n",
            "pymc 5.19.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.0 requires scipy>=1.11.2, but you have scipy 1.9.3 which is incompatible.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, but you have pandas 1.3.5 which is incompatible.\n",
            "tensorstore 0.1.71 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.14.1 which is incompatible.\n",
            "xarray 2024.11.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2024.11.0 requires pandas>=2.1, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-1.0.0 keras-2.14.0 ml-dtypes-0.2.0 numpy-1.23.5 pandas-1.3.5 pybind11-2.11.2 scikit-learn-1.3.2 scipy-1.9.3 tensorboard-2.14.1 tensorflow-2.14.1 tensorflow-estimator-2.14.0 tqdm-4.66.6 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "44c8bc4b48d249aabbe77a45fed7a4e6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOAD DATA AND MODEL**"
      ],
      "metadata": {
        "id": "c7SpWGj2mA10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Dropbox shared link (ensure it's the correct URL)\n",
        "shared_link = \"https://www.dropbox.com/scl/fi/84wgpm5v7beta2jovs1wf/data_val.zip?rlkey=2dudlkuxqnailr6vysr2v2022&st=49oxo7dz&dl=0\"\n",
        "\n",
        "# Modify the link for direct download\n",
        "direct_download_url = shared_link.replace(\"dl=0\", \"dl=1\")\n",
        "\n",
        "# Filepath to save the downloaded file\n",
        "file_path = \"data_val.zip\"\n",
        "\n",
        "try:\n",
        "    # Send a GET request to download the file\n",
        "    response = requests.get(direct_download_url)\n",
        "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    # Save the content to a local file\n",
        "    with open(file_path, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully and saved as {file_path}\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error occurred: {e}\")\n",
        "\n",
        "############################\n",
        "!ls -lt ./data_val.zip\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('./data_val.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1ZDys1ithxQ",
        "outputId": "67852927-9fd6-4725-a365-e896e4391c73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully and saved as data_val.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import PIL\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import tensorflow as tf\n",
        "#from tensorflow import keras\n",
        "\n",
        "eval_folder = \"./data_val/\"\n",
        "table = pd.read_csv(eval_folder + \"errors.csv\")\n",
        "\n",
        "# Use each example image in folder\n",
        "exampleImages = glob.glob(eval_folder + \"*png\")\n",
        "imgNums = sorted([int( f.split(\"/\")[-1].split(\".\")[0] ) for f in exampleImages])\n",
        "\n",
        "x = list()\n",
        "y = list()\n",
        "for imgNum in imgNums:\n",
        "  img = PIL.Image.open(\"{}{}.png\".format(eval_folder,imgNum))\n",
        "  img_copy = np.array(img)\n",
        "  truth = np.array([table.CTE[imgNum],table.HE[imgNum]])\n",
        "  img.close()\n",
        "\n",
        "  #print(img_copy.shape)\n",
        "  x.append(img_copy)\n",
        "  y.append(truth)\n",
        "\n",
        "\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"Shape of X:\", x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3ntAxyq_qUG",
        "outputId": "2438a698-3799-4666-e2e3-b4a820d080c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (7386, 200, 360, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def downsampleImage(img):\n",
        "    \"\"\"\n",
        "    Function for downsampling images of taxiway from 200x360x3 into 8x16x1\n",
        "    \"\"\"\n",
        "\n",
        "    img = np.array(img)\n",
        "\n",
        "    # Remove yellow/orange lines\n",
        "    mask = ((img[:,:,0].astype('float')-img[:,:,2].astype('float'))>60) & ((img[:,:,1].astype('float')-img[:,:,2].astype('float'))>30)\n",
        "    img[mask] = 0\n",
        "\n",
        "    # Convert to grayscale, crop out nose, sky, bottom of image, resize to 256x128, scale so\n",
        "    # values range between 0 and 1\n",
        "    img = np.array(PIL.Image.fromarray(img).convert('L').crop((55,5,360,140)).resize((256,128)))/255.0\n",
        "\n",
        "    # Downsample image\n",
        "    # Split image into stride x stride boxes, average numPix brightest pixels in that box\n",
        "    # As a result, img2 has one value for every box\n",
        "    img2 = np.zeros((height,width))\n",
        "    for i in range(height):\n",
        "        for j in range(width):\n",
        "            img2[i,j] = np.mean(np.sort(img[stride*i:stride*(i+1),stride*j:stride*(j+1)].reshape(-1))[-numPix:])\n",
        "\n",
        "    # Ensure that the mean of the image is 0.5 and that values range between 0 and 1\n",
        "    # The training data only contains images from sunny, 9am conditions.\n",
        "    # Biasing the image helps the network generalize to different lighting conditions (cloudy, noon, etc)\n",
        "    img2 -= img2.mean()\n",
        "    img2 += 0.5\n",
        "    img2[img2>1] = 1\n",
        "    img2[img2<0] = 0\n",
        "    return img2"
      ],
      "metadata": {
        "id": "MpfH8PjPAs-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "model = keras.models.load_model(\"/content/ProphecyPlus/dataset_models/kj_tiny_taxinet/KJ_Taxinet.h5\")\n",
        "model.summary()\n",
        "\n",
        "for layer in model.layers:\n",
        "  print(\"LAYER NAME:\", layer.name)\n",
        "\n",
        "#model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIh1WaR9rjM3",
        "outputId": "cbf9da51-df6d-49a0-8c24-9a73acd4a375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/lambda_layer.py:327: UserWarning: onnx2keras.utils is not loaded, but a Lambda layer uses it. It may cause errors.\n",
            "  function = cls._parse_function_from_config(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/lambda_layer.py:327: UserWarning: onnx2keras.elementwise_layers is not loaded, but a Lambda layer uses it. It may cause errors.\n",
            "  function = cls._parse_function_from_config(\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"Placeholder:0\", shape=(None, 1, 2, 8), dtype=float32) Tensor(\"Placeholder_1:0\", shape=(8,), dtype=float32)\n",
            "Tensor(\"Placeholder:0\", shape=(None, 8), dtype=float32) Tensor(\"Placeholder_1:0\", shape=(8,), dtype=float32)\n",
            "Tensor(\"Placeholder:0\", shape=(None, 8), dtype=float32) Tensor(\"Placeholder_1:0\", shape=(8,), dtype=float32)\n",
            "Tensor(\"Placeholder:0\", shape=(None, 2), dtype=float32) Tensor(\"Placeholder_1:0\", shape=(2,), dtype=float32)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_0 (InputLayer)        [(None, 8, 16, 1)]           0         []                            \n",
            "                                                                                                  \n",
            " layer_0_ (Permute)          (None, 1, 8, 16)             0         ['input_0[0][0]']             \n",
            "                                                                                                  \n",
            " layer_1_ (Conv2D)           (None, 8, 1, 2)              512       ['layer_0_[0][0]']            \n",
            "                                                                                                  \n",
            " layer_2_ (Permute)          (None, 1, 2, 8)              0         ['layer_1_[0][0]']            \n",
            "                                                                                                  \n",
            " layer_3__const2 (Lambda)    (8,)                         0         ['input_0[0][0]']             \n",
            "                                                                                                  \n",
            " layer_3_ (Lambda)           (None, 1, 2, 8)              0         ['layer_2_[0][0]',            \n",
            "                                                                     'layer_3__const2[0][0]']     \n",
            "                                                                                                  \n",
            " layer_4_ (Activation)       (None, 1, 2, 8)              0         ['layer_3_[0][0]']            \n",
            "                                                                                                  \n",
            " layer_6_ (Reshape)          (None, 16)                   0         ['layer_4_[0][0]']            \n",
            "                                                                                                  \n",
            " layer_7_ (Dense)            (None, 8)                    128       ['layer_6_[0][0]']            \n",
            "                                                                                                  \n",
            " layer_8__const2 (Lambda)    (8,)                         0         ['input_0[0][0]']             \n",
            "                                                                                                  \n",
            " layer_8_ (Lambda)           (None, 8)                    0         ['layer_7_[0][0]',            \n",
            "                                                                     'layer_8__const2[0][0]']     \n",
            "                                                                                                  \n",
            " layer_9_ (Activation)       (None, 8)                    0         ['layer_8_[0][0]']            \n",
            "                                                                                                  \n",
            " layer_10 (Dense)            (None, 8)                    64        ['layer_9_[0][0]']            \n",
            "                                                                                                  \n",
            " layer_11_const2 (Lambda)    (8,)                         0         ['input_0[0][0]']             \n",
            "                                                                                                  \n",
            " layer_11 (Lambda)           (None, 8)                    0         ['layer_10[0][0]',            \n",
            "                                                                     'layer_11_const2[0][0]']     \n",
            "                                                                                                  \n",
            " layer_12 (Activation)       (None, 8)                    0         ['layer_11[0][0]']            \n",
            "                                                                                                  \n",
            " layer_13 (Dense)            (None, 2)                    16        ['layer_12[0][0]']            \n",
            "                                                                                                  \n",
            " layer_14_const2 (Lambda)    (2,)                         0         ['input_0[0][0]']             \n",
            "                                                                                                  \n",
            " layer_14 (Lambda)           (None, 2)                    0         ['layer_13[0][0]',            \n",
            "                                                                     'layer_14_const2[0][0]']     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 720 (2.81 KB)\n",
            "Trainable params: 720 (2.81 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "LAYER NAME: input_0\n",
            "LAYER NAME: layer_0_\n",
            "LAYER NAME: layer_1_\n",
            "LAYER NAME: layer_2_\n",
            "LAYER NAME: layer_3__const2\n",
            "LAYER NAME: layer_3_\n",
            "LAYER NAME: layer_4_\n",
            "LAYER NAME: layer_6_\n",
            "LAYER NAME: layer_7_\n",
            "LAYER NAME: layer_8__const2\n",
            "LAYER NAME: layer_8_\n",
            "LAYER NAME: layer_9_\n",
            "LAYER NAME: layer_10\n",
            "LAYER NAME: layer_11_const2\n",
            "LAYER NAME: layer_11\n",
            "LAYER NAME: layer_12\n",
            "LAYER NAME: layer_13\n",
            "LAYER NAME: layer_14_const2\n",
            "LAYER NAME: layer_14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### IMPORTANT PARAMETERS FOR IMAGE PROCESSING ###\n",
        "stride = 16             # Size of square of pixels downsampled to one grayscale value\n",
        "numPix = 16             # During downsampling, average the numPix brightest pixels in each square\n",
        "width  = 256//stride    # Width of downsampled grayscale image\n",
        "height = 128//stride    # Height of downsampled grayscale image\n",
        "#################################################\n",
        "\n",
        "print(\"height:\", height, \",width:\", width)\n",
        "\n",
        "x_train = x[0:int(0.80*len(x))]\n",
        "y_train = y[0:int(0.80*len(y))]\n",
        "\n",
        "print(\"TRAIN:\",np.shape(x_train), np.shape(y_train))\n",
        "\n",
        "x_test = x[int(0.80*len(x)):]\n",
        "y_test = y[int(0.80*len(y)):]\n",
        "\n",
        "print(\"TEST:\",np.shape(x_test), np.shape(y_test))\n",
        "\n",
        "error_train_ct = []\n",
        "error_train_he = []\n",
        "\n",
        "pred_train_cte = []\n",
        "pred_train_he = []\n",
        "\n",
        "\n",
        "flat_img_keras_lst = []\n",
        "\n",
        "for imgNum, img in enumerate(x_train):\n",
        "  dsImg = downsampleImage(img)\n",
        "  # Compute prediction and ground truth of CTE / HE\n",
        "  #print(\"Img Shape:\", img.shape)\n",
        "  flat_img = dsImg.reshape(-1)\n",
        "  #print(\"Flat Img Shape:\", flat_img.shape)\n",
        "\n",
        "  flat_img_keras = flat_img.reshape(1, height, width, 1)\n",
        "  flat_img_keras_lst.append(flat_img.reshape(height, width, 1))\n",
        "  #print(\"Flat Img Keras Shape:\", flat_img_keras.shape)\n",
        "\n",
        "\n",
        "\n",
        "  predictions = model.predict(flat_img_keras)\n",
        "  #print(\"Prediction Keras:\", predictions)\n",
        "  truth = y_train[imgNum]\n",
        "  #print(\"Truth:\", truth)\n",
        "\n",
        "  error_train_ct.append(predictions[0][0]-truth[0])\n",
        "  error_train_he.append(predictions[0][1]-truth[1])\n",
        "\n",
        "  pred_train_cte.append(np.abs(predictions[0][0]))\n",
        "  pred_train_he.append(np.abs(predictions[0][1]))\n",
        "\n",
        "  # print(\"\\nImg Number: %d\" % imgNum)\n",
        " # print(\"Prediction Keras: %.3f CTE, %.3f HE\" % (predictions[0][0],predictions[0][1]))\n",
        " # print(\"Truth: %.3f CTE, %.3f HE\" % (truth[0],truth[1]))\n",
        "\n",
        "\n",
        "np.save('./x_train_npy.npy', np.array(flat_img_keras_lst))\n",
        "\n",
        "################################################################\n",
        "\n",
        "\n",
        "error_test_ct = []\n",
        "error_test_he = []\n",
        "\n",
        "pred_test_cte = []\n",
        "pred_test_he = []\n",
        "\n",
        "flat_img_keras_lst = []\n",
        "\n",
        "for imgNum, img in enumerate(x_test):\n",
        "  dsImg = downsampleImage(img)\n",
        "  # Compute prediction and ground truth of CTE / HE\n",
        "  #print(\"Img Shape:\", img.shape)\n",
        "  flat_img = dsImg.reshape(-1)\n",
        "  #print(\"Flat Img Shape:\", flat_img.shape)\n",
        "\n",
        "  flat_img_keras = flat_img.reshape(1, height, width, 1)\n",
        "  #print(\"Flat Img Keras Shape:\", flat_img_keras.shape)\n",
        "  flat_img_keras_lst.append(flat_img.reshape(height, width, 1))\n",
        "\n",
        "  predictions = model.predict(flat_img_keras)\n",
        "  #print(\"Prediction Keras:\", predictions)\n",
        "  truth = y_test[imgNum]\n",
        "  #print(\"Truth:\", truth)\n",
        "\n",
        "  error_test_ct.append(predictions[0][0]-truth[0])\n",
        "  error_test_he.append(predictions[0][1]-truth[1])\n",
        "\n",
        "  pred_test_cte.append(np.abs(predictions[0][0]))\n",
        "  pred_test_he.append(np.abs(predictions[0][1]))\n",
        "\n",
        " # print(\"\\nImg Number: %d\" % imgNum)\n",
        " # print(\"Prediction Keras: %.3f CTE, %.3f HE\" % (predictions[0][0],predictions[0][1]))\n",
        " # print(\"Truth: %.3f CTE, %.3f HE\" % (truth[0],truth[1]))\n",
        "np.save('./x_test_npy.npy', np.array(flat_img_keras_lst))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QDiFqioRaAM",
        "outputId": "914d4f72-41c3-4d8e-a1a7-ab825b1e3875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EVALUATE ACCURACY IN TERMS OF MAE ON TRAIN AND TEST DATA**"
      ],
      "metadata": {
        "id": "CXZYLnUhmOT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" TRAIN \")\n",
        "print(\"Cross Track Error\")\n",
        "abs_error_train=np.abs(error_train_ct)\n",
        "print('MeanAbsoluteError: '+str(np.mean(abs_error_train)))\n",
        "print('MedianAbsoluteError: '+str(np.median(abs_error_train)))\n",
        "print('StandardDeviation: '+str(np.std(abs_error_train)))\n",
        "print('MaxAbsoluteError: '+str(np.max(abs_error_train)))\n",
        "\n",
        "print(\" TRAIN \")\n",
        "print(\"Heading Error\")\n",
        "abs_error_train=np.abs(error_train_he)\n",
        "print('MeanAbsoluteError: '+str(np.mean(abs_error_train)))\n",
        "print('MedianAbsoluteError: '+str(np.median(abs_error_train)))\n",
        "print('StandardDeviation: '+str(np.std(abs_error_train)))\n",
        "print('MaxAbsoluteError: '+str(np.max(abs_error_train)))\n",
        "\n",
        "print(\" TEST \")\n",
        "print(\"Cross Track Error\")\n",
        "abs_error_test=np.abs(error_test_ct)\n",
        "print('MeanAbsoluteError: '+str(np.mean(abs_error_test)))\n",
        "print('MedianAbsoluteError: '+str(np.median(abs_error_test)))\n",
        "print('StandardDeviation: '+str(np.std(abs_error_test)))\n",
        "print('MaxAbsoluteError: '+str(np.max(abs_error_test)))\n",
        "\n",
        "print(\" TEST \")\n",
        "print(\"Heading Error\")\n",
        "abs_error_test=np.abs(error_test_he)\n",
        "print('MeanAbsoluteError: '+str(np.mean(abs_error_test)))\n",
        "print('MedianAbsoluteError: '+str(np.median(abs_error_test)))\n",
        "print('StandardDeviation: '+str(np.std(abs_error_test)))\n",
        "print('MaxAbsoluteError: '+str(np.max(abs_error_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJC3USyNOzch",
        "outputId": "a059baba-30c1-4f70-b587-2c051c7c312e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " TRAIN \n",
            "Cross Track Error\n",
            "MeanAbsoluteError: 1.3419898688496428\n",
            "MedianAbsoluteError: 1.1240889876785276\n",
            "StandardDeviation: 1.0314191135748914\n",
            "MaxAbsoluteError: 9.604623790901185\n",
            " TRAIN \n",
            "Heading Error\n",
            "MeanAbsoluteError: 2.790885690451818\n",
            "MedianAbsoluteError: 2.3334284747924805\n",
            "StandardDeviation: 2.397462409773755\n",
            "MaxAbsoluteError: 24.663099521484376\n",
            " TEST \n",
            "Cross Track Error\n",
            "MeanAbsoluteError: 1.811774308290391\n",
            "MedianAbsoluteError: 1.9239859447174066\n",
            "StandardDeviation: 1.1206498607945428\n",
            "MaxAbsoluteError: 5.817006080551147\n",
            " TEST \n",
            "Heading Error\n",
            "MeanAbsoluteError: 2.596350178839501\n",
            "MedianAbsoluteError: 2.1431100562896725\n",
            "StandardDeviation: 2.1977444523675893\n",
            "MaxAbsoluteError: 15.552720513183594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CALCULATE MEAN ABSOLUTE MODEL OUTPUT VALUES FOR CTE AND HE.**\n",
        "\n",
        "**The MEAN ABS output value on train data is taken as the safety threshold.**"
      ],
      "metadata": {
        "id": "yEarR7ZAmZhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"AVERAGE TRAIN CTE:\",np.mean(pred_train_cte))\n",
        "print(\"AVERAGE TRAIN HE:\",np.mean(pred_train_he))\n",
        "print(\"AVERAGE TEST CTE:\",np.mean(pred_test_cte))\n",
        "print(\"AVERAGE TEST HE:\",np.mean(pred_test_he))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnkxP6TJSVKU",
        "outputId": "249d2a33-afb6-499e-ba24-c6e3fb551b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVERAGE TRAIN CTE: 4.276557\n",
            "AVERAGE TRAIN HE: 8.026427\n",
            "AVERAGE TEST CTE: 5.2611675\n",
            "AVERAGE TEST HE: 7.742248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_train_cte = []\n",
        "labels_test_cte = []\n",
        "labels_train_he = []\n",
        "labels_test_he = []\n",
        "\n",
        "for i in range(len(pred_train_cte)):\n",
        "  if (pred_train_cte[i] > 4.276557):\n",
        "    labels_train_cte.append(0)\n",
        "  else:\n",
        "    labels_train_cte.append(1)\n",
        "\n",
        "for i in range(len(pred_test_cte)):\n",
        "  if (pred_test_cte[i] > 4.276557):\n",
        "    labels_test_cte.append(0)\n",
        "  else:\n",
        "    labels_test_cte.append(1)\n",
        "\n",
        "\n",
        "for i in range(len(pred_train_he)):\n",
        "  if (pred_train_he[i] > 8.026427):\n",
        "    labels_train_he.append(0)\n",
        "  else:\n",
        "    labels_train_he.append(1)\n",
        "\n",
        "for i in range(len(pred_test_he)):\n",
        "  if (pred_test_he[i] > 8.026427):\n",
        "    labels_test_he.append(0)\n",
        "  else:\n",
        "    labels_test_he.append(1)\n",
        "\n",
        "\n",
        "print(\"TRAIN CTE 0:\", labels_train_cte.count(0))\n",
        "print(\"TRAIN CTE 1:\", labels_train_cte.count(1))\n",
        "print(\"TEST CTE 0:\", labels_test_cte.count(0))\n",
        "print(\"TEST CTE 1:\", labels_test_cte.count(1))\n",
        "\n",
        "print(\"TRAIN HE 0:\", labels_train_he.count(0))\n",
        "print(\"TRAIN HE 1:\", labels_train_he.count(1))\n",
        "print(\"TEST HE 0:\", labels_test_he.count(0))\n",
        "print(\"TEST HE 1:\", labels_test_he.count(1))\n",
        "\n",
        "\n",
        "np.save('./y_train_cte_npy.npy', np.array(labels_train_cte))\n",
        "np.save('./y_test_cte_npy.npy', np.array(labels_test_cte))\n",
        "\n",
        "np.save('./y_train_he_npy.npy', np.array(labels_train_he))\n",
        "np.save('./y_test_he_npy.npy', np.array(labels_test_he))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GolABpLSV4QX",
        "outputId": "fd68e964-0856-49fc-d106-4cbdeba79727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN CTE 0: 2995\n",
            "TRAIN CTE 1: 2913\n",
            "TEST CTE 0: 1143\n",
            "TEST CTE 1: 335\n",
            "TRAIN HE 0: 2905\n",
            "TRAIN HE 1: 3003\n",
            "TEST HE 0: 684\n",
            "TEST HE 1: 794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.load('./x_train_npy.npy')\n",
        "x_test = np.load('./x_test_npy.npy')\n",
        "\n",
        "print(np.shape(x_train))\n",
        "print(np.shape(x_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T585m-NDbIBt",
        "outputId": "25fd0f32-c96a-4675-dd3a-8329fef4218b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5908, 8, 16, 1)\n",
            "(1478, 8, 16, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INVOKE PROPHECY TO EXTRACT RULES**"
      ],
      "metadata": {
        "id": "BANgNDK_oaUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL NEEDS TO BE EXECUTED GPU**"
      ],
      "metadata": {
        "id": "xFCxHeSZnniU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from prophecy.utils.misc import get_model, read_split\n",
        "from prophecy.core.extract import Extractor\n",
        "from prophecy.core.detect import RulesDetector, ClassifierDetector\n",
        "from prophecy.utils.paths import results_path\n",
        "\n",
        "\n",
        "def run_analyze_command(model,train_features,train_labels,val_features,val_labels,l_name,typ,inptype,acts,top,working_dir):\n",
        "    train_features, train_labels = read_split(train_features, train_labels)\n",
        "    val_features, val_labels = read_split(val_features, val_labels)\n",
        "\n",
        "    rule_extractor = Extractor(model=model, train_features=train_features, train_labels=train_labels,\n",
        "                               val_features=val_features, val_labels=val_labels, skip_rules=False, layer_name=l_name,\n",
        "                               only_dense=False, balance=False, confidence=False,\n",
        "                               only_activation=False, type=typ, inptype=inptype, acts=acts, top=top)\n",
        "\n",
        "    classifiers_path = working_dir / \"classifiers\"\n",
        "    ruleset = rule_extractor(path=classifiers_path)\n",
        "    rules_path = working_dir / 'ruleset.csv'\n",
        "    pd.DataFrame(ruleset).to_csv(rules_path, index=False)\n"
      ],
      "metadata": {
        "id": "DDGwuVEBi9Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PARAMETERS: rules per label (based on given labels array), rules in terms of on/off activation values, TOP rules (highest train recall), all dense and activation layers.**"
      ],
      "metadata": {
        "id": "SHr4__9xnsb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory_path = '/content/ProphecyPlus/results/kjt/rules_cte_1/'\n",
        "os.makedirs(directory_path,exist_ok=True)\n",
        "\n",
        "wd = Path('/content/ProphecyPlus/results/kjt/rules_cte_1/')\n",
        "run_analyze_command(model,\"./x_train_npy.npy\",\"./y_train_cte_npy.npy\",\"./x_test_npy.npy\",\"./y_test_cte_npy.npy\",\"layer_10\",3,0,False,False,wd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlTwp7D3qIWH",
        "outputId": "f85850c4-04d3-4475-ea00-f1bb64e4e474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONFIG PARAMS: LAYER NAME: layer_10 ,TYPE: 3 ,INP TYPE: 0 ,ACTS: False ,Top/All: False\n",
            "Layer Name: layer_10\n",
            "Layers to be considered for fingerprinting: ['layer_10']\n",
            "Invoking Dec-tree classifier based on FEATURES\n",
            "\n",
            "Fingerprinting TRAIN data after layer_10 layer\n",
            "Processing layer_10:   0%|          | 0/24 [00:00<?, ?it/s]tf.Tensor(\n",
            "[[[[-0.26016584 -0.43339378  2.2210205  ...  2.5751863   4.813346\n",
            "    -0.2545323 ]\n",
            "   [ 0.98201674 -0.43722454  3.0293176  ...  4.171374    4.887096\n",
            "    -0.29122013]]]\n",
            "\n",
            "\n",
            " [[[-0.18652447 -0.4324616   2.2048576  ...  2.6458187   4.772342\n",
            "    -0.25256512]\n",
            "   [ 1.0005517  -0.4410913   2.9933293  ...  4.2622576   4.9028606\n",
            "    -0.29046923]]]\n",
            "\n",
            "\n",
            " [[[-0.11948881 -0.43224463  2.210664   ...  2.6556752   4.703244\n",
            "    -0.24672768]\n",
            "   [ 1.0262357  -0.44452363  3.0207863  ...  4.361699    4.8975844\n",
            "    -0.2941592 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.9637208  -0.42356405  2.7032819  ...  2.80697     4.7505593\n",
            "    -0.23325606]\n",
            "   [ 1.5171398  -0.45405763  3.1704452  ...  3.8166356   6.071677\n",
            "    -0.2802743 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.62951213 -0.40280724  2.6120062  ...  2.6688855   4.489611\n",
            "    -0.21430571]\n",
            "   [ 1.6079441  -0.46705773  3.2359548  ...  3.9210074   6.1389837\n",
            "    -0.271785  ]]]\n",
            "\n",
            "\n",
            " [[[ 0.5021482  -0.39605284  2.5866833  ...  2.6958737   4.4166527\n",
            "    -0.21439838]\n",
            "   [ 1.6255888  -0.48035645  3.3096368  ...  4.000192    6.1305647\n",
            "    -0.2720794 ]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-4.457205  -1.948548  -3.0069826 ... -1.2161914  8.097293   3.6387482]\n",
            " [-4.3902693 -2.0272279 -3.0420167 ... -1.389712   7.9507484  3.773254 ]\n",
            " [-4.354619  -2.0635738 -3.1056032 ... -1.5577652  7.80088    3.8919835]\n",
            " ...\n",
            " [-3.1147358 -2.5729704 -3.658525  ... -1.5251598  6.032284   4.794036 ]\n",
            " [-4.0242867 -2.5113208 -3.8059726 ... -2.156045   6.539422   4.270197 ]\n",
            " [-4.3546286 -2.4867432 -3.8454487 ... -2.3913746  6.8256297  4.2003183]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.2945927  -0.40944356  2.4705894  ...  2.7538157   4.313176\n",
            "    -0.2262594 ]\n",
            "   [ 1.642166   -0.4838804   3.308957   ...  4.0255175   6.110197\n",
            "    -0.2754134 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.02331182 -0.41847667  2.3130493  ...  2.7739213   4.1687484\n",
            "    -0.2182032 ]\n",
            "   [ 1.6270735  -0.48757473  3.307221   ...  4.025069    6.054591\n",
            "    -0.25894448]]]\n",
            "\n",
            "\n",
            " [[[ 0.08693323 -0.41155493  2.308256   ...  2.8093865   4.3523707\n",
            "    -0.2209701 ]\n",
            "   [ 1.6121562  -0.4870502   3.2389073  ...  3.9863951   5.9107294\n",
            "    -0.26591173]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.2558877  -0.43960044  2.7361324  ...  3.5275197   4.8821745\n",
            "    -0.2933403 ]\n",
            "   [ 0.92073274 -0.46044797  3.2807178  ...  3.9787374   5.525749\n",
            "    -0.24444312]]]\n",
            "\n",
            "\n",
            " [[[ 1.1765486  -0.43638358  2.752878   ...  3.5033934   4.8120904\n",
            "    -0.28814203]\n",
            "   [ 0.8504875  -0.4421609   3.2470806  ...  3.9105177   5.56613\n",
            "    -0.26643878]]]\n",
            "\n",
            "\n",
            " [[[ 1.1241825  -0.4318964   2.7451065  ...  3.4897544   4.752567\n",
            "    -0.28844988]\n",
            "   [ 0.7836656  -0.44641083  3.235027   ...  3.8569093   5.579747\n",
            "    -0.26377976]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-4.77587   -2.460104  -3.842127  ... -2.6657782  7.2082944  3.9902217]\n",
            " [-5.292351  -2.4003975 -3.8203008 ... -2.9648197  7.6575074  3.6543174]\n",
            " [-4.9701524 -2.347891  -3.6809888 ... -2.5367706  7.6219606  3.7792156]\n",
            " ...\n",
            " [-1.7594391 -2.695713  -3.120762  ... -0.9665728  5.665867   6.2357826]\n",
            " [-1.8513744 -2.7059126 -3.0936565 ... -1.0389011  5.6866527  6.112764 ]\n",
            " [-1.8876712 -2.7072754 -3.0506678 ... -1.0461735  5.693456   6.030977 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.2844673  -0.44309783  2.6903489  ...  3.5076537   4.6931095\n",
            "    -0.28745386]\n",
            "   [ 0.7637842  -0.44439983  3.2175372  ...  3.8831596   5.5457516\n",
            "    -0.26962328]]]\n",
            "\n",
            "\n",
            " [[[ 1.2649641  -0.4468607   2.6690855  ...  3.536266    4.6734643\n",
            "    -0.29272646]\n",
            "   [ 0.75184864 -0.44551507  3.1992981  ...  3.8251925   5.546185\n",
            "    -0.27206355]]]\n",
            "\n",
            "\n",
            " [[[ 1.2816038  -0.44205517  2.6852136  ...  3.5235918   4.672925\n",
            "    -0.29955792]\n",
            "   [ 0.75108653 -0.44924167  3.1870813  ...  3.7994316   5.537158\n",
            "    -0.26595512]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-0.15149665 -0.42004165  2.4200432  ...  2.6833847   4.836601\n",
            "    -0.25897592]\n",
            "   [ 1.3319124  -0.41600344  2.9786701  ...  4.4829206   4.9891276\n",
            "    -0.24657123]]]\n",
            "\n",
            "\n",
            " [[[-0.10127237 -0.41907415  2.391985   ...  2.7633262   4.775067\n",
            "    -0.25496304]\n",
            "   [ 1.3418417  -0.41768146  2.9839363  ...  4.462092    4.9947076\n",
            "    -0.25349948]]]\n",
            "\n",
            "\n",
            " [[[-0.05349371 -0.4186569   2.3245163  ...  2.8623006   4.6789904\n",
            "    -0.24617669]\n",
            "   [ 1.3256288  -0.41764057  2.9531531  ...  4.4666424   4.9648757\n",
            "    -0.26192218]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-1.5977607 -2.7626178 -3.1034439 ... -1.1232705  5.330541   6.1802125]\n",
            " [-1.5861858 -2.7582312 -3.0768363 ... -1.1098584  5.349943   6.157692 ]\n",
            " [-1.526861  -2.7488666 -3.069363  ... -1.0671998  5.298257   6.168269 ]\n",
            " ...\n",
            " [-4.592199  -2.0446074 -3.1318338 ... -1.5377163  8.012997   4.020826 ]\n",
            " [-4.500476  -2.0742378 -3.1289997 ... -1.6056702  7.9101667  4.127909 ]\n",
            " [-4.375276  -2.1208994 -3.1240878 ... -1.7413268  7.762356   4.243509 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 8.9637435e-04 -4.2890719e-01  2.2420006e+00 ...  2.9011722e+00\n",
            "     4.6669974e+00 -2.4107249e-01]\n",
            "   [ 1.2861633e+00 -4.2174512e-01  2.9453769e+00 ...  4.4414296e+00\n",
            "     4.9440665e+00 -2.5664800e-01]]]\n",
            "\n",
            "\n",
            " [[[ 1.9070577e-02 -4.3223417e-01  2.1950064e+00 ...  2.9211295e+00\n",
            "     4.6315641e+00 -2.3677893e-01]\n",
            "   [ 1.1868577e+00 -4.2367807e-01  2.8743522e+00 ...  4.4514732e+00\n",
            "     4.7296433e+00 -2.4646364e-01]]]\n",
            "\n",
            "\n",
            " [[[-1.5123675e-02 -4.3693101e-01  2.1774950e+00 ...  2.8980598e+00\n",
            "     4.6145334e+00 -2.3357549e-01]\n",
            "   [ 1.2518770e+00 -4.2500979e-01  2.9754794e+00 ...  4.4833035e+00\n",
            "     4.2703176e+00 -2.3177277e-01]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-1.8569146e-01 -4.1876003e-01  2.1194873e+00 ...  2.8957410e+00\n",
            "     4.6846862e+00 -2.5341314e-01]\n",
            "   [ 1.1603460e+00 -4.3523800e-01  3.1268334e+00 ...  4.0576940e+00\n",
            "     5.0898304e+00 -2.9604337e-01]]]\n",
            "\n",
            "\n",
            " [[[-2.8288352e-01 -4.2669219e-01  2.1640987e+00 ...  2.7214737e+00\n",
            "     4.8351822e+00 -2.5939935e-01]\n",
            "   [ 1.0216421e+00 -4.2564943e-01  3.0319331e+00 ...  3.9936786e+00\n",
            "     4.9470134e+00 -2.9840305e-01]]]\n",
            "\n",
            "\n",
            " [[[-3.0040878e-01 -4.3839452e-01  2.2140303e+00 ...  2.5837636e+00\n",
            "     4.8981857e+00 -2.5240961e-01]\n",
            "   [ 9.8328066e-01 -4.2291522e-01  3.0224354e+00 ...  4.0522552e+00\n",
            "     4.9105811e+00 -2.7957243e-01]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-4.2483263 -2.1469276 -3.139227  ... -1.7656794  7.6647673  4.270034 ]\n",
            " [-3.9790375 -2.1025403 -3.0150793 ... -1.6256521  7.507927   4.370911 ]\n",
            " [-3.872204  -1.813519  -2.9174616 ... -1.3818175  7.678281   4.520206 ]\n",
            " ...\n",
            " [-4.5421963 -2.0432785 -3.0290391 ... -1.4517606  8.179376   3.8518968]\n",
            " [-4.3973274 -1.9469923 -2.9228444 ... -1.0638764  8.202885   3.6699142]\n",
            " [-4.38785   -1.919303  -2.9638946 ... -0.9990241  8.173242   3.5925715]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[-0.28638956 -0.44328701  2.2314246  ...  2.542456    4.910076\n",
            "    -0.2540363 ]\n",
            "   [ 0.9972938  -0.44669774  3.0084677  ...  4.1393847   4.8968596\n",
            "    -0.27969217]]]\n",
            "\n",
            "\n",
            " [[[-0.20291579 -0.44470707  2.342267   ...  2.5647683   4.9252143\n",
            "    -0.25389007]\n",
            "   [ 0.9981139  -0.44542548  3.0003066  ...  4.1344366   4.932055\n",
            "    -0.28136313]]]\n",
            "\n",
            "\n",
            " [[[-0.19253974 -0.44727677  2.3976786  ...  2.5263665   4.9226646\n",
            "    -0.257702  ]\n",
            "   [ 1.0576462  -0.44514206  2.9771347  ...  4.1882124   4.9282103\n",
            "    -0.27630934]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-0.2568779  -0.42112592  2.4036689  ...  2.5209203   4.7134023\n",
            "    -0.25616738]\n",
            "   [ 1.1298703  -0.4421002   2.9847033  ...  4.35973     4.9627495\n",
            "    -0.27336043]]]\n",
            "\n",
            "\n",
            " [[[-0.27129278 -0.41157663  2.3580706  ...  2.495857    4.6930776\n",
            "    -0.2532306 ]\n",
            "   [ 1.1639844  -0.44379577  2.9956417  ...  4.3368015   4.9851823\n",
            "    -0.27524716]]]\n",
            "\n",
            "\n",
            " [[[-0.22745307 -0.41408545  2.3350348  ...  2.536842    4.7155714\n",
            "    -0.2541727 ]\n",
            "   [ 1.1790556  -0.44507456  2.987473   ...  4.3177857   5.0064964\n",
            "    -0.2705574 ]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-4.4257193  -1.9282677  -2.9807978  ... -1.0352212   8.149955\n",
            "   3.589972  ]\n",
            " [-4.31278    -1.9517741  -2.9469998  ... -0.9640379   8.043581\n",
            "   3.731534  ]\n",
            " [-4.334705   -1.9464623  -2.9626615  ... -0.98840845  7.990105\n",
            "   3.754518  ]\n",
            " ...\n",
            " [-4.6021957  -1.9943551  -3.0199702  ... -1.3902731   7.944337\n",
            "   3.7558537 ]\n",
            " [-4.6609163  -1.9858574  -3.0667398  ... -1.4425746   7.958831\n",
            "   3.6732218 ]\n",
            " [-4.6318803  -2.0038505  -3.083852   ... -1.4540873   7.966959\n",
            "   3.6959908 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[-0.22347422 -0.41223466  2.2880363  ...  2.624777    4.56918\n",
            "    -0.24275549]\n",
            "   [ 1.1558398  -0.44499266  3.0006592  ...  4.324402    5.0476904\n",
            "    -0.26891676]]]\n",
            "\n",
            "\n",
            " [[[-0.12913215 -0.4208067   2.2281084  ...  2.704123    4.499159\n",
            "    -0.24043453]\n",
            "   [ 1.1699522  -0.43934742  2.9768136  ...  4.293225    5.0957046\n",
            "    -0.2612269 ]]]\n",
            "\n",
            "\n",
            " [[[-0.0321148  -0.4198573   2.1843069  ...  2.7557418   4.4357944\n",
            "    -0.2298562 ]\n",
            "   [ 1.1834464  -0.44279754  2.9883244  ...  4.3457866   5.0817933\n",
            "    -0.26405242]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.71226573 -0.4277592   2.5915935  ...  2.8202019   4.56738\n",
            "    -0.23528792]\n",
            "   [ 1.550194   -0.4847257   3.286548   ...  4.068661    6.1456623\n",
            "    -0.27282855]]]\n",
            "\n",
            "\n",
            " [[[ 0.7716956  -0.41333368  2.6184216  ...  2.8042686   4.706711\n",
            "    -0.2335366 ]\n",
            "   [ 1.5184163  -0.4854835   3.2830875  ...  4.0598617   6.0884767\n",
            "    -0.26612312]]]\n",
            "\n",
            "\n",
            " [[[ 0.718638   -0.41791707  2.5681047  ...  2.822566    4.771684\n",
            "    -0.22995564]\n",
            "   [ 1.4720275  -0.48268616  3.2274635  ...  4.007796    6.0167623\n",
            "    -0.26098853]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-4.673673  -2.0680885 -3.119785  ... -1.7370135  7.886817   3.7503583]\n",
            " [-4.527275  -2.1385076 -3.1671114 ... -1.8716995  7.683101   3.8443727]\n",
            " [-4.400014  -2.1852682 -3.2211006 ... -2.013518   7.4916153  3.9813576]\n",
            " ...\n",
            " [-3.8926523 -2.6110506 -3.8081167 ... -2.1950104  6.530595   4.5479827]\n",
            " [-3.6909857 -2.585364  -3.7642286 ... -1.9244941  6.502752   4.626193 ]\n",
            " [-3.652455  -2.5526688 -3.6731555 ... -1.7605696  6.605845   4.5589705]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.55585927 -0.41318807  2.4849434  ...  2.805816    4.7349358\n",
            "    -0.22310774]\n",
            "   [ 1.4561652  -0.47872633  3.218633   ...  4.046014    5.9448605\n",
            "    -0.2624189 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.36614105 -0.40752664  2.4279828  ...  2.851608    4.621887\n",
            "    -0.22231087]\n",
            "   [ 1.4802496  -0.473722    3.1689627  ...  4.0869355   5.919163\n",
            "    -0.2791545 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.32469943 -0.40944117  2.3551     ...  2.848808    4.670869\n",
            "    -0.22145893]\n",
            "   [ 1.3569812  -0.46091568  3.1382854  ...  3.9678879   5.6989636\n",
            "    -0.27609044]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.59763336 -0.40436816  2.782832   ...  3.057978    4.762677\n",
            "    -0.23652272]\n",
            "   [ 0.50364363 -0.46941897  3.0504212  ...  3.4489996   5.8032045\n",
            "    -0.27312285]]]\n",
            "\n",
            "\n",
            " [[[ 0.6923541  -0.39707735  2.7849543  ...  3.0625322   4.7886057\n",
            "    -0.24373746]\n",
            "   [ 0.5224302  -0.4662295   3.0268946  ...  3.4618149   5.8181515\n",
            "    -0.2732399 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.71594584 -0.40092772  2.7688653  ...  3.0389125   4.7340856\n",
            "    -0.24406327]\n",
            "   [ 0.5559027  -0.46327105  3.0190027  ...  3.4509518   5.8123207\n",
            "    -0.28054512]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-3.9325035  -2.500385   -3.627351   ... -1.831507    6.8870206\n",
            "   4.363997  ]\n",
            " [-4.27969    -2.4855008  -3.5804372  ... -2.0590284   7.1538787\n",
            "   4.2086153 ]\n",
            " [-4.0689144  -2.369777   -3.4019833  ... -1.7014456   7.203134\n",
            "   4.1691737 ]\n",
            " ...\n",
            " [-2.4686112  -2.5835805  -2.6216283  ... -0.28637156  6.2094765\n",
            "   4.919277  ]\n",
            " [-2.3078637  -2.6199865  -2.6332088  ... -0.24263635  6.027954\n",
            "   5.0050445 ]\n",
            " [-2.3022187  -2.6138363  -2.6779022  ... -0.3311322   5.9416323\n",
            "   4.9837375 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.82201314 -0.41015622  2.773456   ...  3.1068656   4.70104\n",
            "    -0.254671  ]\n",
            "   [ 0.5965302  -0.46255425  2.9964316  ...  3.5298417   5.7954516\n",
            "    -0.27348194]]]\n",
            "\n",
            "\n",
            " [[[ 0.95959634 -0.4249346   2.716337   ...  3.1571667   4.6885433\n",
            "    -0.2711509 ]\n",
            "   [ 0.658852   -0.46418932  3.0046732  ...  3.5528045   5.7655606\n",
            "    -0.27737215]]]\n",
            "\n",
            "\n",
            " [[[ 1.1396888  -0.4495818   2.6722627  ...  3.2834907   4.769908\n",
            "    -0.28151864]\n",
            "   [ 0.7656321  -0.46357414  3.0100594  ...  3.5566785   5.7744107\n",
            "    -0.28067678]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.8301887  -0.4020927   2.660238   ...  3.3486347   4.3534226\n",
            "    -0.24411333]\n",
            "   [ 0.9899341  -0.4591455   3.1867552  ...  4.096067    5.394674\n",
            "    -0.24503218]]]\n",
            "\n",
            "\n",
            " [[[ 0.88038146 -0.41096538  2.7141902  ...  3.3299031   4.467941\n",
            "    -0.2519986 ]\n",
            "   [ 0.90047365 -0.47704837  3.137717   ...  3.9736998   5.3329687\n",
            "    -0.2460262 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.86107856 -0.409118    2.7546332  ...  3.3192306   4.5020776\n",
            "    -0.2479161 ]\n",
            "   [ 0.86727715 -0.4860326   3.1682432  ...  3.9418268   5.3953867\n",
            "    -0.23600549]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-2.1423085  -2.6697862  -2.7122028  ... -0.44485041  5.727787\n",
            "   5.185664  ]\n",
            " [-1.9582998  -2.6945388  -2.817158   ... -0.5627396   5.533854\n",
            "   5.3319693 ]\n",
            " [-1.7090908  -2.7415977  -2.9450932  ... -0.63390356  5.389324\n",
            "   5.5744324 ]\n",
            " ...\n",
            " [-2.641017   -2.5803003  -3.0594585  ... -1.6647552   5.896969\n",
            "   5.68042   ]\n",
            " [-2.3027074  -2.5399833  -2.8972049  ... -1.19492     5.782935\n",
            "   5.747623  ]\n",
            " [-2.330035   -2.5468416  -2.8754015  ... -1.1055918   5.8487005\n",
            "   5.7263074 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.0015117e+00 -4.1277653e-01  2.7575724e+00 ...  3.3632658e+00\n",
            "     4.6390319e+00 -2.5934401e-01]\n",
            "   [ 7.1832675e-01 -4.8596862e-01  3.1914680e+00 ...  3.8418887e+00\n",
            "     5.6173940e+00 -2.4527696e-01]]]\n",
            "\n",
            "\n",
            " [[[ 1.0354038e+00 -4.2298624e-01  2.7491696e+00 ...  3.3866835e+00\n",
            "     4.8397179e+00 -2.6998684e-01]\n",
            "   [ 5.5080557e-01 -4.6989471e-01  3.1757016e+00 ...  3.6343472e+00\n",
            "     5.6492910e+00 -2.5294873e-01]]]\n",
            "\n",
            "\n",
            " [[[ 9.7322994e-01 -4.2372775e-01  2.7645335e+00 ...  3.3618822e+00\n",
            "     4.9571891e+00 -2.6772594e-01]\n",
            "   [ 4.7353706e-01 -4.6287626e-01  3.0637305e+00 ...  3.5123692e+00\n",
            "     5.6924443e+00 -2.6790902e-01]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 3.2620530e-03 -4.4027126e-01  2.2654068e+00 ...  2.9118364e+00\n",
            "     4.4909086e+00 -2.2539359e-01]\n",
            "   [ 1.2642205e+00 -4.2793074e-01  2.8754916e+00 ...  4.4577761e+00\n",
            "     4.7430735e+00 -2.5212824e-01]]]\n",
            "\n",
            "\n",
            " [[[ 1.9142158e-01 -4.2822093e-01  2.3003647e+00 ...  3.1089849e+00\n",
            "     4.4691749e+00 -2.3001957e-01]\n",
            "   [ 1.1619060e+00 -4.4851154e-01  2.9471509e+00 ...  4.5566092e+00\n",
            "     4.7905664e+00 -2.4754545e-01]]]\n",
            "\n",
            "\n",
            " [[[ 3.4638643e-01 -4.1504115e-01  2.3805416e+00 ...  3.0998607e+00\n",
            "     4.4262519e+00 -2.3338129e-01]\n",
            "   [ 1.3695617e+00 -4.5361087e-01  3.1271930e+00 ...  4.6432376e+00\n",
            "     5.0689669e+00 -2.5313199e-01]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-2.0495152  -2.696601   -2.9062445  ... -0.9364122   5.6802115\n",
            "   5.7960176 ]\n",
            " [-1.737206   -2.6946507  -2.7854486  ... -0.4451537   5.7228065\n",
            "   5.777482  ]\n",
            " [-1.6703405  -2.7013693  -2.6671178  ... -0.15924989  5.7908025\n",
            "   5.6345596 ]\n",
            " ...\n",
            " [-4.0858116  -2.0903668  -3.0720773  ... -1.864391    7.4354186\n",
            "   4.39797   ]\n",
            " [-3.7480426  -2.2444787  -3.0454016  ... -1.9364293   7.121286\n",
            "   4.8495684 ]\n",
            " [-3.9439359  -2.3387053  -3.3485842  ... -2.333482    7.0234556\n",
            "   4.9617457 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "Processing layer_10:  38%|███▊      | 9/24 [00:00<00:00, 86.62it/s]tf.Tensor(\n",
            "[[[[ 0.37488687 -0.42256033  2.3886364  ...  2.913814    4.334441\n",
            "    -0.23295231]\n",
            "   [ 1.3371114  -0.43316728  3.1645217  ...  4.657327    5.032683\n",
            "    -0.27212062]]]\n",
            "\n",
            "\n",
            " [[[ 0.4202268  -0.4329069   2.4113295  ...  2.8804572   4.3716607\n",
            "    -0.23570289]\n",
            "   [ 1.3645097  -0.41939747  3.131568   ...  4.5124936   5.0635295\n",
            "    -0.31462905]]]\n",
            "\n",
            "\n",
            " [[[ 0.5294581  -0.42483345  2.555229   ...  2.883997    4.277044\n",
            "    -0.2422359 ]\n",
            "   [ 1.4222682  -0.4215715   3.2961864  ...  4.5181394   5.178384\n",
            "    -0.3516038 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.40424004 -0.4831876   2.112017   ...  1.5813892   4.657303\n",
            "    -0.27423197]\n",
            "   [ 0.7562356  -0.4940462   3.0425375  ...  2.6358588   7.063868\n",
            "    -0.2923607 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.19578438 -0.4999675   1.9463894  ...  1.2976952   4.7306523\n",
            "    -0.28431726]\n",
            "   [ 0.8585798  -0.5186156   3.2018807  ...  2.8113313   7.0241566\n",
            "    -0.27419546]]]\n",
            "\n",
            "\n",
            " [[[ 0.12506527 -0.47927076  1.9848733  ...  1.3473548   4.7776237\n",
            "    -0.24056543]\n",
            "   [ 0.96678853 -0.5274284   3.1959522  ...  2.8121343   6.972017\n",
            "    -0.28357685]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-3.9559634  -2.2949498  -3.451394   ... -2.437639    6.8442287\n",
            "   4.8248653 ]\n",
            " [-3.8076081  -2.2650127  -3.419972   ... -2.2258613   6.7516427\n",
            "   4.787041  ]\n",
            " [-3.8051217  -2.2844663  -3.5338662  ... -2.3889785   6.597966\n",
            "   4.990404  ]\n",
            " ...\n",
            " [-3.9826953  -2.5865974  -3.5720031  ... -0.53517896  6.2811785\n",
            "   1.7905377 ]\n",
            " [-4.7128086  -2.4389312  -3.7714496  ... -0.7040492   6.875053\n",
            "   1.2216567 ]\n",
            " [-4.8559213  -2.3724914  -3.780958   ... -0.7324413   7.11242\n",
            "   1.2410754 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 3.8984187e-03 -4.5033139e-01  1.9024500e+00 ...  1.2731750e+00\n",
            "     4.7824674e+00 -2.0947360e-01]\n",
            "   [ 9.5184410e-01 -5.4138136e-01  3.2474988e+00 ...  2.8666360e+00\n",
            "     6.9061050e+00 -2.9842973e-01]]]\n",
            "\n",
            "\n",
            " [[[-4.3683622e-02 -4.1562393e-01  1.9343249e+00 ...  1.3135904e+00\n",
            "     4.7693119e+00 -1.8219785e-01]\n",
            "   [ 1.0170449e+00 -5.1963300e-01  3.3162205e+00 ...  2.9425254e+00\n",
            "     6.8629422e+00 -2.9820737e-01]]]\n",
            "\n",
            "\n",
            " [[[-1.5854795e-01 -3.8918811e-01  1.9608127e+00 ...  1.3153100e+00\n",
            "     4.7499266e+00 -1.4427948e-01]\n",
            "   [ 1.0656229e+00 -5.0459403e-01  3.3925750e+00 ...  3.0701087e+00\n",
            "     6.8354139e+00 -2.9781163e-01]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.1216224e+00 -4.3839362e-01  2.6261106e+00 ...  3.1212685e+00\n",
            "     5.7068977e+00 -2.2063573e-01]\n",
            "   [-1.8767534e-01 -4.6803588e-01  2.2017572e+00 ...  2.6110988e+00\n",
            "     5.1416206e+00 -2.5397581e-01]]]\n",
            "\n",
            "\n",
            " [[[ 1.1361405e+00 -4.5314533e-01  2.7977149e+00 ...  3.0237899e+00\n",
            "     5.6467147e+00 -2.0399462e-01]\n",
            "   [-1.9231939e-01 -4.6606842e-01  2.1764848e+00 ...  2.5799849e+00\n",
            "     5.1592765e+00 -2.4901064e-01]]]\n",
            "\n",
            "\n",
            " [[[ 1.2307731e+00 -4.4446495e-01  2.8721392e+00 ...  2.9573593e+00\n",
            "     5.6629868e+00 -2.1543178e-01]\n",
            "   [-2.0562150e-01 -4.6730205e-01  2.1790802e+00 ...  2.5470607e+00\n",
            "     5.1734295e+00 -2.5050989e-01]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-5.144497   -2.3071826  -3.8198762  ... -0.82794183  7.3858194\n",
            "   1.0231831 ]\n",
            " [-5.309732   -2.2655902  -3.8592577  ... -0.9532559   7.5729704\n",
            "   1.0931143 ]\n",
            " [-5.61974    -2.223705   -3.882256   ... -1.0909956   7.867322\n",
            "   1.0722414 ]\n",
            " ...\n",
            " [ 0.34833124 -2.5147614  -1.872302   ...  2.1849349   5.01399\n",
            "   5.237093  ]\n",
            " [ 0.41900328 -2.4923756  -1.9004469  ...  2.172656    4.844231\n",
            "   5.2749557 ]\n",
            " [ 0.6142302  -2.4922674  -1.9067498  ...  2.2994142   4.6229477\n",
            "   5.3412004 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.3616353  -0.43504277  3.0507066  ...  2.75805     5.5656238\n",
            "    -0.24043517]\n",
            "   [-0.20692702 -0.46603757  2.187771   ...  2.5445244   5.2555647\n",
            "    -0.2519736 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.3748902  -0.43562505  3.1369007  ...  2.8138542   5.459497\n",
            "    -0.234472  ]\n",
            "   [-0.18782616 -0.46762916  2.1803322  ...  2.516514    5.285539\n",
            "    -0.25174734]]]\n",
            "\n",
            "\n",
            " [[[ 1.306252   -0.4359454   3.0680888  ...  2.8767347   5.397738\n",
            "    -0.23297298]\n",
            "   [-0.16969901 -0.4698556   2.1609347  ...  2.5056098   5.329001\n",
            "    -0.24830958]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.2607175  -0.43900576  2.387674   ...  2.8545883   4.7583838\n",
            "    -0.2868318 ]\n",
            "   [ 0.70756423 -0.48755687  3.0036535  ...  3.1091306   6.116461\n",
            "    -0.29205337]]]\n",
            "\n",
            "\n",
            " [[[ 1.2578678  -0.44248024  2.3279366  ...  2.8370717   4.677709\n",
            "    -0.2799595 ]\n",
            "   [ 0.5836546  -0.48184523  2.968123   ...  3.0612404   6.0900855\n",
            "    -0.29464152]]]\n",
            "\n",
            "\n",
            " [[[ 1.2304727  -0.44107267  2.3658574  ...  2.8815534   4.6293397\n",
            "    -0.27186322]\n",
            "   [ 0.44051486 -0.48116505  2.9261062  ...  3.0182204   6.049173\n",
            "    -0.2917481 ]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.7979926  -2.5082467  -1.9634334  ...  2.333206    4.1619787\n",
            "   5.3791094 ]\n",
            " [ 0.8409157  -2.527664   -1.9431512  ...  2.2384472   4.047505\n",
            "   5.5100245 ]\n",
            " [ 0.673208   -2.5575068  -1.9425912  ...  2.0733569   4.157108\n",
            "   5.42947   ]\n",
            " ...\n",
            " [-1.6488445  -2.747936   -3.2675302  ... -0.5153446   5.0913014\n",
            "   4.719999  ]\n",
            " [-1.5458674  -2.7715652  -3.2119212  ... -0.5125688   4.946219\n",
            "   4.6591077 ]\n",
            " [-1.3945029  -2.7916598  -3.0572233  ... -0.38499436  4.852022\n",
            "   4.735724  ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.2844563  -0.44579527  2.3355472  ...  2.898382    4.64985\n",
            "    -0.27097633]\n",
            "   [ 0.32011005 -0.47619736  2.8524096  ...  2.9524744   6.015839\n",
            "    -0.28771847]]]\n",
            "\n",
            "\n",
            " [[[ 1.3844684  -0.44740996  2.3394935  ...  2.8547478   4.7461042\n",
            "    -0.26115057]\n",
            "   [ 0.30660877 -0.47733024  2.8552575  ...  2.9288127   6.026408\n",
            "    -0.2900022 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.3177892  -0.4559186   2.2952785  ...  2.8376606   4.9059567\n",
            "    -0.26015583]\n",
            "   [ 0.25795254 -0.47600785  2.8754323  ...  2.9077635   6.0234747\n",
            "    -0.2903814 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.7809411  -0.44978428  2.2991786  ...  2.231063    4.880342\n",
            "    -0.26212656]\n",
            "   [ 0.9954784  -0.4857459   3.1585808  ...  2.990821    6.3823867\n",
            "    -0.284467  ]]]\n",
            "\n",
            "\n",
            " [[[ 0.5579111  -0.43621713  2.5303366  ...  2.2562587   4.9816823\n",
            "    -0.22913224]\n",
            "   [ 1.1483968  -0.48930725  3.2248924  ...  3.0367289   6.303703\n",
            "    -0.2859646 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.71320575 -0.42607173  2.60449    ...  2.2809157   5.1383142\n",
            "    -0.23284587]\n",
            "   [ 1.2152232  -0.4894542   3.2125778  ...  3.0471933   6.200951\n",
            "    -0.28510574]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-1.121376   -2.8279967  -2.9698172  ... -0.22280285  4.654335\n",
            "   4.773905  ]\n",
            " [-0.91845083 -2.8345737  -2.9741287  ... -0.02039311  4.5188403\n",
            "   4.81119   ]\n",
            " [-1.0007691  -2.804587   -2.9505131  ...  0.16746438  4.7927155\n",
            "   4.6836653 ]\n",
            " ...\n",
            " [-3.109196   -2.4623542  -3.6230152  ... -0.6597869   6.201588\n",
            "   3.3419826 ]\n",
            " [-3.4817243  -2.293115   -3.3865442  ... -0.31455606  6.7465734\n",
            "   3.400149  ]\n",
            " [-3.1216433  -2.2528727  -3.3609781  ... -0.0153387   6.583904\n",
            "   3.6640296 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.8477132  -0.4371411   2.6130822  ...  2.4031916   5.191813\n",
            "    -0.23259759]\n",
            "   [ 1.2189537  -0.48775494  3.2264295  ...  3.0340836   6.1657715\n",
            "    -0.2837079 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.92382085 -0.43952298  2.6100852  ...  2.4466658   5.183427\n",
            "    -0.23878537]\n",
            "   [ 1.2238634  -0.4931045   3.2339725  ...  3.0322397   6.148224\n",
            "    -0.2827644 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.9551409  -0.44462556  2.6111448  ...  2.4730484   5.174603\n",
            "    -0.24494676]\n",
            "   [ 1.2828631  -0.49090505  3.241602   ...  3.0767481   6.1483145\n",
            "    -0.27902484]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.595823   -0.48782402  2.4095297  ...  2.470652    5.670236\n",
            "    -0.3035807 ]\n",
            "   [-0.09601089 -0.48307577  2.2427876  ...  2.5252295   5.8011546\n",
            "    -0.2697268 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.5946729  -0.47907513  2.3647416  ...  2.8227096   5.6235003\n",
            "    -0.31412035]\n",
            "   [-0.08370595 -0.4859066   2.2259097  ...  2.5028732   5.837829\n",
            "    -0.26449654]]]\n",
            "\n",
            "\n",
            " [[[ 1.5960354  -0.47955295  2.3321753  ...  2.8921258   5.6177974\n",
            "    -0.3136957 ]\n",
            "   [-0.08320272 -0.48312673  2.2231736  ...  2.4850361   5.886863\n",
            "    -0.27140442]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-2.8299637  -2.281697   -3.3890986  ...  0.01763625  6.4452224\n",
            "   3.9362464 ]\n",
            " [-2.6799982  -2.2975175  -3.4040906  ...  0.01525529  6.322097\n",
            "   4.061545  ]\n",
            " [-2.6853824  -2.307013   -3.4541054  ... -0.07625785  6.302005\n",
            "   4.1290073 ]\n",
            " ...\n",
            " [ 0.48667702 -2.780112   -2.4743402  ...  1.9801128   3.9498181\n",
            "   4.49771   ]\n",
            " [ 0.5456351  -2.8824139  -2.3702319  ...  1.8352801   4.042528\n",
            "   4.8149133 ]\n",
            " [ 0.5306863  -2.9225276  -2.3843193  ...  1.7626736   4.074082\n",
            "   4.836216  ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.5823468  -0.4762602   2.286267   ...  2.881253    5.529124\n",
            "    -0.3168611 ]\n",
            "   [-0.09052605 -0.48085162  2.2452939  ...  2.5238335   5.9549603\n",
            "    -0.26665607]]]\n",
            "\n",
            "\n",
            " [[[ 1.2877678  -0.45342624  2.2363787  ...  2.8070054   5.4896975\n",
            "    -0.3121385 ]\n",
            "   [-0.08451293 -0.48302913  2.2427552  ...  2.5072887   5.982986\n",
            "    -0.26352128]]]\n",
            "\n",
            "\n",
            " [[[ 0.8455321  -0.46395218  2.211678   ...  2.760936    5.3755684\n",
            "    -0.25567836]\n",
            "   [-0.04783674 -0.4737502   2.2169993  ...  2.499887    5.968783\n",
            "    -0.2644302 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.8890441  -0.4168255   2.552314   ...  3.1000755   4.679377\n",
            "    -0.19300997]\n",
            "   [-0.00808319 -0.47375444  2.197586   ...  2.5961454   5.969635\n",
            "    -0.26281822]]]\n",
            "\n",
            "\n",
            " [[[ 0.5237513  -0.40874806  2.5241485  ...  2.7940216   5.220296\n",
            "    -0.16247933]\n",
            "   [ 0.01421359 -0.47926635  2.2518058  ...  2.5606914   6.010765\n",
            "    -0.2550747 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.79084873 -0.40516096  2.6113908  ...  2.874214    5.0066643\n",
            "    -0.1566458 ]\n",
            "   [ 0.07569356 -0.4829044   2.3450801  ...  2.595025    6.0855803\n",
            "    -0.26306456]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.40179622 -2.9702337  -2.4580443  ...  1.5384392   4.0564146\n",
            "   4.7661195 ]\n",
            " [-0.18742833 -2.897896   -2.4479911  ...  1.3804282   4.5829196\n",
            "   4.3461432 ]\n",
            " [-1.017376   -2.7795002  -2.356301   ...  1.145399    5.3153477\n",
            "   3.857333  ]\n",
            " ...\n",
            " [-0.8890351  -2.9181101  -2.18365    ...  0.39710674  4.6354218\n",
            "   4.6289845 ]\n",
            " [-1.5801293  -2.7021737  -2.1128898  ...  1.1137216   5.7488265\n",
            "   3.8761027 ]\n",
            " [-1.2345794  -2.7967486  -2.2342236  ...  0.8542631   5.1753917\n",
            "   4.297859  ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.7841968  -0.38653427  2.643857   ...  3.0734727   4.922495\n",
            "    -0.20185882]\n",
            "   [ 0.1246333  -0.48767272  2.4264996  ...  2.5720892   6.092429\n",
            "    -0.26859093]]]\n",
            "\n",
            "\n",
            " [[[ 0.8203859  -0.37947184  2.4811225  ...  2.8772433   4.9965124\n",
            "    -0.24493179]\n",
            "   [ 0.21626303 -0.48005486  2.4736507  ...  2.60675     6.14756\n",
            "    -0.28086856]]]\n",
            "\n",
            "\n",
            " [[[ 0.7782485  -0.4228188   2.4830146  ...  2.9164226   4.9383636\n",
            "    -0.29878974]\n",
            "   [ 0.24861947 -0.47589868  2.4945111  ...  2.585269    6.1469474\n",
            "    -0.27894393]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.37001    -0.46989068  2.5046813  ...  2.5804608   5.6779346\n",
            "    -0.2886006 ]\n",
            "   [-0.14910004 -0.4803507   2.2922907  ...  2.6515245   5.3033147\n",
            "    -0.25993988]]]\n",
            "\n",
            "\n",
            " [[[ 1.4335898  -0.49225998  2.322513   ...  2.1489139   5.950061\n",
            "    -0.29959676]\n",
            "   [-0.1580096  -0.4857258   2.2881117  ...  2.6379821   5.423597\n",
            "    -0.26088464]]]\n",
            "\n",
            "\n",
            " [[[ 1.3788166  -0.4994134   2.2514677  ...  2.1377091   5.9140935\n",
            "    -0.30223736]\n",
            "   [-0.16170324 -0.4857585   2.265078   ...  2.6275206   5.4376607\n",
            "    -0.26557967]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-1.2741848  -2.8016758  -2.2504492  ...  0.6608978   5.3000045\n",
            "   4.537328  ]\n",
            " [-1.4613317  -2.7754712  -2.4686143  ...  0.57487416  5.3566265\n",
            "   4.1967545 ]\n",
            " [-1.563589   -2.7571049  -2.4675214  ...  0.4816842   5.436392\n",
            "   4.196199  ]\n",
            " ...\n",
            " [ 0.4056442  -2.5202813  -2.2877107  ...  2.0905926   4.4586616\n",
            "   4.747303  ]\n",
            " [ 0.32228643 -2.4940708  -2.4816625  ...  2.410567    4.4408083\n",
            "   4.1129203 ]\n",
            " [ 0.19633663 -2.5004995  -2.5007699  ...  2.3048885   4.5096035\n",
            "   3.975904  ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.3270243  -0.49937287  2.2162676  ...  2.2028594   5.8629484\n",
            "    -0.30291277]\n",
            "   [-0.14605126 -0.4868871   2.265175   ...  2.6314285   5.4543533\n",
            "    -0.26046693]]]\n",
            "\n",
            "\n",
            " [[[ 1.2895522  -0.49565795  2.217028   ...  2.3799338   5.833417\n",
            "    -0.30400017]\n",
            "   [-0.13616058 -0.48425952  2.2301328  ...  2.5735145   5.474239\n",
            "    -0.2633685 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.3905041  -0.48156166  2.2836995  ...  2.75544     5.716791\n",
            "    -0.3131193 ]\n",
            "   [-0.1338626  -0.48489454  2.2404158  ...  2.580045    5.5561175\n",
            "    -0.2613805 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.957088   -0.45742294  2.4665911  ...  2.8531845   4.879974\n",
            "    -0.262826  ]\n",
            "   [ 0.35850555 -0.48054495  2.892068   ...  2.9818306   6.087369\n",
            "    -0.30132672]]]\n",
            "\n",
            "\n",
            " [[[ 0.8115011  -0.45427716  2.406576   ...  2.904704    4.920872\n",
            "    -0.23931763]\n",
            "   [ 0.27315217 -0.48823744  2.8476338  ...  2.9552872   6.0947356\n",
            "    -0.29677495]]]\n",
            "\n",
            "\n",
            " [[[ 0.87647814 -0.4434499   2.4396262  ...  2.8370712   5.070584\n",
            "    -0.23956509]\n",
            "   [ 0.18153685 -0.4937599   2.7218802  ...  2.9244375   6.060132\n",
            "    -0.2940557 ]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.07425802 -2.5158799  -2.5037656  ...  2.164635    4.6100154\n",
            "   3.96188   ]\n",
            " [ 0.07578148 -2.549979   -2.4426737  ...  2.0915718   4.702456\n",
            "   4.0854554 ]\n",
            " [ 0.31290466 -2.70417    -2.3007927  ...  1.9643869   4.517734\n",
            "   4.6243296 ]\n",
            " ...\n",
            " [-1.7104144  -2.7273736  -2.7780254  ...  0.13925686  5.4531503\n",
            "   4.5017376 ]\n",
            " [-1.8750998  -2.7374425  -2.6933346  ...  0.16852753  5.731225\n",
            "   4.357921  ]\n",
            " [-1.552016   -2.758329   -2.5523934  ...  0.57817596  5.5192847\n",
            "   4.3640437 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.87182546 -0.43129718  2.4336944  ...  2.7559345   5.1016755\n",
            "    -0.23936228]\n",
            "   [ 0.10192153 -0.50214547  2.6727488  ...  2.9342742   6.06999\n",
            "    -0.28693274]]]\n",
            "\n",
            "\n",
            " [[[ 1.1159099  -0.42592987  2.3812697  ...  2.7215137   5.210662\n",
            "    -0.24395066]\n",
            "   [ 0.06616135 -0.49646372  2.6228688  ...  2.9059625   6.0284786\n",
            "    -0.28214705]]]\n",
            "\n",
            "\n",
            " [[[ 1.1193694  -0.40117013  2.2828743  ...  2.754452    5.318308\n",
            "    -0.25503418]\n",
            "   [-0.02050177 -0.49110013  2.5518622  ...  2.8082814   5.9416924\n",
            "    -0.2801955 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.7741971  -0.44958743  2.4480112  ...  2.4352427   4.832724\n",
            "    -0.23910323]\n",
            "   [ 1.1828443  -0.49417683  3.202301   ...  3.0268295   6.2826304\n",
            "    -0.28850636]]]\n",
            "\n",
            "\n",
            " [[[ 0.60372823 -0.45244592  2.3521276  ...  2.4876134   4.6231356\n",
            "    -0.24181959]\n",
            "   [ 1.1748995  -0.49065924  3.2252517  ...  3.0798593   6.267411\n",
            "    -0.2823052 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.6673628  -0.4439599   2.3752697  ...  2.5285456   4.655521\n",
            "    -0.2419048 ]\n",
            "   [ 1.1316091  -0.49346545  3.2007482  ...  3.0362809   6.2552176\n",
            "    -0.2840936 ]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-1.5058684  -2.78183    -2.5329413  ...  0.6528025   5.4419575\n",
            "   4.264816  ]\n",
            " [-0.99415386 -2.8258224  -2.57922    ...  0.86369807  5.0049067\n",
            "   4.4088283 ]\n",
            " [-0.7966293  -2.8058846  -2.4704525  ...  1.1321942   5.019926\n",
            "   4.3601375 ]\n",
            " ...\n",
            " [-3.104771   -2.3900154  -3.500889   ... -0.6203797   6.3137555\n",
            "   3.7338276 ]\n",
            " [-3.497078   -2.3906755  -3.5306766  ... -1.0517098   6.5346084\n",
            "   3.5858715 ]\n",
            " [-3.2733557  -2.408713   -3.4610758  ... -0.8843837   6.402916\n",
            "   3.7056448 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.65477115 -0.43845123  2.3700163  ...  2.5855172   4.5884914\n",
            "    -0.2494864 ]\n",
            "   [ 1.0719512  -0.49324486  3.1580105  ...  3.043288    6.2275476\n",
            "    -0.28324968]]]\n",
            "\n",
            "\n",
            " [[[ 0.7001794  -0.43032435  2.4042766  ...  2.6259649   4.5916734\n",
            "    -0.2554597 ]\n",
            "   [ 0.96898705 -0.4960049   3.109169   ...  3.0213687   6.2273946\n",
            "    -0.2832528 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.81438327 -0.42359316  2.3894153  ...  2.6449873   4.595714\n",
            "    -0.25789437]\n",
            "   [ 0.89180213 -0.49381465  3.0860014  ...  3.0486293   6.1805706\n",
            "    -0.28876162]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.81168157 -0.42297703  1.684784   ...  1.3373233   4.6281824\n",
            "    -0.24079037]\n",
            "   [ 0.4706826  -0.57229656  2.8901963  ...  2.6567416   7.5580626\n",
            "    -0.27894297]]]\n",
            "\n",
            "\n",
            " [[[ 0.7463493  -0.4462062   1.6913633  ...  1.2223818   4.568145\n",
            "    -0.24468844]\n",
            "   [ 0.4750526  -0.5442956   3.1370766  ...  2.7637565   7.410342\n",
            "    -0.30041268]]]\n",
            "\n",
            "\n",
            " [[[ 0.3492512  -0.5101941   1.6605681  ...  1.1937978   4.4425697\n",
            "    -0.25582018]\n",
            "   [ 0.5964034  -0.54103786  3.1612456  ...  2.759673    7.2320895\n",
            "    -0.30600274]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-3.2151527  -2.4420474  -3.41704    ... -0.964217    6.329269\n",
            "   3.7684097 ]\n",
            " [-2.994108   -2.494637   -3.3326058  ... -0.8558838   6.155402\n",
            "   3.8835988 ]\n",
            " [-2.711282   -2.5413036  -3.3259075  ... -0.8166347   5.894122\n",
            "   4.0311837 ]\n",
            " ...\n",
            " [-3.485241   -3.0546954  -3.8755577  ... -0.7621865   5.227711\n",
            "   1.3685836 ]\n",
            " [-3.749527   -2.8951046  -4.001261   ... -0.88542724  5.5115323\n",
            "   1.3296442 ]\n",
            " [-4.500607   -2.674001   -3.9211895  ... -1.0962433   6.2341523\n",
            "   0.9606049 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "Processing layer_10:  79%|███████▉  | 19/24 [00:00<00:00, 93.34it/s]tf.Tensor(\n",
            "[[[[ 0.01722857 -0.46974614  1.6984457  ...  0.89488286  4.478372\n",
            "    -0.24879625]\n",
            "   [ 0.7293118  -0.5435326   3.2542224  ...  2.9749963   7.24759\n",
            "    -0.30347738]]]\n",
            "\n",
            "\n",
            " [[[-0.28799582 -0.44902223  1.6552993  ...  0.79907984  4.465758\n",
            "    -0.17538816]\n",
            "   [ 0.7795641  -0.54572946  3.2193081  ...  2.898942    7.2326493\n",
            "    -0.31739944]]]\n",
            "\n",
            "\n",
            " [[[-0.6055826  -0.42275724  1.6769439  ...  0.6096033   4.4916263\n",
            "    -0.17625886]\n",
            "   [ 0.9802112  -0.55064976  3.4391625  ...  3.185358    7.154032\n",
            "    -0.31848705]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.4973661  -0.46172425  2.1604574  ...  3.098231    5.220422\n",
            "    -0.30575925]\n",
            "   [ 0.43539914 -0.5271499   2.0044136  ...  2.7043355   5.70368\n",
            "    -0.35884982]]]\n",
            "\n",
            "\n",
            " [[[ 1.7101948  -0.45928466  2.113448   ...  3.1227362   5.267789\n",
            "    -0.30734098]\n",
            "   [ 0.5295265  -0.5262228   1.9713484  ...  2.637107    5.6440125\n",
            "    -0.36485684]]]\n",
            "\n",
            "\n",
            " [[[ 1.7478254  -0.45964456  1.9928932  ...  3.1382113   5.2990766\n",
            "    -0.31352818]\n",
            "   [ 0.54096234 -0.5226089   1.9806247  ...  2.6020045   5.5462966\n",
            "    -0.3522655 ]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-5.4049454  -2.5270371  -4.0250273  ... -1.2650559   6.9187613\n",
            "   0.41513517]\n",
            " [-5.9004526  -2.4132915  -3.983646   ... -1.2971706   7.3776107\n",
            "  -0.00897028]\n",
            " [-6.2841663  -2.2903433  -4.20365    ... -1.4711722   7.6115985\n",
            "  -0.05779538]\n",
            " ...\n",
            " [ 0.14120096 -2.8813608  -2.4201217  ...  1.0459175   4.065699\n",
            "   4.923754  ]\n",
            " [ 0.55806136 -2.8707454  -2.4281974  ...  1.2523963   3.7193558\n",
            "   5.1026273 ]\n",
            " [ 0.64981204 -2.8268445  -2.4078722  ...  1.3501731   3.7126138\n",
            "   5.078154  ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.6504526  -0.4733653   1.8260343  ...  3.1417863   5.425719\n",
            "    -0.3165268 ]\n",
            "   [ 0.30639613 -0.52505654  2.026092   ...  2.5533674   5.512755\n",
            "    -0.34296957]]]\n",
            "\n",
            "\n",
            " [[[ 1.6830503  -0.47746107  1.8391311  ...  3.1577516   5.6147413\n",
            "    -0.32604554]\n",
            "   [ 0.12470638 -0.5032675   2.1192522  ...  2.4625022   5.4679747\n",
            "    -0.32464567]]]\n",
            "\n",
            "\n",
            " [[[ 1.8257539  -0.49131295  2.2726886  ...  2.8915799   5.7681127\n",
            "    -0.31222573]\n",
            "   [ 0.07764716 -0.49477553  2.0875206  ...  2.344205    5.412942\n",
            "    -0.25859717]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.7193551  -0.4572055   2.64913    ...  3.5682497   5.4123645\n",
            "    -0.2937677 ]\n",
            "   [ 0.21111585 -0.43471044  2.3120396  ...  2.733434    5.1553483\n",
            "    -0.23613657]]]\n",
            "\n",
            "\n",
            " [[[ 1.7671195  -0.4579295   2.5662649  ...  3.5691726   5.418332\n",
            "    -0.29192668]\n",
            "   [ 0.06300949 -0.4356755   2.35621    ...  2.654756    5.145066\n",
            "    -0.22876404]]]\n",
            "\n",
            "\n",
            " [[[ 1.6352441  -0.45407674  2.414353   ...  3.6038818   5.3887815\n",
            "    -0.29108736]\n",
            "   [-0.02024271 -0.43105683  2.3415987  ...  2.6042097   5.1971765\n",
            "    -0.24017426]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.54853815 -2.8367138  -2.4203484  ...  1.3546848   4.0057216\n",
            "   4.8461933 ]\n",
            " [ 0.7553757  -2.8063283  -2.3662353  ...  1.6833465   4.1196127\n",
            "   4.9074597 ]\n",
            " [ 1.2690432  -2.681194   -2.347494   ...  2.2401738   3.7435384\n",
            "   5.105749  ]\n",
            " ...\n",
            " [ 1.049118   -2.6933653  -2.1899247  ...  1.4637332   4.053695\n",
            "   6.314212  ]\n",
            " [ 1.211091   -2.7113993  -2.1902013  ...  1.5289891   3.9730523\n",
            "   6.289349  ]\n",
            " [ 0.9746864  -2.743051   -2.1736054  ...  1.4074602   4.209552\n",
            "   6.0409694 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.46621    -0.45869648  2.2356489  ...  3.5984836   5.258553\n",
            "    -0.29612046]\n",
            "   [-0.06616883 -0.43083465  2.321969   ...  2.5760539   5.1937532\n",
            "    -0.24231711]]]\n",
            "\n",
            "\n",
            " [[[ 1.3279781  -0.46295094  2.1572514  ...  3.6578684   5.154042\n",
            "    -0.30110094]\n",
            "   [-0.04743397 -0.43239373  2.2776034  ...  2.544651    5.249898\n",
            "    -0.2541662 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.2014736  -0.45516914  2.1556315  ...  3.6393578   5.1382093\n",
            "    -0.30000943]\n",
            "   [-0.0550586  -0.4350697   2.2663934  ...  2.515509    5.318096\n",
            "    -0.24766126]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.8767359  -0.48743418  2.9830852  ...  3.9490483   6.1731687\n",
            "    -0.38077652]\n",
            "   [ 0.85914814 -0.4228866   2.2432027  ...  2.8099315   4.79845\n",
            "    -0.30459124]]]\n",
            "\n",
            "\n",
            " [[[ 1.8956369  -0.48956338  2.9929688  ...  3.9725718   6.162235\n",
            "    -0.3798504 ]\n",
            "   [ 0.95074874 -0.42591128  2.135255   ...  2.7816648   4.906713\n",
            "    -0.32087123]]]\n",
            "\n",
            "\n",
            " [[[ 1.887866   -0.489122    3.0287619  ...  3.9715772   6.1513214\n",
            "    -0.3743875 ]\n",
            "   [ 0.98134047 -0.43101794  2.1338289  ...  2.7753978   4.956779\n",
            "    -0.31938457]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.63300556 -2.7370906  -2.1883163  ...  1.1351503   4.436701\n",
            "   5.7222543 ]\n",
            " [ 0.35119438 -2.7596102  -2.1824837  ...  0.9014313   4.631001\n",
            "   5.5533166 ]\n",
            " [ 0.10552409 -2.7568526  -2.1501296  ...  0.8686615   4.838154\n",
            "   5.3761334 ]\n",
            " ...\n",
            " [ 1.4617885  -2.40957    -2.0691     ...  2.3679087   4.682013\n",
            "   7.191822  ]\n",
            " [ 1.438051   -2.4769044  -2.1053424  ...  2.2487147   4.578989\n",
            "   7.1554604 ]\n",
            " [ 1.386568   -2.4884312  -2.113826   ...  2.2188025   4.5842853\n",
            "   7.151163  ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.8416789  -0.4818439   3.0222754  ...  3.9551082   6.079909\n",
            "    -0.36817822]\n",
            "   [ 1.033948   -0.43178916  2.1393664  ...  2.8296583   5.040238\n",
            "    -0.3233934 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.9152449  -0.47998548  3.0366666  ...  3.994534    6.094713\n",
            "    -0.37432992]\n",
            "   [ 1.0612224  -0.43897638  2.090271   ...  2.8571432   5.0971265\n",
            "    -0.32179534]]]\n",
            "\n",
            "\n",
            " [[[ 1.9434298  -0.47734174  3.045952   ...  3.9594047   6.068109\n",
            "    -0.36835998]\n",
            "   [ 1.0592498  -0.43896797  2.0865505  ...  2.8549113   5.1447654\n",
            "    -0.32393014]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 2.0723424  -0.44539124  2.7058988  ...  3.1252253   5.7665114\n",
            "    -0.3281763 ]\n",
            "   [ 0.51649815 -0.4477844   2.2984293  ...  3.306496    4.799825\n",
            "    -0.28743875]]]\n",
            "\n",
            "\n",
            " [[[ 2.0998006  -0.44321904  2.7592676  ...  3.2793741   5.921586\n",
            "    -0.33456686]\n",
            "   [ 0.5596672  -0.41817483  2.382136   ...  3.06125     4.7967343\n",
            "    -0.32141837]]]\n",
            "\n",
            "\n",
            " [[[ 2.0013022  -0.43451542  2.7385266  ...  3.2868946   5.86861\n",
            "    -0.33530787]\n",
            "   [ 0.5297107  -0.4203511   2.5021176  ...  2.9789033   4.8082695\n",
            "    -0.30734682]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 1.1659877 -2.5242097 -2.1708255 ...  2.0041544  4.6311293  7.0640087]\n",
            " [ 1.2804635 -2.5938332 -2.1884913 ...  1.9742416  4.477727   7.161613 ]\n",
            " [ 1.2904241 -2.6202931 -2.225898  ...  1.9200832  4.380161   7.137078 ]\n",
            " ...\n",
            " [ 1.432205  -2.5587595 -2.29837   ...  1.9873035  3.5310197  6.498103 ]\n",
            " [ 1.6168613 -2.4652808 -2.2107875 ...  2.3844202  3.7680633  6.664659 ]\n",
            " [ 1.3675982 -2.4064355 -2.2643497 ...  2.2359345  4.0298557  6.5428476]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 2.030677   -0.44007614  2.8163323  -0.22023006  0.5898158\n",
            "     3.303007    6.0360985  -0.33926514]\n",
            "   [ 0.6552676  -0.4420004   2.4554677  -0.17808542  1.1485611\n",
            "     2.8238347   4.801036   -0.31887555]]]\n",
            "\n",
            "\n",
            " [[[ 1.9687142  -0.44397363  2.847862   -0.22174773  0.61033475\n",
            "     3.3244026   6.038877   -0.3406438 ]\n",
            "   [ 0.5137205  -0.425146    2.7005649  -0.18564112  1.100534\n",
            "     2.787792    4.6691623  -0.2834905 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.9389356  -0.4542449   2.903357   -0.21610454  0.6124853\n",
            "     3.3987296   6.1456656  -0.345155  ]\n",
            "   [ 0.6834383  -0.44820446  2.6788003  -0.25935867  0.94780004\n",
            "     2.7505386   4.910075   -0.23287459]]]\n",
            "\n",
            "\n",
            " [[[ 1.9484751  -0.45926878  2.9361055  -0.21255311  0.6215358\n",
            "     3.444371    6.1924667  -0.35295287]\n",
            "   [ 0.5485799  -0.44094685  2.7063863  -0.23645279  1.1084778\n",
            "     2.828996    4.716704   -0.21737027]]]\n",
            "\n",
            "\n",
            " [[[ 1.9821537  -0.47088188  2.9718418  -0.201765    0.63264006\n",
            "     3.5318637   6.319147   -0.3537626 ]\n",
            "   [ 0.59007204 -0.41139755  2.6230783  -0.22450642  1.2163409\n",
            "     2.7599394   4.6924396  -0.2506137 ]]]\n",
            "\n",
            "\n",
            " [[[ 2.057512   -0.4850409   2.965347   -0.1935767   0.6201978\n",
            "     3.573765    6.413164   -0.3579697 ]\n",
            "   [ 0.66346437 -0.4257095   2.550738   -0.22478592  1.1638423\n",
            "     2.6908467   4.7534127  -0.2752925 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.950156   -0.49394387  2.9662971  -0.19142638  0.7062771\n",
            "     3.6502156   6.3823757  -0.36387303]\n",
            "   [ 0.88657814 -0.42963234  2.184684   -0.21320103  1.0529169\n",
            "     2.5363915   4.919843   -0.31779954]]]\n",
            "\n",
            "\n",
            " [[[ 1.7843714  -0.49529392  2.9253545  -0.1943362   0.73787314\n",
            "     3.6455598   6.315577   -0.3649796 ]\n",
            "   [ 0.92934346 -0.42710635  2.155172   -0.20951301  1.0049149\n",
            "     2.570623    4.9734154  -0.31444263]]]\n",
            "\n",
            "\n",
            " [[[ 1.6917403  -0.49138424  2.923693   -0.202309    0.7252385\n",
            "     3.6472845   6.305879   -0.36558744]\n",
            "   [ 0.8521495  -0.42389748  2.1732821  -0.21506013  0.9661709\n",
            "     2.5546107   4.983915   -0.3073671 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.6242216  -0.49462312  2.872154   -0.1995295   0.75136626\n",
            "     3.6741416   6.2870674  -0.37171525]\n",
            "   [ 0.8060405  -0.4186303   2.1815498  -0.22124848  0.95022434\n",
            "     2.572885    4.9934435  -0.30129924]]]\n",
            "\n",
            "\n",
            " [[[ 1.6665251  -0.48622614  2.862204   -0.20085308  0.7441044\n",
            "     3.6456952   6.23847    -0.3702819 ]\n",
            "   [ 0.72979826 -0.41761437  2.2337143  -0.21829171  0.96969974\n",
            "     2.5832458   4.90311    -0.28783026]]]\n",
            "\n",
            "\n",
            " [[[ 1.6668154  -0.48795235  2.8661678  -0.19981751  0.76171935\n",
            "     3.6981316   6.1876397  -0.36494422]\n",
            "   [ 0.71454656 -0.41995245  2.2847483  -0.21455604  0.99605435\n",
            "     2.5511029   4.858655   -0.29034778]]]\n",
            "\n",
            "\n",
            " [[[ 1.6953428  -0.4854104   2.8680065  -0.1991942   0.7533553\n",
            "     3.7418694   6.1428576  -0.36857504]\n",
            "   [ 0.6946044  -0.4207369   2.315831   -0.19231826  0.98616713\n",
            "     2.694148    4.940915   -0.28541136]]]\n",
            "\n",
            "\n",
            " [[[ 1.7660253  -0.48396388  2.8921418  -0.19711375  0.7247007\n",
            "     3.7458737   6.097101   -0.3665613 ]\n",
            "   [ 0.7144412  -0.42657545  2.406443   -0.1849256   1.0601711\n",
            "     2.8563116   4.8903985  -0.2770695 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.8515472  -0.4801009   2.8777826  -0.19260451  0.6998744\n",
            "     3.8207552   6.096119   -0.36516362]\n",
            "   [ 0.7699717  -0.43596232  2.3843842  -0.18581958  1.1276298\n",
            "     2.9793377   4.828252   -0.26045603]]]\n",
            "\n",
            "\n",
            " [[[ 1.8855369  -0.4725026   2.9261956  -0.19286554  0.7080655\n",
            "     3.8731046   6.0492496  -0.3674153 ]\n",
            "   [ 0.71376395 -0.4392374   2.3463879  -0.1818234   1.127492\n",
            "     3.0228267   4.837862   -0.2548459 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.8965405  -0.47159848  2.9380536  -0.19684505  0.70688325\n",
            "     3.8383837   6.0330462  -0.36178878]\n",
            "   [ 0.74933946 -0.43560234  2.3036008  -0.18319789  1.1438568\n",
            "     2.9963915   4.840121   -0.26133683]]]\n",
            "\n",
            "\n",
            " [[[ 1.9117573  -0.47347075  2.9767928  -0.19363837  0.72066253\n",
            "     3.8442156   6.0059633  -0.35950464]\n",
            "   [ 0.7352615  -0.43866083  2.328126   -0.17725742  1.1506783\n",
            "     3.0062077   4.824507   -0.26145965]]]\n",
            "\n",
            "\n",
            " [[[ 1.9717796  -0.47032592  3.008598   -0.19827989  0.717973\n",
            "     3.8727074   5.998828   -0.35775906]\n",
            "   [ 0.7046753  -0.4417593   2.3306613  -0.18229482  1.1699932\n",
            "     3.0122507   4.8048153  -0.25541165]]]\n",
            "\n",
            "\n",
            " [[[ 2.1173859  -0.46397308  3.0816064  -0.20276305  0.68596613\n",
            "     3.8387425   6.0247927  -0.35299283]\n",
            "   [ 0.71075135 -0.44885433  2.3026514  -0.18635605  1.1420292\n",
            "     3.055552    4.8334317  -0.25397968]]]], shape=(20, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 1.4953027  -2.3246255  -2.2408319  -0.83291954  3.2560098   2.539866\n",
            "   4.123515    6.5737762 ]\n",
            " [ 1.4172384  -2.1998937  -2.2472484  -0.90953106  2.9079242   2.5779934\n",
            "   4.427719    6.6534925 ]\n",
            " [ 1.1611471  -2.2709575  -2.3845856  -0.7050501   3.4259803   2.4058418\n",
            "   4.6655116   6.5988693 ]\n",
            " [ 1.3993846  -2.225282   -2.2417102  -0.94519615  3.1572201   2.6527624\n",
            "   4.6622376   6.820458  ]\n",
            " [ 1.6148936  -2.2148683  -2.1544046  -1.1055597   3.503456    2.9249964\n",
            "   4.681508    6.9506884 ]\n",
            " [ 1.749695   -2.2495253  -2.1851254  -1.2347107   3.9004653   2.995147\n",
            "   4.6041474   6.9828086 ]\n",
            " [ 1.5962218  -2.333723   -2.1575212  -0.9458006   4.956089    2.8078697\n",
            "   4.5764556   6.7558107 ]\n",
            " [ 1.2017087  -2.3381116  -2.1890888  -0.5874984   4.993565    2.549424\n",
            "   4.843778    6.537165  ]\n",
            " [ 1.0715001  -2.3336985  -2.1487772  -0.4632561   4.9298224   2.537103\n",
            "   4.998885    6.4505634 ]\n",
            " [ 0.94353324 -2.3506608  -2.1507409  -0.31838816  4.8925667   2.437028\n",
            "   5.1303625   6.3794694 ]\n",
            " [ 1.0695179  -2.321956   -2.1253316  -0.44891104  4.6424932   2.4799974\n",
            "   5.0056844   6.4415526 ]\n",
            " [ 1.1012294  -2.2930462  -2.1020222  -0.47217077  4.5499616   2.4703243\n",
            "   5.0238557   6.5251746 ]\n",
            " [ 1.0418094  -2.3951383  -2.1478684  -0.37183276  4.3147383   2.2707326\n",
            "   4.9627295   6.6242986 ]\n",
            " [ 1.0722133  -2.4042323  -2.1852863  -0.46935153  3.8942146   2.1806378\n",
            "   4.865329    6.8093987 ]\n",
            " [ 1.2058429  -2.4404476  -2.186061   -0.6528371   3.8002424   2.1272206\n",
            "   4.7265043   7.017097  ]\n",
            " [ 1.3332177  -2.504656   -2.1311953  -0.6954099   3.7476895   2.1001263\n",
            "   4.5612645   7.1630535 ]\n",
            " [ 1.3640928  -2.4952526  -2.1204233  -0.7024278   3.8115337   2.134481\n",
            "   4.480774    7.129375  ]\n",
            " [ 1.4038738  -2.4899793  -2.1124203  -0.7322792   3.7379768   2.1309512\n",
            "   4.436715    7.201006  ]\n",
            " [ 1.5641558  -2.5073762  -2.0818114  -0.85356426  3.6906133   2.1876621\n",
            "   4.302824    7.3329673 ]\n",
            " [ 1.8199339  -2.5622702  -2.1004162  -1.0835232   3.6983886   2.2579951\n",
            "   3.9769657   7.4903245 ]], shape=(20, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "Processing layer_10: 100%|██████████| 24/24 [00:00<00:00, 92.95it/s]\n",
            "Fingerprint after layer_10. ((5908, 8) inputs, (5908, 8) neurons)\n",
            "Inputs: (neuron signature (On/Off activations) dataset)(labels dataset)\n",
            "(5908, 8) (5908,)\n",
            "\n",
            "RULES FROM LAYER LAYER_10 IN TERMS OF FEATURES\n",
            "\n",
            "Obtained all paths\n",
            "Processing paths for validation set: 100%|██████████| 189/189 [00:00<00:00, 833568.30it/s]\n",
            "InV 0\n",
            "\n",
            "Fingerprinting VAL data after layer_10 layer\n",
            "tf.Tensor(\n",
            "[[[[ 2.1276286  -0.45872194  3.0703611  ...  3.8000035   5.975162\n",
            "    -0.35156393]\n",
            "   [ 0.6832804  -0.45225006  2.3075774  ...  3.088109    4.878751\n",
            "    -0.25065225]]]\n",
            "\n",
            "\n",
            " [[[ 2.1631389  -0.46149826  3.0903087  ...  3.7597666   5.9435205\n",
            "    -0.34818932]\n",
            "   [ 0.5795801  -0.44570726  2.2464976  ...  3.073068    4.849266\n",
            "    -0.24122323]]]\n",
            "\n",
            "\n",
            " [[[ 2.1687045  -0.45851028  3.0923743  ...  3.7212481   5.9222665\n",
            "    -0.3482495 ]\n",
            "   [ 0.5854442  -0.45273954  2.2588687  ...  3.110774    4.9263167\n",
            "    -0.24556194]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.91119856 -0.501205    1.9101183  ...  1.708681    5.9174843\n",
            "    -0.33751863]\n",
            "   [ 0.27170566 -0.47742683  2.0707712  ...  2.2555883   6.384035\n",
            "    -0.25167108]]]\n",
            "\n",
            "\n",
            " [[[ 0.86899155 -0.5049854   2.0100675  ...  1.8889083   5.9883738\n",
            "    -0.34192958]\n",
            "   [ 0.23442528 -0.47809556  2.004123   ...  2.2129211   6.315107\n",
            "    -0.24214305]]]\n",
            "\n",
            "\n",
            " [[[ 0.8075959  -0.51460475  2.1112816  ...  1.8732309   5.945819\n",
            "    -0.30717918]\n",
            "   [ 0.2807172  -0.4753329   2.0644155  ...  2.3918772   6.167274\n",
            "    -0.25091094]]]], shape=(1478, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 1.7887452 -2.600243  -2.1392412 ...  2.1567411  3.8960261  7.447544 ]\n",
            " [ 1.9812506 -2.626962  -2.085217  ...  2.2301388  3.6764011  7.468869 ]\n",
            " [ 1.912619  -2.6655228 -2.1161985 ...  2.1755545  3.6417542  7.420916 ]\n",
            " ...\n",
            " [-1.5540088 -2.643988  -3.0561888 ...  1.4508435  5.37698    2.1470428]\n",
            " [-1.4061325 -2.6414475 -2.9023414 ...  1.6058913  5.488998   2.3903215]\n",
            " [-1.5358373 -2.5626435 -2.884534  ...  1.5369734  5.613146   2.5327067]], shape=(1478, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "Fingerprint after layer_10. ((1478, 8) inputs, (1478, 8) neurons)\n",
            "PRINTING ALL RULES.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "_output_path = \"/content/ProphecyPlus/results/kjt/rules_cte_1/ruleset.csv\"\n",
        "\n",
        "\n",
        "print(\"****** RULES ********\")\n",
        "df_op = pd.read_csv(_output_path)\n",
        "df_op = df_op[df_op['test_precision'] > 90.0]\n",
        "df_op = df_op[df_op['label'] == 1]\n",
        "\n",
        "\n",
        "df_op.to_csv(\"./rules_cte_10_1.csv\",index=False)\n",
        "\n",
        "\n",
        "df_op = pd.read_csv(_output_path)\n",
        "df_op = df_op[df_op['test_precision'] > 90.0]\n",
        "df_op = df_op[df_op['label'] == 0]\n",
        "\n",
        "\n",
        "df_op.to_csv(\"./rules_cte_10_0.csv\",index=False)\n",
        "\n",
        "#df_op = df_op.reset_index()  # Reset index to make 'index' column\n",
        "#df_op\n",
        "#top_df_cte_13 = df_op[df_op['index'] == 0]  # Use 'index' column instead of 'row'\n",
        "\n",
        "#top_df_cte_13"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmpCVTLpqw7a",
        "outputId": "99bf2588-023c-4e4e-d4a9-4c109ef034b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** RULES ********\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wd = Path('/content/ProphecyPlus/results/kjt/rules_cte_1/')\n",
        "run_analyze_command(model,\"./x_train_npy.npy\",\"./y_train_he_npy.npy\",\"./x_test_npy.npy\",\"./y_test_he_npy.npy\",\"layer_10\",3,0,False,False,wd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF9XL8VxVFIx",
        "outputId": "12b0cef1-1240-48ac-cba1-beb9eb879fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONFIG PARAMS: LAYER NAME: layer_10 ,TYPE: 3 ,INP TYPE: 0 ,ACTS: False ,Top/All: False\n",
            "Layer Name: layer_10\n",
            "Layers to be considered for fingerprinting: ['layer_10']\n",
            "Invoking Dec-tree classifier based on FEATURES\n",
            "\n",
            "Fingerprinting TRAIN data after layer_10 layer\n",
            "Processing layer_10:   0%|          | 0/24 [00:00<?, ?it/s]tf.Tensor(\n",
            "[[[[-0.26016584 -0.43339378  2.2210205  ...  2.5751863   4.813346\n",
            "    -0.2545323 ]\n",
            "   [ 0.98201674 -0.43722454  3.0293176  ...  4.171374    4.887096\n",
            "    -0.29122013]]]\n",
            "\n",
            "\n",
            " [[[-0.18652447 -0.4324616   2.2048576  ...  2.6458187   4.772342\n",
            "    -0.25256512]\n",
            "   [ 1.0005517  -0.4410913   2.9933293  ...  4.2622576   4.9028606\n",
            "    -0.29046923]]]\n",
            "\n",
            "\n",
            " [[[-0.11948881 -0.43224463  2.210664   ...  2.6556752   4.703244\n",
            "    -0.24672768]\n",
            "   [ 1.0262357  -0.44452363  3.0207863  ...  4.361699    4.8975844\n",
            "    -0.2941592 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.9637208  -0.42356405  2.7032819  ...  2.80697     4.7505593\n",
            "    -0.23325606]\n",
            "   [ 1.5171398  -0.45405763  3.1704452  ...  3.8166356   6.071677\n",
            "    -0.2802743 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.62951213 -0.40280724  2.6120062  ...  2.6688855   4.489611\n",
            "    -0.21430571]\n",
            "   [ 1.6079441  -0.46705773  3.2359548  ...  3.9210074   6.1389837\n",
            "    -0.271785  ]]]\n",
            "\n",
            "\n",
            " [[[ 0.5021482  -0.39605284  2.5866833  ...  2.6958737   4.4166527\n",
            "    -0.21439838]\n",
            "   [ 1.6255888  -0.48035645  3.3096368  ...  4.000192    6.1305647\n",
            "    -0.2720794 ]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-4.457205  -1.948548  -3.0069826 ... -1.2161914  8.097293   3.6387482]\n",
            " [-4.3902693 -2.0272279 -3.0420167 ... -1.389712   7.9507484  3.773254 ]\n",
            " [-4.354619  -2.0635738 -3.1056032 ... -1.5577652  7.80088    3.8919835]\n",
            " ...\n",
            " [-3.1147358 -2.5729704 -3.658525  ... -1.5251598  6.032284   4.794036 ]\n",
            " [-4.0242867 -2.5113208 -3.8059726 ... -2.156045   6.539422   4.270197 ]\n",
            " [-4.3546286 -2.4867432 -3.8454487 ... -2.3913746  6.8256297  4.2003183]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.2945927  -0.40944356  2.4705894  ...  2.7538157   4.313176\n",
            "    -0.2262594 ]\n",
            "   [ 1.642166   -0.4838804   3.308957   ...  4.0255175   6.110197\n",
            "    -0.2754134 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.02331182 -0.41847667  2.3130493  ...  2.7739213   4.1687484\n",
            "    -0.2182032 ]\n",
            "   [ 1.6270735  -0.48757473  3.307221   ...  4.025069    6.054591\n",
            "    -0.25894448]]]\n",
            "\n",
            "\n",
            " [[[ 0.08693323 -0.41155493  2.308256   ...  2.8093865   4.3523707\n",
            "    -0.2209701 ]\n",
            "   [ 1.6121562  -0.4870502   3.2389073  ...  3.9863951   5.9107294\n",
            "    -0.26591173]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.2558877  -0.43960044  2.7361324  ...  3.5275197   4.8821745\n",
            "    -0.2933403 ]\n",
            "   [ 0.92073274 -0.46044797  3.2807178  ...  3.9787374   5.525749\n",
            "    -0.24444312]]]\n",
            "\n",
            "\n",
            " [[[ 1.1765486  -0.43638358  2.752878   ...  3.5033934   4.8120904\n",
            "    -0.28814203]\n",
            "   [ 0.8504875  -0.4421609   3.2470806  ...  3.9105177   5.56613\n",
            "    -0.26643878]]]\n",
            "\n",
            "\n",
            " [[[ 1.1241825  -0.4318964   2.7451065  ...  3.4897544   4.752567\n",
            "    -0.28844988]\n",
            "   [ 0.7836656  -0.44641083  3.235027   ...  3.8569093   5.579747\n",
            "    -0.26377976]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-4.77587   -2.460104  -3.842127  ... -2.6657782  7.2082944  3.9902217]\n",
            " [-5.292351  -2.4003975 -3.8203008 ... -2.9648197  7.6575074  3.6543174]\n",
            " [-4.9701524 -2.347891  -3.6809888 ... -2.5367706  7.6219606  3.7792156]\n",
            " ...\n",
            " [-1.7594391 -2.695713  -3.120762  ... -0.9665728  5.665867   6.2357826]\n",
            " [-1.8513744 -2.7059126 -3.0936565 ... -1.0389011  5.6866527  6.112764 ]\n",
            " [-1.8876712 -2.7072754 -3.0506678 ... -1.0461735  5.693456   6.030977 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.2844673  -0.44309783  2.6903489  ...  3.5076537   4.6931095\n",
            "    -0.28745386]\n",
            "   [ 0.7637842  -0.44439983  3.2175372  ...  3.8831596   5.5457516\n",
            "    -0.26962328]]]\n",
            "\n",
            "\n",
            " [[[ 1.2649641  -0.4468607   2.6690855  ...  3.536266    4.6734643\n",
            "    -0.29272646]\n",
            "   [ 0.75184864 -0.44551507  3.1992981  ...  3.8251925   5.546185\n",
            "    -0.27206355]]]\n",
            "\n",
            "\n",
            " [[[ 1.2816038  -0.44205517  2.6852136  ...  3.5235918   4.672925\n",
            "    -0.29955792]\n",
            "   [ 0.75108653 -0.44924167  3.1870813  ...  3.7994316   5.537158\n",
            "    -0.26595512]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-0.15149665 -0.42004165  2.4200432  ...  2.6833847   4.836601\n",
            "    -0.25897592]\n",
            "   [ 1.3319124  -0.41600344  2.9786701  ...  4.4829206   4.9891276\n",
            "    -0.24657123]]]\n",
            "\n",
            "\n",
            " [[[-0.10127237 -0.41907415  2.391985   ...  2.7633262   4.775067\n",
            "    -0.25496304]\n",
            "   [ 1.3418417  -0.41768146  2.9839363  ...  4.462092    4.9947076\n",
            "    -0.25349948]]]\n",
            "\n",
            "\n",
            " [[[-0.05349371 -0.4186569   2.3245163  ...  2.8623006   4.6789904\n",
            "    -0.24617669]\n",
            "   [ 1.3256288  -0.41764057  2.9531531  ...  4.4666424   4.9648757\n",
            "    -0.26192218]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-1.5977607 -2.7626178 -3.1034439 ... -1.1232705  5.330541   6.1802125]\n",
            " [-1.5861858 -2.7582312 -3.0768363 ... -1.1098584  5.349943   6.157692 ]\n",
            " [-1.526861  -2.7488666 -3.069363  ... -1.0671998  5.298257   6.168269 ]\n",
            " ...\n",
            " [-4.592199  -2.0446074 -3.1318338 ... -1.5377163  8.012997   4.020826 ]\n",
            " [-4.500476  -2.0742378 -3.1289997 ... -1.6056702  7.9101667  4.127909 ]\n",
            " [-4.375276  -2.1208994 -3.1240878 ... -1.7413268  7.762356   4.243509 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 8.9637435e-04 -4.2890719e-01  2.2420006e+00 ...  2.9011722e+00\n",
            "     4.6669974e+00 -2.4107249e-01]\n",
            "   [ 1.2861633e+00 -4.2174512e-01  2.9453769e+00 ...  4.4414296e+00\n",
            "     4.9440665e+00 -2.5664800e-01]]]\n",
            "\n",
            "\n",
            " [[[ 1.9070577e-02 -4.3223417e-01  2.1950064e+00 ...  2.9211295e+00\n",
            "     4.6315641e+00 -2.3677893e-01]\n",
            "   [ 1.1868577e+00 -4.2367807e-01  2.8743522e+00 ...  4.4514732e+00\n",
            "     4.7296433e+00 -2.4646364e-01]]]\n",
            "\n",
            "\n",
            " [[[-1.5123675e-02 -4.3693101e-01  2.1774950e+00 ...  2.8980598e+00\n",
            "     4.6145334e+00 -2.3357549e-01]\n",
            "   [ 1.2518770e+00 -4.2500979e-01  2.9754794e+00 ...  4.4833035e+00\n",
            "     4.2703176e+00 -2.3177277e-01]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-1.8569146e-01 -4.1876003e-01  2.1194873e+00 ...  2.8957410e+00\n",
            "     4.6846862e+00 -2.5341314e-01]\n",
            "   [ 1.1603460e+00 -4.3523800e-01  3.1268334e+00 ...  4.0576940e+00\n",
            "     5.0898304e+00 -2.9604337e-01]]]\n",
            "\n",
            "\n",
            " [[[-2.8288352e-01 -4.2669219e-01  2.1640987e+00 ...  2.7214737e+00\n",
            "     4.8351822e+00 -2.5939935e-01]\n",
            "   [ 1.0216421e+00 -4.2564943e-01  3.0319331e+00 ...  3.9936786e+00\n",
            "     4.9470134e+00 -2.9840305e-01]]]\n",
            "\n",
            "\n",
            " [[[-3.0040878e-01 -4.3839452e-01  2.2140303e+00 ...  2.5837636e+00\n",
            "     4.8981857e+00 -2.5240961e-01]\n",
            "   [ 9.8328066e-01 -4.2291522e-01  3.0224354e+00 ...  4.0522552e+00\n",
            "     4.9105811e+00 -2.7957243e-01]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-4.2483263 -2.1469276 -3.139227  ... -1.7656794  7.6647673  4.270034 ]\n",
            " [-3.9790375 -2.1025403 -3.0150793 ... -1.6256521  7.507927   4.370911 ]\n",
            " [-3.872204  -1.813519  -2.9174616 ... -1.3818175  7.678281   4.520206 ]\n",
            " ...\n",
            " [-4.5421963 -2.0432785 -3.0290391 ... -1.4517606  8.179376   3.8518968]\n",
            " [-4.3973274 -1.9469923 -2.9228444 ... -1.0638764  8.202885   3.6699142]\n",
            " [-4.38785   -1.919303  -2.9638946 ... -0.9990241  8.173242   3.5925715]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[-0.28638956 -0.44328701  2.2314246  ...  2.542456    4.910076\n",
            "    -0.2540363 ]\n",
            "   [ 0.9972938  -0.44669774  3.0084677  ...  4.1393847   4.8968596\n",
            "    -0.27969217]]]\n",
            "\n",
            "\n",
            " [[[-0.20291579 -0.44470707  2.342267   ...  2.5647683   4.9252143\n",
            "    -0.25389007]\n",
            "   [ 0.9981139  -0.44542548  3.0003066  ...  4.1344366   4.932055\n",
            "    -0.28136313]]]\n",
            "\n",
            "\n",
            " [[[-0.19253974 -0.44727677  2.3976786  ...  2.5263665   4.9226646\n",
            "    -0.257702  ]\n",
            "   [ 1.0576462  -0.44514206  2.9771347  ...  4.1882124   4.9282103\n",
            "    -0.27630934]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-0.2568779  -0.42112592  2.4036689  ...  2.5209203   4.7134023\n",
            "    -0.25616738]\n",
            "   [ 1.1298703  -0.4421002   2.9847033  ...  4.35973     4.9627495\n",
            "    -0.27336043]]]\n",
            "\n",
            "\n",
            " [[[-0.27129278 -0.41157663  2.3580706  ...  2.495857    4.6930776\n",
            "    -0.2532306 ]\n",
            "   [ 1.1639844  -0.44379577  2.9956417  ...  4.3368015   4.9851823\n",
            "    -0.27524716]]]\n",
            "\n",
            "\n",
            " [[[-0.22745307 -0.41408545  2.3350348  ...  2.536842    4.7155714\n",
            "    -0.2541727 ]\n",
            "   [ 1.1790556  -0.44507456  2.987473   ...  4.3177857   5.0064964\n",
            "    -0.2705574 ]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-4.4257193  -1.9282677  -2.9807978  ... -1.0352212   8.149955\n",
            "   3.589972  ]\n",
            " [-4.31278    -1.9517741  -2.9469998  ... -0.9640379   8.043581\n",
            "   3.731534  ]\n",
            " [-4.334705   -1.9464623  -2.9626615  ... -0.98840845  7.990105\n",
            "   3.754518  ]\n",
            " ...\n",
            " [-4.6021957  -1.9943551  -3.0199702  ... -1.3902731   7.944337\n",
            "   3.7558537 ]\n",
            " [-4.6609163  -1.9858574  -3.0667398  ... -1.4425746   7.958831\n",
            "   3.6732218 ]\n",
            " [-4.6318803  -2.0038505  -3.083852   ... -1.4540873   7.966959\n",
            "   3.6959908 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[-0.22347422 -0.41223466  2.2880363  ...  2.624777    4.56918\n",
            "    -0.24275549]\n",
            "   [ 1.1558398  -0.44499266  3.0006592  ...  4.324402    5.0476904\n",
            "    -0.26891676]]]\n",
            "\n",
            "\n",
            " [[[-0.12913215 -0.4208067   2.2281084  ...  2.704123    4.499159\n",
            "    -0.24043453]\n",
            "   [ 1.1699522  -0.43934742  2.9768136  ...  4.293225    5.0957046\n",
            "    -0.2612269 ]]]\n",
            "\n",
            "\n",
            " [[[-0.0321148  -0.4198573   2.1843069  ...  2.7557418   4.4357944\n",
            "    -0.2298562 ]\n",
            "   [ 1.1834464  -0.44279754  2.9883244  ...  4.3457866   5.0817933\n",
            "    -0.26405242]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.71226573 -0.4277592   2.5915935  ...  2.8202019   4.56738\n",
            "    -0.23528792]\n",
            "   [ 1.550194   -0.4847257   3.286548   ...  4.068661    6.1456623\n",
            "    -0.27282855]]]\n",
            "\n",
            "\n",
            " [[[ 0.7716956  -0.41333368  2.6184216  ...  2.8042686   4.706711\n",
            "    -0.2335366 ]\n",
            "   [ 1.5184163  -0.4854835   3.2830875  ...  4.0598617   6.0884767\n",
            "    -0.26612312]]]\n",
            "\n",
            "\n",
            " [[[ 0.718638   -0.41791707  2.5681047  ...  2.822566    4.771684\n",
            "    -0.22995564]\n",
            "   [ 1.4720275  -0.48268616  3.2274635  ...  4.007796    6.0167623\n",
            "    -0.26098853]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-4.673673  -2.0680885 -3.119785  ... -1.7370135  7.886817   3.7503583]\n",
            " [-4.527275  -2.1385076 -3.1671114 ... -1.8716995  7.683101   3.8443727]\n",
            " [-4.400014  -2.1852682 -3.2211006 ... -2.013518   7.4916153  3.9813576]\n",
            " ...\n",
            " [-3.8926523 -2.6110506 -3.8081167 ... -2.1950104  6.530595   4.5479827]\n",
            " [-3.6909857 -2.585364  -3.7642286 ... -1.9244941  6.502752   4.626193 ]\n",
            " [-3.652455  -2.5526688 -3.6731555 ... -1.7605696  6.605845   4.5589705]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.55585927 -0.41318807  2.4849434  ...  2.805816    4.7349358\n",
            "    -0.22310774]\n",
            "   [ 1.4561652  -0.47872633  3.218633   ...  4.046014    5.9448605\n",
            "    -0.2624189 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.36614105 -0.40752664  2.4279828  ...  2.851608    4.621887\n",
            "    -0.22231087]\n",
            "   [ 1.4802496  -0.473722    3.1689627  ...  4.0869355   5.919163\n",
            "    -0.2791545 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.32469943 -0.40944117  2.3551     ...  2.848808    4.670869\n",
            "    -0.22145893]\n",
            "   [ 1.3569812  -0.46091568  3.1382854  ...  3.9678879   5.6989636\n",
            "    -0.27609044]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.59763336 -0.40436816  2.782832   ...  3.057978    4.762677\n",
            "    -0.23652272]\n",
            "   [ 0.50364363 -0.46941897  3.0504212  ...  3.4489996   5.8032045\n",
            "    -0.27312285]]]\n",
            "\n",
            "\n",
            " [[[ 0.6923541  -0.39707735  2.7849543  ...  3.0625322   4.7886057\n",
            "    -0.24373746]\n",
            "   [ 0.5224302  -0.4662295   3.0268946  ...  3.4618149   5.8181515\n",
            "    -0.2732399 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.71594584 -0.40092772  2.7688653  ...  3.0389125   4.7340856\n",
            "    -0.24406327]\n",
            "   [ 0.5559027  -0.46327105  3.0190027  ...  3.4509518   5.8123207\n",
            "    -0.28054512]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-3.9325035  -2.500385   -3.627351   ... -1.831507    6.8870206\n",
            "   4.363997  ]\n",
            " [-4.27969    -2.4855008  -3.5804372  ... -2.0590284   7.1538787\n",
            "   4.2086153 ]\n",
            " [-4.0689144  -2.369777   -3.4019833  ... -1.7014456   7.203134\n",
            "   4.1691737 ]\n",
            " ...\n",
            " [-2.4686112  -2.5835805  -2.6216283  ... -0.28637156  6.2094765\n",
            "   4.919277  ]\n",
            " [-2.3078637  -2.6199865  -2.6332088  ... -0.24263635  6.027954\n",
            "   5.0050445 ]\n",
            " [-2.3022187  -2.6138363  -2.6779022  ... -0.3311322   5.9416323\n",
            "   4.9837375 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.82201314 -0.41015622  2.773456   ...  3.1068656   4.70104\n",
            "    -0.254671  ]\n",
            "   [ 0.5965302  -0.46255425  2.9964316  ...  3.5298417   5.7954516\n",
            "    -0.27348194]]]\n",
            "\n",
            "\n",
            " [[[ 0.95959634 -0.4249346   2.716337   ...  3.1571667   4.6885433\n",
            "    -0.2711509 ]\n",
            "   [ 0.658852   -0.46418932  3.0046732  ...  3.5528045   5.7655606\n",
            "    -0.27737215]]]\n",
            "\n",
            "\n",
            " [[[ 1.1396888  -0.4495818   2.6722627  ...  3.2834907   4.769908\n",
            "    -0.28151864]\n",
            "   [ 0.7656321  -0.46357414  3.0100594  ...  3.5566785   5.7744107\n",
            "    -0.28067678]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.8301887  -0.4020927   2.660238   ...  3.3486347   4.3534226\n",
            "    -0.24411333]\n",
            "   [ 0.9899341  -0.4591455   3.1867552  ...  4.096067    5.394674\n",
            "    -0.24503218]]]\n",
            "\n",
            "\n",
            " [[[ 0.88038146 -0.41096538  2.7141902  ...  3.3299031   4.467941\n",
            "    -0.2519986 ]\n",
            "   [ 0.90047365 -0.47704837  3.137717   ...  3.9736998   5.3329687\n",
            "    -0.2460262 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.86107856 -0.409118    2.7546332  ...  3.3192306   4.5020776\n",
            "    -0.2479161 ]\n",
            "   [ 0.86727715 -0.4860326   3.1682432  ...  3.9418268   5.3953867\n",
            "    -0.23600549]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-2.1423085  -2.6697862  -2.7122028  ... -0.44485041  5.727787\n",
            "   5.185664  ]\n",
            " [-1.9582998  -2.6945388  -2.817158   ... -0.5627396   5.533854\n",
            "   5.3319693 ]\n",
            " [-1.7090908  -2.7415977  -2.9450932  ... -0.63390356  5.389324\n",
            "   5.5744324 ]\n",
            " ...\n",
            " [-2.641017   -2.5803003  -3.0594585  ... -1.6647552   5.896969\n",
            "   5.68042   ]\n",
            " [-2.3027074  -2.5399833  -2.8972049  ... -1.19492     5.782935\n",
            "   5.747623  ]\n",
            " [-2.330035   -2.5468416  -2.8754015  ... -1.1055918   5.8487005\n",
            "   5.7263074 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.0015117e+00 -4.1277653e-01  2.7575724e+00 ...  3.3632658e+00\n",
            "     4.6390319e+00 -2.5934401e-01]\n",
            "   [ 7.1832675e-01 -4.8596862e-01  3.1914680e+00 ...  3.8418887e+00\n",
            "     5.6173940e+00 -2.4527696e-01]]]\n",
            "\n",
            "\n",
            " [[[ 1.0354038e+00 -4.2298624e-01  2.7491696e+00 ...  3.3866835e+00\n",
            "     4.8397179e+00 -2.6998684e-01]\n",
            "   [ 5.5080557e-01 -4.6989471e-01  3.1757016e+00 ...  3.6343472e+00\n",
            "     5.6492910e+00 -2.5294873e-01]]]\n",
            "\n",
            "\n",
            " [[[ 9.7322994e-01 -4.2372775e-01  2.7645335e+00 ...  3.3618822e+00\n",
            "     4.9571891e+00 -2.6772594e-01]\n",
            "   [ 4.7353706e-01 -4.6287626e-01  3.0637305e+00 ...  3.5123692e+00\n",
            "     5.6924443e+00 -2.6790902e-01]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 3.2620530e-03 -4.4027126e-01  2.2654068e+00 ...  2.9118364e+00\n",
            "     4.4909086e+00 -2.2539359e-01]\n",
            "   [ 1.2642205e+00 -4.2793074e-01  2.8754916e+00 ...  4.4577761e+00\n",
            "     4.7430735e+00 -2.5212824e-01]]]\n",
            "\n",
            "\n",
            " [[[ 1.9142158e-01 -4.2822093e-01  2.3003647e+00 ...  3.1089849e+00\n",
            "     4.4691749e+00 -2.3001957e-01]\n",
            "   [ 1.1619060e+00 -4.4851154e-01  2.9471509e+00 ...  4.5566092e+00\n",
            "     4.7905664e+00 -2.4754545e-01]]]\n",
            "\n",
            "\n",
            " [[[ 3.4638643e-01 -4.1504115e-01  2.3805416e+00 ...  3.0998607e+00\n",
            "     4.4262519e+00 -2.3338129e-01]\n",
            "   [ 1.3695617e+00 -4.5361087e-01  3.1271930e+00 ...  4.6432376e+00\n",
            "     5.0689669e+00 -2.5313199e-01]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-2.0495152  -2.696601   -2.9062445  ... -0.9364122   5.6802115\n",
            "   5.7960176 ]\n",
            " [-1.737206   -2.6946507  -2.7854486  ... -0.4451537   5.7228065\n",
            "   5.777482  ]\n",
            " [-1.6703405  -2.7013693  -2.6671178  ... -0.15924989  5.7908025\n",
            "   5.6345596 ]\n",
            " ...\n",
            " [-4.0858116  -2.0903668  -3.0720773  ... -1.864391    7.4354186\n",
            "   4.39797   ]\n",
            " [-3.7480426  -2.2444787  -3.0454016  ... -1.9364293   7.121286\n",
            "   4.8495684 ]\n",
            " [-3.9439359  -2.3387053  -3.3485842  ... -2.333482    7.0234556\n",
            "   4.9617457 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.37488687 -0.42256033  2.3886364  ...  2.913814    4.334441\n",
            "    -0.23295231]\n",
            "   [ 1.3371114  -0.43316728  3.1645217  ...  4.657327    5.032683\n",
            "    -0.27212062]]]\n",
            "\n",
            "\n",
            " [[[ 0.4202268  -0.4329069   2.4113295  ...  2.8804572   4.3716607\n",
            "    -0.23570289]\n",
            "   [ 1.3645097  -0.41939747  3.131568   ...  4.5124936   5.0635295\n",
            "    -0.31462905]]]\n",
            "\n",
            "\n",
            " [[[ 0.5294581  -0.42483345  2.555229   ...  2.883997    4.277044\n",
            "    -0.2422359 ]\n",
            "   [ 1.4222682  -0.4215715   3.2961864  ...  4.5181394   5.178384\n",
            "    -0.3516038 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.40424004 -0.4831876   2.112017   ...  1.5813892   4.657303\n",
            "    -0.27423197]\n",
            "   [ 0.7562356  -0.4940462   3.0425375  ...  2.6358588   7.063868\n",
            "    -0.2923607 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.19578438 -0.4999675   1.9463894  ...  1.2976952   4.7306523\n",
            "    -0.28431726]\n",
            "   [ 0.8585798  -0.5186156   3.2018807  ...  2.8113313   7.0241566\n",
            "    -0.27419546]]]\n",
            "\n",
            "\n",
            " [[[ 0.12506527 -0.47927076  1.9848733  ...  1.3473548   4.7776237\n",
            "    -0.24056543]\n",
            "   [ 0.96678853 -0.5274284   3.1959522  ...  2.8121343   6.972017\n",
            "    -0.28357685]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-3.9559634  -2.2949498  -3.451394   ... -2.437639    6.8442287\n",
            "   4.8248653 ]\n",
            " [-3.8076081  -2.2650127  -3.419972   ... -2.2258613   6.7516427\n",
            "   4.787041  ]\n",
            " [-3.8051217  -2.2844663  -3.5338662  ... -2.3889785   6.597966\n",
            "   4.990404  ]\n",
            " ...\n",
            " [-3.9826953  -2.5865974  -3.5720031  ... -0.53517896  6.2811785\n",
            "   1.7905377 ]\n",
            " [-4.7128086  -2.4389312  -3.7714496  ... -0.7040492   6.875053\n",
            "   1.2216567 ]\n",
            " [-4.8559213  -2.3724914  -3.780958   ... -0.7324413   7.11242\n",
            "   1.2410754 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 3.8984187e-03 -4.5033139e-01  1.9024500e+00 ...  1.2731750e+00\n",
            "     4.7824674e+00 -2.0947360e-01]\n",
            "   [ 9.5184410e-01 -5.4138136e-01  3.2474988e+00 ...  2.8666360e+00\n",
            "     6.9061050e+00 -2.9842973e-01]]]\n",
            "\n",
            "\n",
            " [[[-4.3683622e-02 -4.1562393e-01  1.9343249e+00 ...  1.3135904e+00\n",
            "     4.7693119e+00 -1.8219785e-01]\n",
            "   [ 1.0170449e+00 -5.1963300e-01  3.3162205e+00 ...  2.9425254e+00\n",
            "     6.8629422e+00 -2.9820737e-01]]]\n",
            "\n",
            "\n",
            " [[[-1.5854795e-01 -3.8918811e-01  1.9608127e+00 ...  1.3153100e+00\n",
            "     4.7499266e+00 -1.4427948e-01]\n",
            "   [ 1.0656229e+00 -5.0459403e-01  3.3925750e+00 ...  3.0701087e+00\n",
            "     6.8354139e+00 -2.9781163e-01]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.1216224e+00 -4.3839362e-01  2.6261106e+00 ...  3.1212685e+00\n",
            "     5.7068977e+00 -2.2063573e-01]\n",
            "   [-1.8767534e-01 -4.6803588e-01  2.2017572e+00 ...  2.6110988e+00\n",
            "     5.1416206e+00 -2.5397581e-01]]]\n",
            "\n",
            "\n",
            " [[[ 1.1361405e+00 -4.5314533e-01  2.7977149e+00 ...  3.0237899e+00\n",
            "     5.6467147e+00 -2.0399462e-01]\n",
            "   [-1.9231939e-01 -4.6606842e-01  2.1764848e+00 ...  2.5799849e+00\n",
            "     5.1592765e+00 -2.4901064e-01]]]\n",
            "\n",
            "\n",
            " [[[ 1.2307731e+00 -4.4446495e-01  2.8721392e+00 ...  2.9573593e+00\n",
            "     5.6629868e+00 -2.1543178e-01]\n",
            "   [-2.0562150e-01 -4.6730205e-01  2.1790802e+00 ...  2.5470607e+00\n",
            "     5.1734295e+00 -2.5050989e-01]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-5.144497   -2.3071826  -3.8198762  ... -0.82794183  7.3858194\n",
            "   1.0231831 ]\n",
            " [-5.309732   -2.2655902  -3.8592577  ... -0.9532559   7.5729704\n",
            "   1.0931143 ]\n",
            " [-5.61974    -2.223705   -3.882256   ... -1.0909956   7.867322\n",
            "   1.0722414 ]\n",
            " ...\n",
            " [ 0.34833124 -2.5147614  -1.872302   ...  2.1849349   5.01399\n",
            "   5.237093  ]\n",
            " [ 0.41900328 -2.4923756  -1.9004469  ...  2.172656    4.844231\n",
            "   5.2749557 ]\n",
            " [ 0.6142302  -2.4922674  -1.9067498  ...  2.2994142   4.6229477\n",
            "   5.3412004 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "Processing layer_10:  46%|████▌     | 11/24 [00:00<00:00, 104.89it/s]tf.Tensor(\n",
            "[[[[ 1.3616353  -0.43504277  3.0507066  ...  2.75805     5.5656238\n",
            "    -0.24043517]\n",
            "   [-0.20692702 -0.46603757  2.187771   ...  2.5445244   5.2555647\n",
            "    -0.2519736 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.3748902  -0.43562505  3.1369007  ...  2.8138542   5.459497\n",
            "    -0.234472  ]\n",
            "   [-0.18782616 -0.46762916  2.1803322  ...  2.516514    5.285539\n",
            "    -0.25174734]]]\n",
            "\n",
            "\n",
            " [[[ 1.306252   -0.4359454   3.0680888  ...  2.8767347   5.397738\n",
            "    -0.23297298]\n",
            "   [-0.16969901 -0.4698556   2.1609347  ...  2.5056098   5.329001\n",
            "    -0.24830958]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.2607175  -0.43900576  2.387674   ...  2.8545883   4.7583838\n",
            "    -0.2868318 ]\n",
            "   [ 0.70756423 -0.48755687  3.0036535  ...  3.1091306   6.116461\n",
            "    -0.29205337]]]\n",
            "\n",
            "\n",
            " [[[ 1.2578678  -0.44248024  2.3279366  ...  2.8370717   4.677709\n",
            "    -0.2799595 ]\n",
            "   [ 0.5836546  -0.48184523  2.968123   ...  3.0612404   6.0900855\n",
            "    -0.29464152]]]\n",
            "\n",
            "\n",
            " [[[ 1.2304727  -0.44107267  2.3658574  ...  2.8815534   4.6293397\n",
            "    -0.27186322]\n",
            "   [ 0.44051486 -0.48116505  2.9261062  ...  3.0182204   6.049173\n",
            "    -0.2917481 ]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.7979926  -2.5082467  -1.9634334  ...  2.333206    4.1619787\n",
            "   5.3791094 ]\n",
            " [ 0.8409157  -2.527664   -1.9431512  ...  2.2384472   4.047505\n",
            "   5.5100245 ]\n",
            " [ 0.673208   -2.5575068  -1.9425912  ...  2.0733569   4.157108\n",
            "   5.42947   ]\n",
            " ...\n",
            " [-1.6488445  -2.747936   -3.2675302  ... -0.5153446   5.0913014\n",
            "   4.719999  ]\n",
            " [-1.5458674  -2.7715652  -3.2119212  ... -0.5125688   4.946219\n",
            "   4.6591077 ]\n",
            " [-1.3945029  -2.7916598  -3.0572233  ... -0.38499436  4.852022\n",
            "   4.735724  ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.2844563  -0.44579527  2.3355472  ...  2.898382    4.64985\n",
            "    -0.27097633]\n",
            "   [ 0.32011005 -0.47619736  2.8524096  ...  2.9524744   6.015839\n",
            "    -0.28771847]]]\n",
            "\n",
            "\n",
            " [[[ 1.3844684  -0.44740996  2.3394935  ...  2.8547478   4.7461042\n",
            "    -0.26115057]\n",
            "   [ 0.30660877 -0.47733024  2.8552575  ...  2.9288127   6.026408\n",
            "    -0.2900022 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.3177892  -0.4559186   2.2952785  ...  2.8376606   4.9059567\n",
            "    -0.26015583]\n",
            "   [ 0.25795254 -0.47600785  2.8754323  ...  2.9077635   6.0234747\n",
            "    -0.2903814 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.7809411  -0.44978428  2.2991786  ...  2.231063    4.880342\n",
            "    -0.26212656]\n",
            "   [ 0.9954784  -0.4857459   3.1585808  ...  2.990821    6.3823867\n",
            "    -0.284467  ]]]\n",
            "\n",
            "\n",
            " [[[ 0.5579111  -0.43621713  2.5303366  ...  2.2562587   4.9816823\n",
            "    -0.22913224]\n",
            "   [ 1.1483968  -0.48930725  3.2248924  ...  3.0367289   6.303703\n",
            "    -0.2859646 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.71320575 -0.42607173  2.60449    ...  2.2809157   5.1383142\n",
            "    -0.23284587]\n",
            "   [ 1.2152232  -0.4894542   3.2125778  ...  3.0471933   6.200951\n",
            "    -0.28510574]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-1.121376   -2.8279967  -2.9698172  ... -0.22280285  4.654335\n",
            "   4.773905  ]\n",
            " [-0.91845083 -2.8345737  -2.9741287  ... -0.02039311  4.5188403\n",
            "   4.81119   ]\n",
            " [-1.0007691  -2.804587   -2.9505131  ...  0.16746438  4.7927155\n",
            "   4.6836653 ]\n",
            " ...\n",
            " [-3.109196   -2.4623542  -3.6230152  ... -0.6597869   6.201588\n",
            "   3.3419826 ]\n",
            " [-3.4817243  -2.293115   -3.3865442  ... -0.31455606  6.7465734\n",
            "   3.400149  ]\n",
            " [-3.1216433  -2.2528727  -3.3609781  ... -0.0153387   6.583904\n",
            "   3.6640296 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.8477132  -0.4371411   2.6130822  ...  2.4031916   5.191813\n",
            "    -0.23259759]\n",
            "   [ 1.2189537  -0.48775494  3.2264295  ...  3.0340836   6.1657715\n",
            "    -0.2837079 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.92382085 -0.43952298  2.6100852  ...  2.4466658   5.183427\n",
            "    -0.23878537]\n",
            "   [ 1.2238634  -0.4931045   3.2339725  ...  3.0322397   6.148224\n",
            "    -0.2827644 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.9551409  -0.44462556  2.6111448  ...  2.4730484   5.174603\n",
            "    -0.24494676]\n",
            "   [ 1.2828631  -0.49090505  3.241602   ...  3.0767481   6.1483145\n",
            "    -0.27902484]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.595823   -0.48782402  2.4095297  ...  2.470652    5.670236\n",
            "    -0.3035807 ]\n",
            "   [-0.09601089 -0.48307577  2.2427876  ...  2.5252295   5.8011546\n",
            "    -0.2697268 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.5946729  -0.47907513  2.3647416  ...  2.8227096   5.6235003\n",
            "    -0.31412035]\n",
            "   [-0.08370595 -0.4859066   2.2259097  ...  2.5028732   5.837829\n",
            "    -0.26449654]]]\n",
            "\n",
            "\n",
            " [[[ 1.5960354  -0.47955295  2.3321753  ...  2.8921258   5.6177974\n",
            "    -0.3136957 ]\n",
            "   [-0.08320272 -0.48312673  2.2231736  ...  2.4850361   5.886863\n",
            "    -0.27140442]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-2.8299637  -2.281697   -3.3890986  ...  0.01763625  6.4452224\n",
            "   3.9362464 ]\n",
            " [-2.6799982  -2.2975175  -3.4040906  ...  0.01525529  6.322097\n",
            "   4.061545  ]\n",
            " [-2.6853824  -2.307013   -3.4541054  ... -0.07625785  6.302005\n",
            "   4.1290073 ]\n",
            " ...\n",
            " [ 0.48667702 -2.780112   -2.4743402  ...  1.9801128   3.9498181\n",
            "   4.49771   ]\n",
            " [ 0.5456351  -2.8824139  -2.3702319  ...  1.8352801   4.042528\n",
            "   4.8149133 ]\n",
            " [ 0.5306863  -2.9225276  -2.3843193  ...  1.7626736   4.074082\n",
            "   4.836216  ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.5823468  -0.4762602   2.286267   ...  2.881253    5.529124\n",
            "    -0.3168611 ]\n",
            "   [-0.09052605 -0.48085162  2.2452939  ...  2.5238335   5.9549603\n",
            "    -0.26665607]]]\n",
            "\n",
            "\n",
            " [[[ 1.2877678  -0.45342624  2.2363787  ...  2.8070054   5.4896975\n",
            "    -0.3121385 ]\n",
            "   [-0.08451293 -0.48302913  2.2427552  ...  2.5072887   5.982986\n",
            "    -0.26352128]]]\n",
            "\n",
            "\n",
            " [[[ 0.8455321  -0.46395218  2.211678   ...  2.760936    5.3755684\n",
            "    -0.25567836]\n",
            "   [-0.04783674 -0.4737502   2.2169993  ...  2.499887    5.968783\n",
            "    -0.2644302 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.8890441  -0.4168255   2.552314   ...  3.1000755   4.679377\n",
            "    -0.19300997]\n",
            "   [-0.00808319 -0.47375444  2.197586   ...  2.5961454   5.969635\n",
            "    -0.26281822]]]\n",
            "\n",
            "\n",
            " [[[ 0.5237513  -0.40874806  2.5241485  ...  2.7940216   5.220296\n",
            "    -0.16247933]\n",
            "   [ 0.01421359 -0.47926635  2.2518058  ...  2.5606914   6.010765\n",
            "    -0.2550747 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.79084873 -0.40516096  2.6113908  ...  2.874214    5.0066643\n",
            "    -0.1566458 ]\n",
            "   [ 0.07569356 -0.4829044   2.3450801  ...  2.595025    6.0855803\n",
            "    -0.26306456]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.40179622 -2.9702337  -2.4580443  ...  1.5384392   4.0564146\n",
            "   4.7661195 ]\n",
            " [-0.18742833 -2.897896   -2.4479911  ...  1.3804282   4.5829196\n",
            "   4.3461432 ]\n",
            " [-1.017376   -2.7795002  -2.356301   ...  1.145399    5.3153477\n",
            "   3.857333  ]\n",
            " ...\n",
            " [-0.8890351  -2.9181101  -2.18365    ...  0.39710674  4.6354218\n",
            "   4.6289845 ]\n",
            " [-1.5801293  -2.7021737  -2.1128898  ...  1.1137216   5.7488265\n",
            "   3.8761027 ]\n",
            " [-1.2345794  -2.7967486  -2.2342236  ...  0.8542631   5.1753917\n",
            "   4.297859  ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.7841968  -0.38653427  2.643857   ...  3.0734727   4.922495\n",
            "    -0.20185882]\n",
            "   [ 0.1246333  -0.48767272  2.4264996  ...  2.5720892   6.092429\n",
            "    -0.26859093]]]\n",
            "\n",
            "\n",
            " [[[ 0.8203859  -0.37947184  2.4811225  ...  2.8772433   4.9965124\n",
            "    -0.24493179]\n",
            "   [ 0.21626303 -0.48005486  2.4736507  ...  2.60675     6.14756\n",
            "    -0.28086856]]]\n",
            "\n",
            "\n",
            " [[[ 0.7782485  -0.4228188   2.4830146  ...  2.9164226   4.9383636\n",
            "    -0.29878974]\n",
            "   [ 0.24861947 -0.47589868  2.4945111  ...  2.585269    6.1469474\n",
            "    -0.27894393]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.37001    -0.46989068  2.5046813  ...  2.5804608   5.6779346\n",
            "    -0.2886006 ]\n",
            "   [-0.14910004 -0.4803507   2.2922907  ...  2.6515245   5.3033147\n",
            "    -0.25993988]]]\n",
            "\n",
            "\n",
            " [[[ 1.4335898  -0.49225998  2.322513   ...  2.1489139   5.950061\n",
            "    -0.29959676]\n",
            "   [-0.1580096  -0.4857258   2.2881117  ...  2.6379821   5.423597\n",
            "    -0.26088464]]]\n",
            "\n",
            "\n",
            " [[[ 1.3788166  -0.4994134   2.2514677  ...  2.1377091   5.9140935\n",
            "    -0.30223736]\n",
            "   [-0.16170324 -0.4857585   2.265078   ...  2.6275206   5.4376607\n",
            "    -0.26557967]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-1.2741848  -2.8016758  -2.2504492  ...  0.6608978   5.3000045\n",
            "   4.537328  ]\n",
            " [-1.4613317  -2.7754712  -2.4686143  ...  0.57487416  5.3566265\n",
            "   4.1967545 ]\n",
            " [-1.563589   -2.7571049  -2.4675214  ...  0.4816842   5.436392\n",
            "   4.196199  ]\n",
            " ...\n",
            " [ 0.4056442  -2.5202813  -2.2877107  ...  2.0905926   4.4586616\n",
            "   4.747303  ]\n",
            " [ 0.32228643 -2.4940708  -2.4816625  ...  2.410567    4.4408083\n",
            "   4.1129203 ]\n",
            " [ 0.19633663 -2.5004995  -2.5007699  ...  2.3048885   4.5096035\n",
            "   3.975904  ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.3270243  -0.49937287  2.2162676  ...  2.2028594   5.8629484\n",
            "    -0.30291277]\n",
            "   [-0.14605126 -0.4868871   2.265175   ...  2.6314285   5.4543533\n",
            "    -0.26046693]]]\n",
            "\n",
            "\n",
            " [[[ 1.2895522  -0.49565795  2.217028   ...  2.3799338   5.833417\n",
            "    -0.30400017]\n",
            "   [-0.13616058 -0.48425952  2.2301328  ...  2.5735145   5.474239\n",
            "    -0.2633685 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.3905041  -0.48156166  2.2836995  ...  2.75544     5.716791\n",
            "    -0.3131193 ]\n",
            "   [-0.1338626  -0.48489454  2.2404158  ...  2.580045    5.5561175\n",
            "    -0.2613805 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.957088   -0.45742294  2.4665911  ...  2.8531845   4.879974\n",
            "    -0.262826  ]\n",
            "   [ 0.35850555 -0.48054495  2.892068   ...  2.9818306   6.087369\n",
            "    -0.30132672]]]\n",
            "\n",
            "\n",
            " [[[ 0.8115011  -0.45427716  2.406576   ...  2.904704    4.920872\n",
            "    -0.23931763]\n",
            "   [ 0.27315217 -0.48823744  2.8476338  ...  2.9552872   6.0947356\n",
            "    -0.29677495]]]\n",
            "\n",
            "\n",
            " [[[ 0.87647814 -0.4434499   2.4396262  ...  2.8370712   5.070584\n",
            "    -0.23956509]\n",
            "   [ 0.18153685 -0.4937599   2.7218802  ...  2.9244375   6.060132\n",
            "    -0.2940557 ]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.07425802 -2.5158799  -2.5037656  ...  2.164635    4.6100154\n",
            "   3.96188   ]\n",
            " [ 0.07578148 -2.549979   -2.4426737  ...  2.0915718   4.702456\n",
            "   4.0854554 ]\n",
            " [ 0.31290466 -2.70417    -2.3007927  ...  1.9643869   4.517734\n",
            "   4.6243296 ]\n",
            " ...\n",
            " [-1.7104144  -2.7273736  -2.7780254  ...  0.13925686  5.4531503\n",
            "   4.5017376 ]\n",
            " [-1.8750998  -2.7374425  -2.6933346  ...  0.16852753  5.731225\n",
            "   4.357921  ]\n",
            " [-1.552016   -2.758329   -2.5523934  ...  0.57817596  5.5192847\n",
            "   4.3640437 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.87182546 -0.43129718  2.4336944  ...  2.7559345   5.1016755\n",
            "    -0.23936228]\n",
            "   [ 0.10192153 -0.50214547  2.6727488  ...  2.9342742   6.06999\n",
            "    -0.28693274]]]\n",
            "\n",
            "\n",
            " [[[ 1.1159099  -0.42592987  2.3812697  ...  2.7215137   5.210662\n",
            "    -0.24395066]\n",
            "   [ 0.06616135 -0.49646372  2.6228688  ...  2.9059625   6.0284786\n",
            "    -0.28214705]]]\n",
            "\n",
            "\n",
            " [[[ 1.1193694  -0.40117013  2.2828743  ...  2.754452    5.318308\n",
            "    -0.25503418]\n",
            "   [-0.02050177 -0.49110013  2.5518622  ...  2.8082814   5.9416924\n",
            "    -0.2801955 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.7741971  -0.44958743  2.4480112  ...  2.4352427   4.832724\n",
            "    -0.23910323]\n",
            "   [ 1.1828443  -0.49417683  3.202301   ...  3.0268295   6.2826304\n",
            "    -0.28850636]]]\n",
            "\n",
            "\n",
            " [[[ 0.60372823 -0.45244592  2.3521276  ...  2.4876134   4.6231356\n",
            "    -0.24181959]\n",
            "   [ 1.1748995  -0.49065924  3.2252517  ...  3.0798593   6.267411\n",
            "    -0.2823052 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.6673628  -0.4439599   2.3752697  ...  2.5285456   4.655521\n",
            "    -0.2419048 ]\n",
            "   [ 1.1316091  -0.49346545  3.2007482  ...  3.0362809   6.2552176\n",
            "    -0.2840936 ]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-1.5058684  -2.78183    -2.5329413  ...  0.6528025   5.4419575\n",
            "   4.264816  ]\n",
            " [-0.99415386 -2.8258224  -2.57922    ...  0.86369807  5.0049067\n",
            "   4.4088283 ]\n",
            " [-0.7966293  -2.8058846  -2.4704525  ...  1.1321942   5.019926\n",
            "   4.3601375 ]\n",
            " ...\n",
            " [-3.104771   -2.3900154  -3.500889   ... -0.6203797   6.3137555\n",
            "   3.7338276 ]\n",
            " [-3.497078   -2.3906755  -3.5306766  ... -1.0517098   6.5346084\n",
            "   3.5858715 ]\n",
            " [-3.2733557  -2.408713   -3.4610758  ... -0.8843837   6.402916\n",
            "   3.7056448 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.65477115 -0.43845123  2.3700163  ...  2.5855172   4.5884914\n",
            "    -0.2494864 ]\n",
            "   [ 1.0719512  -0.49324486  3.1580105  ...  3.043288    6.2275476\n",
            "    -0.28324968]]]\n",
            "\n",
            "\n",
            " [[[ 0.7001794  -0.43032435  2.4042766  ...  2.6259649   4.5916734\n",
            "    -0.2554597 ]\n",
            "   [ 0.96898705 -0.4960049   3.109169   ...  3.0213687   6.2273946\n",
            "    -0.2832528 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.81438327 -0.42359316  2.3894153  ...  2.6449873   4.595714\n",
            "    -0.25789437]\n",
            "   [ 0.89180213 -0.49381465  3.0860014  ...  3.0486293   6.1805706\n",
            "    -0.28876162]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.81168157 -0.42297703  1.684784   ...  1.3373233   4.6281824\n",
            "    -0.24079037]\n",
            "   [ 0.4706826  -0.57229656  2.8901963  ...  2.6567416   7.5580626\n",
            "    -0.27894297]]]\n",
            "\n",
            "\n",
            " [[[ 0.7463493  -0.4462062   1.6913633  ...  1.2223818   4.568145\n",
            "    -0.24468844]\n",
            "   [ 0.4750526  -0.5442956   3.1370766  ...  2.7637565   7.410342\n",
            "    -0.30041268]]]\n",
            "\n",
            "\n",
            " [[[ 0.3492512  -0.5101941   1.6605681  ...  1.1937978   4.4425697\n",
            "    -0.25582018]\n",
            "   [ 0.5964034  -0.54103786  3.1612456  ...  2.759673    7.2320895\n",
            "    -0.30600274]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-3.2151527  -2.4420474  -3.41704    ... -0.964217    6.329269\n",
            "   3.7684097 ]\n",
            " [-2.994108   -2.494637   -3.3326058  ... -0.8558838   6.155402\n",
            "   3.8835988 ]\n",
            " [-2.711282   -2.5413036  -3.3259075  ... -0.8166347   5.894122\n",
            "   4.0311837 ]\n",
            " ...\n",
            " [-3.485241   -3.0546954  -3.8755577  ... -0.7621865   5.227711\n",
            "   1.3685836 ]\n",
            " [-3.749527   -2.8951046  -4.001261   ... -0.88542724  5.5115323\n",
            "   1.3296442 ]\n",
            " [-4.500607   -2.674001   -3.9211895  ... -1.0962433   6.2341523\n",
            "   0.9606049 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 0.01722857 -0.46974614  1.6984457  ...  0.89488286  4.478372\n",
            "    -0.24879625]\n",
            "   [ 0.7293118  -0.5435326   3.2542224  ...  2.9749963   7.24759\n",
            "    -0.30347738]]]\n",
            "\n",
            "\n",
            " [[[-0.28799582 -0.44902223  1.6552993  ...  0.79907984  4.465758\n",
            "    -0.17538816]\n",
            "   [ 0.7795641  -0.54572946  3.2193081  ...  2.898942    7.2326493\n",
            "    -0.31739944]]]\n",
            "\n",
            "\n",
            " [[[-0.6055826  -0.42275724  1.6769439  ...  0.6096033   4.4916263\n",
            "    -0.17625886]\n",
            "   [ 0.9802112  -0.55064976  3.4391625  ...  3.185358    7.154032\n",
            "    -0.31848705]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.4973661  -0.46172425  2.1604574  ...  3.098231    5.220422\n",
            "    -0.30575925]\n",
            "   [ 0.43539914 -0.5271499   2.0044136  ...  2.7043355   5.70368\n",
            "    -0.35884982]]]\n",
            "\n",
            "\n",
            " [[[ 1.7101948  -0.45928466  2.113448   ...  3.1227362   5.267789\n",
            "    -0.30734098]\n",
            "   [ 0.5295265  -0.5262228   1.9713484  ...  2.637107    5.6440125\n",
            "    -0.36485684]]]\n",
            "\n",
            "\n",
            " [[[ 1.7478254  -0.45964456  1.9928932  ...  3.1382113   5.2990766\n",
            "    -0.31352818]\n",
            "   [ 0.54096234 -0.5226089   1.9806247  ...  2.6020045   5.5462966\n",
            "    -0.3522655 ]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-5.4049454  -2.5270371  -4.0250273  ... -1.2650559   6.9187613\n",
            "   0.41513517]\n",
            " [-5.9004526  -2.4132915  -3.983646   ... -1.2971706   7.3776107\n",
            "  -0.00897028]\n",
            " [-6.2841663  -2.2903433  -4.20365    ... -1.4711722   7.6115985\n",
            "  -0.05779538]\n",
            " ...\n",
            " [ 0.14120096 -2.8813608  -2.4201217  ...  1.0459175   4.065699\n",
            "   4.923754  ]\n",
            " [ 0.55806136 -2.8707454  -2.4281974  ...  1.2523963   3.7193558\n",
            "   5.1026273 ]\n",
            " [ 0.64981204 -2.8268445  -2.4078722  ...  1.3501731   3.7126138\n",
            "   5.078154  ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.6504526  -0.4733653   1.8260343  ...  3.1417863   5.425719\n",
            "    -0.3165268 ]\n",
            "   [ 0.30639613 -0.52505654  2.026092   ...  2.5533674   5.512755\n",
            "    -0.34296957]]]\n",
            "\n",
            "\n",
            " [[[ 1.6830503  -0.47746107  1.8391311  ...  3.1577516   5.6147413\n",
            "    -0.32604554]\n",
            "   [ 0.12470638 -0.5032675   2.1192522  ...  2.4625022   5.4679747\n",
            "    -0.32464567]]]\n",
            "\n",
            "\n",
            " [[[ 1.8257539  -0.49131295  2.2726886  ...  2.8915799   5.7681127\n",
            "    -0.31222573]\n",
            "   [ 0.07764716 -0.49477553  2.0875206  ...  2.344205    5.412942\n",
            "    -0.25859717]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.7193551  -0.4572055   2.64913    ...  3.5682497   5.4123645\n",
            "    -0.2937677 ]\n",
            "   [ 0.21111585 -0.43471044  2.3120396  ...  2.733434    5.1553483\n",
            "    -0.23613657]]]\n",
            "\n",
            "\n",
            " [[[ 1.7671195  -0.4579295   2.5662649  ...  3.5691726   5.418332\n",
            "    -0.29192668]\n",
            "   [ 0.06300949 -0.4356755   2.35621    ...  2.654756    5.145066\n",
            "    -0.22876404]]]\n",
            "\n",
            "\n",
            " [[[ 1.6352441  -0.45407674  2.414353   ...  3.6038818   5.3887815\n",
            "    -0.29108736]\n",
            "   [-0.02024271 -0.43105683  2.3415987  ...  2.6042097   5.1971765\n",
            "    -0.24017426]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.54853815 -2.8367138  -2.4203484  ...  1.3546848   4.0057216\n",
            "   4.8461933 ]\n",
            " [ 0.7553757  -2.8063283  -2.3662353  ...  1.6833465   4.1196127\n",
            "   4.9074597 ]\n",
            " [ 1.2690432  -2.681194   -2.347494   ...  2.2401738   3.7435384\n",
            "   5.105749  ]\n",
            " ...\n",
            " [ 1.049118   -2.6933653  -2.1899247  ...  1.4637332   4.053695\n",
            "   6.314212  ]\n",
            " [ 1.211091   -2.7113993  -2.1902013  ...  1.5289891   3.9730523\n",
            "   6.289349  ]\n",
            " [ 0.9746864  -2.743051   -2.1736054  ...  1.4074602   4.209552\n",
            "   6.0409694 ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 1.46621    -0.45869648  2.2356489  ...  3.5984836   5.258553\n",
            "    -0.29612046]\n",
            "   [-0.06616883 -0.43083465  2.321969   ...  2.5760539   5.1937532\n",
            "    -0.24231711]]]\n",
            "\n",
            "\n",
            " [[[ 1.3279781  -0.46295094  2.1572514  ...  3.6578684   5.154042\n",
            "    -0.30110094]\n",
            "   [-0.04743397 -0.43239373  2.2776034  ...  2.544651    5.249898\n",
            "    -0.2541662 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.2014736  -0.45516914  2.1556315  ...  3.6393578   5.1382093\n",
            "    -0.30000943]\n",
            "   [-0.0550586  -0.4350697   2.2663934  ...  2.515509    5.318096\n",
            "    -0.24766126]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.8767359  -0.48743418  2.9830852  ...  3.9490483   6.1731687\n",
            "    -0.38077652]\n",
            "   [ 0.85914814 -0.4228866   2.2432027  ...  2.8099315   4.79845\n",
            "    -0.30459124]]]\n",
            "\n",
            "\n",
            " [[[ 1.8956369  -0.48956338  2.9929688  ...  3.9725718   6.162235\n",
            "    -0.3798504 ]\n",
            "   [ 0.95074874 -0.42591128  2.135255   ...  2.7816648   4.906713\n",
            "    -0.32087123]]]\n",
            "\n",
            "\n",
            " [[[ 1.887866   -0.489122    3.0287619  ...  3.9715772   6.1513214\n",
            "    -0.3743875 ]\n",
            "   [ 0.98134047 -0.43101794  2.1338289  ...  2.7753978   4.956779\n",
            "    -0.31938457]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.63300556 -2.7370906  -2.1883163  ...  1.1351503   4.436701\n",
            "   5.7222543 ]\n",
            " [ 0.35119438 -2.7596102  -2.1824837  ...  0.9014313   4.631001\n",
            "   5.5533166 ]\n",
            " [ 0.10552409 -2.7568526  -2.1501296  ...  0.8686615   4.838154\n",
            "   5.3761334 ]\n",
            " ...\n",
            " [ 1.4617885  -2.40957    -2.0691     ...  2.3679087   4.682013\n",
            "   7.191822  ]\n",
            " [ 1.438051   -2.4769044  -2.1053424  ...  2.2487147   4.578989\n",
            "   7.1554604 ]\n",
            " [ 1.386568   -2.4884312  -2.113826   ...  2.2188025   4.5842853\n",
            "   7.151163  ]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "Processing layer_10:  92%|█████████▏| 22/24 [00:00<00:00, 102.48it/s]tf.Tensor(\n",
            "[[[[ 1.8416789  -0.4818439   3.0222754  ...  3.9551082   6.079909\n",
            "    -0.36817822]\n",
            "   [ 1.033948   -0.43178916  2.1393664  ...  2.8296583   5.040238\n",
            "    -0.3233934 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.9152449  -0.47998548  3.0366666  ...  3.994534    6.094713\n",
            "    -0.37432992]\n",
            "   [ 1.0612224  -0.43897638  2.090271   ...  2.8571432   5.0971265\n",
            "    -0.32179534]]]\n",
            "\n",
            "\n",
            " [[[ 1.9434298  -0.47734174  3.045952   ...  3.9594047   6.068109\n",
            "    -0.36835998]\n",
            "   [ 1.0592498  -0.43896797  2.0865505  ...  2.8549113   5.1447654\n",
            "    -0.32393014]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 2.0723424  -0.44539124  2.7058988  ...  3.1252253   5.7665114\n",
            "    -0.3281763 ]\n",
            "   [ 0.51649815 -0.4477844   2.2984293  ...  3.306496    4.799825\n",
            "    -0.28743875]]]\n",
            "\n",
            "\n",
            " [[[ 2.0998006  -0.44321904  2.7592676  ...  3.2793741   5.921586\n",
            "    -0.33456686]\n",
            "   [ 0.5596672  -0.41817483  2.382136   ...  3.06125     4.7967343\n",
            "    -0.32141837]]]\n",
            "\n",
            "\n",
            " [[[ 2.0013022  -0.43451542  2.7385266  ...  3.2868946   5.86861\n",
            "    -0.33530787]\n",
            "   [ 0.5297107  -0.4203511   2.5021176  ...  2.9789033   4.8082695\n",
            "    -0.30734682]]]], shape=(256, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 1.1659877 -2.5242097 -2.1708255 ...  2.0041544  4.6311293  7.0640087]\n",
            " [ 1.2804635 -2.5938332 -2.1884913 ...  1.9742416  4.477727   7.161613 ]\n",
            " [ 1.2904241 -2.6202931 -2.225898  ...  1.9200832  4.380161   7.137078 ]\n",
            " ...\n",
            " [ 1.432205  -2.5587595 -2.29837   ...  1.9873035  3.5310197  6.498103 ]\n",
            " [ 1.6168613 -2.4652808 -2.2107875 ...  2.3844202  3.7680633  6.664659 ]\n",
            " [ 1.3675982 -2.4064355 -2.2643497 ...  2.2359345  4.0298557  6.5428476]], shape=(256, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[ 2.030677   -0.44007614  2.8163323  -0.22023006  0.5898158\n",
            "     3.303007    6.0360985  -0.33926514]\n",
            "   [ 0.6552676  -0.4420004   2.4554677  -0.17808542  1.1485611\n",
            "     2.8238347   4.801036   -0.31887555]]]\n",
            "\n",
            "\n",
            " [[[ 1.9687142  -0.44397363  2.847862   -0.22174773  0.61033475\n",
            "     3.3244026   6.038877   -0.3406438 ]\n",
            "   [ 0.5137205  -0.425146    2.7005649  -0.18564112  1.100534\n",
            "     2.787792    4.6691623  -0.2834905 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.9389356  -0.4542449   2.903357   -0.21610454  0.6124853\n",
            "     3.3987296   6.1456656  -0.345155  ]\n",
            "   [ 0.6834383  -0.44820446  2.6788003  -0.25935867  0.94780004\n",
            "     2.7505386   4.910075   -0.23287459]]]\n",
            "\n",
            "\n",
            " [[[ 1.9484751  -0.45926878  2.9361055  -0.21255311  0.6215358\n",
            "     3.444371    6.1924667  -0.35295287]\n",
            "   [ 0.5485799  -0.44094685  2.7063863  -0.23645279  1.1084778\n",
            "     2.828996    4.716704   -0.21737027]]]\n",
            "\n",
            "\n",
            " [[[ 1.9821537  -0.47088188  2.9718418  -0.201765    0.63264006\n",
            "     3.5318637   6.319147   -0.3537626 ]\n",
            "   [ 0.59007204 -0.41139755  2.6230783  -0.22450642  1.2163409\n",
            "     2.7599394   4.6924396  -0.2506137 ]]]\n",
            "\n",
            "\n",
            " [[[ 2.057512   -0.4850409   2.965347   -0.1935767   0.6201978\n",
            "     3.573765    6.413164   -0.3579697 ]\n",
            "   [ 0.66346437 -0.4257095   2.550738   -0.22478592  1.1638423\n",
            "     2.6908467   4.7534127  -0.2752925 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.950156   -0.49394387  2.9662971  -0.19142638  0.7062771\n",
            "     3.6502156   6.3823757  -0.36387303]\n",
            "   [ 0.88657814 -0.42963234  2.184684   -0.21320103  1.0529169\n",
            "     2.5363915   4.919843   -0.31779954]]]\n",
            "\n",
            "\n",
            " [[[ 1.7843714  -0.49529392  2.9253545  -0.1943362   0.73787314\n",
            "     3.6455598   6.315577   -0.3649796 ]\n",
            "   [ 0.92934346 -0.42710635  2.155172   -0.20951301  1.0049149\n",
            "     2.570623    4.9734154  -0.31444263]]]\n",
            "\n",
            "\n",
            " [[[ 1.6917403  -0.49138424  2.923693   -0.202309    0.7252385\n",
            "     3.6472845   6.305879   -0.36558744]\n",
            "   [ 0.8521495  -0.42389748  2.1732821  -0.21506013  0.9661709\n",
            "     2.5546107   4.983915   -0.3073671 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.6242216  -0.49462312  2.872154   -0.1995295   0.75136626\n",
            "     3.6741416   6.2870674  -0.37171525]\n",
            "   [ 0.8060405  -0.4186303   2.1815498  -0.22124848  0.95022434\n",
            "     2.572885    4.9934435  -0.30129924]]]\n",
            "\n",
            "\n",
            " [[[ 1.6665251  -0.48622614  2.862204   -0.20085308  0.7441044\n",
            "     3.6456952   6.23847    -0.3702819 ]\n",
            "   [ 0.72979826 -0.41761437  2.2337143  -0.21829171  0.96969974\n",
            "     2.5832458   4.90311    -0.28783026]]]\n",
            "\n",
            "\n",
            " [[[ 1.6668154  -0.48795235  2.8661678  -0.19981751  0.76171935\n",
            "     3.6981316   6.1876397  -0.36494422]\n",
            "   [ 0.71454656 -0.41995245  2.2847483  -0.21455604  0.99605435\n",
            "     2.5511029   4.858655   -0.29034778]]]\n",
            "\n",
            "\n",
            " [[[ 1.6953428  -0.4854104   2.8680065  -0.1991942   0.7533553\n",
            "     3.7418694   6.1428576  -0.36857504]\n",
            "   [ 0.6946044  -0.4207369   2.315831   -0.19231826  0.98616713\n",
            "     2.694148    4.940915   -0.28541136]]]\n",
            "\n",
            "\n",
            " [[[ 1.7660253  -0.48396388  2.8921418  -0.19711375  0.7247007\n",
            "     3.7458737   6.097101   -0.3665613 ]\n",
            "   [ 0.7144412  -0.42657545  2.406443   -0.1849256   1.0601711\n",
            "     2.8563116   4.8903985  -0.2770695 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.8515472  -0.4801009   2.8777826  -0.19260451  0.6998744\n",
            "     3.8207552   6.096119   -0.36516362]\n",
            "   [ 0.7699717  -0.43596232  2.3843842  -0.18581958  1.1276298\n",
            "     2.9793377   4.828252   -0.26045603]]]\n",
            "\n",
            "\n",
            " [[[ 1.8855369  -0.4725026   2.9261956  -0.19286554  0.7080655\n",
            "     3.8731046   6.0492496  -0.3674153 ]\n",
            "   [ 0.71376395 -0.4392374   2.3463879  -0.1818234   1.127492\n",
            "     3.0228267   4.837862   -0.2548459 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.8965405  -0.47159848  2.9380536  -0.19684505  0.70688325\n",
            "     3.8383837   6.0330462  -0.36178878]\n",
            "   [ 0.74933946 -0.43560234  2.3036008  -0.18319789  1.1438568\n",
            "     2.9963915   4.840121   -0.26133683]]]\n",
            "\n",
            "\n",
            " [[[ 1.9117573  -0.47347075  2.9767928  -0.19363837  0.72066253\n",
            "     3.8442156   6.0059633  -0.35950464]\n",
            "   [ 0.7352615  -0.43866083  2.328126   -0.17725742  1.1506783\n",
            "     3.0062077   4.824507   -0.26145965]]]\n",
            "\n",
            "\n",
            " [[[ 1.9717796  -0.47032592  3.008598   -0.19827989  0.717973\n",
            "     3.8727074   5.998828   -0.35775906]\n",
            "   [ 0.7046753  -0.4417593   2.3306613  -0.18229482  1.1699932\n",
            "     3.0122507   4.8048153  -0.25541165]]]\n",
            "\n",
            "\n",
            " [[[ 2.1173859  -0.46397308  3.0816064  -0.20276305  0.68596613\n",
            "     3.8387425   6.0247927  -0.35299283]\n",
            "   [ 0.71075135 -0.44885433  2.3026514  -0.18635605  1.1420292\n",
            "     3.055552    4.8334317  -0.25397968]]]], shape=(20, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 1.4953027  -2.3246255  -2.2408319  -0.83291954  3.2560098   2.539866\n",
            "   4.123515    6.5737762 ]\n",
            " [ 1.4172384  -2.1998937  -2.2472484  -0.90953106  2.9079242   2.5779934\n",
            "   4.427719    6.6534925 ]\n",
            " [ 1.1611471  -2.2709575  -2.3845856  -0.7050501   3.4259803   2.4058418\n",
            "   4.6655116   6.5988693 ]\n",
            " [ 1.3993846  -2.225282   -2.2417102  -0.94519615  3.1572201   2.6527624\n",
            "   4.6622376   6.820458  ]\n",
            " [ 1.6148936  -2.2148683  -2.1544046  -1.1055597   3.503456    2.9249964\n",
            "   4.681508    6.9506884 ]\n",
            " [ 1.749695   -2.2495253  -2.1851254  -1.2347107   3.9004653   2.995147\n",
            "   4.6041474   6.9828086 ]\n",
            " [ 1.5962218  -2.333723   -2.1575212  -0.9458006   4.956089    2.8078697\n",
            "   4.5764556   6.7558107 ]\n",
            " [ 1.2017087  -2.3381116  -2.1890888  -0.5874984   4.993565    2.549424\n",
            "   4.843778    6.537165  ]\n",
            " [ 1.0715001  -2.3336985  -2.1487772  -0.4632561   4.9298224   2.537103\n",
            "   4.998885    6.4505634 ]\n",
            " [ 0.94353324 -2.3506608  -2.1507409  -0.31838816  4.8925667   2.437028\n",
            "   5.1303625   6.3794694 ]\n",
            " [ 1.0695179  -2.321956   -2.1253316  -0.44891104  4.6424932   2.4799974\n",
            "   5.0056844   6.4415526 ]\n",
            " [ 1.1012294  -2.2930462  -2.1020222  -0.47217077  4.5499616   2.4703243\n",
            "   5.0238557   6.5251746 ]\n",
            " [ 1.0418094  -2.3951383  -2.1478684  -0.37183276  4.3147383   2.2707326\n",
            "   4.9627295   6.6242986 ]\n",
            " [ 1.0722133  -2.4042323  -2.1852863  -0.46935153  3.8942146   2.1806378\n",
            "   4.865329    6.8093987 ]\n",
            " [ 1.2058429  -2.4404476  -2.186061   -0.6528371   3.8002424   2.1272206\n",
            "   4.7265043   7.017097  ]\n",
            " [ 1.3332177  -2.504656   -2.1311953  -0.6954099   3.7476895   2.1001263\n",
            "   4.5612645   7.1630535 ]\n",
            " [ 1.3640928  -2.4952526  -2.1204233  -0.7024278   3.8115337   2.134481\n",
            "   4.480774    7.129375  ]\n",
            " [ 1.4038738  -2.4899793  -2.1124203  -0.7322792   3.7379768   2.1309512\n",
            "   4.436715    7.201006  ]\n",
            " [ 1.5641558  -2.5073762  -2.0818114  -0.85356426  3.6906133   2.1876621\n",
            "   4.302824    7.3329673 ]\n",
            " [ 1.8199339  -2.5622702  -2.1004162  -1.0835232   3.6983886   2.2579951\n",
            "   3.9769657   7.4903245 ]], shape=(20, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "Processing layer_10: 100%|██████████| 24/24 [00:00<00:00, 101.17it/s]\n",
            "Fingerprint after layer_10. ((5908, 8) inputs, (5908, 8) neurons)\n",
            "Inputs: (neuron signature (On/Off activations) dataset)(labels dataset)\n",
            "(5908, 8) (5908,)\n",
            "\n",
            "RULES FROM LAYER LAYER_10 IN TERMS OF FEATURES\n",
            "\n",
            "Obtained all paths\n",
            "Processing paths for validation set: 100%|██████████| 237/237 [00:00<00:00, 645068.17it/s]\n",
            "InV 0\n",
            "\n",
            "Fingerprinting VAL data after layer_10 layer\n",
            "tf.Tensor(\n",
            "[[[[ 2.1276286  -0.45872194  3.0703611  ...  3.8000035   5.975162\n",
            "    -0.35156393]\n",
            "   [ 0.6832804  -0.45225006  2.3075774  ...  3.088109    4.878751\n",
            "    -0.25065225]]]\n",
            "\n",
            "\n",
            " [[[ 2.1631389  -0.46149826  3.0903087  ...  3.7597666   5.9435205\n",
            "    -0.34818932]\n",
            "   [ 0.5795801  -0.44570726  2.2464976  ...  3.073068    4.849266\n",
            "    -0.24122323]]]\n",
            "\n",
            "\n",
            " [[[ 2.1687045  -0.45851028  3.0923743  ...  3.7212481   5.9222665\n",
            "    -0.3482495 ]\n",
            "   [ 0.5854442  -0.45273954  2.2588687  ...  3.110774    4.9263167\n",
            "    -0.24556194]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.91119856 -0.501205    1.9101183  ...  1.708681    5.9174843\n",
            "    -0.33751863]\n",
            "   [ 0.27170566 -0.47742683  2.0707712  ...  2.2555883   6.384035\n",
            "    -0.25167108]]]\n",
            "\n",
            "\n",
            " [[[ 0.86899155 -0.5049854   2.0100675  ...  1.8889083   5.9883738\n",
            "    -0.34192958]\n",
            "   [ 0.23442528 -0.47809556  2.004123   ...  2.2129211   6.315107\n",
            "    -0.24214305]]]\n",
            "\n",
            "\n",
            " [[[ 0.8075959  -0.51460475  2.1112816  ...  1.8732309   5.945819\n",
            "    -0.30717918]\n",
            "   [ 0.2807172  -0.4753329   2.0644155  ...  2.3918772   6.167274\n",
            "    -0.25091094]]]], shape=(1478, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 1.7887452 -2.600243  -2.1392412 ...  2.1567411  3.8960261  7.447544 ]\n",
            " [ 1.9812506 -2.626962  -2.085217  ...  2.2301388  3.6764011  7.468869 ]\n",
            " [ 1.912619  -2.6655228 -2.1161985 ...  2.1755545  3.6417542  7.420916 ]\n",
            " ...\n",
            " [-1.5540088 -2.643988  -3.0561888 ...  1.4508435  5.37698    2.1470428]\n",
            " [-1.4061325 -2.6414475 -2.9023414 ...  1.6058913  5.488998   2.3903215]\n",
            " [-1.5358373 -2.5626435 -2.884534  ...  1.5369734  5.613146   2.5327067]], shape=(1478, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n",
            "Fingerprint after layer_10. ((1478, 8) inputs, (1478, 8) neurons)\n",
            "PRINTING ALL RULES.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "_output_path = \"/content/ProphecyPlus/results/kjt/rules_cte_1/ruleset.csv\"\n",
        "\n",
        "\n",
        "print(\"****** RULES ********\")\n",
        "df_op = pd.read_csv(_output_path)\n",
        "df_op = df_op[df_op['test_precision'] > 90.0]\n",
        "df_op = df_op[df_op['label'] == 1]\n",
        "\n",
        "\n",
        "df_op.to_csv(\"./rules_he_10_1.csv\",index=False)\n",
        "\n",
        "\n",
        "df_op = pd.read_csv(_output_path)\n",
        "df_op = df_op[df_op['test_precision'] > 90.0]\n",
        "df_op = df_op[df_op['label'] == 0]\n",
        "\n",
        "\n",
        "df_op.to_csv(\"./rules_he_10_0.csv\",index=False)\n",
        "\n",
        "#df_op = df_op.reset_index()  # Reset index to make 'index' column\n",
        "#df_op\n",
        "#top_df_cte_13 = df_op[df_op['index'] == 0]  # Use 'index' column instead of 'row'\n",
        "\n",
        "#top_df_cte_13"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGoJagNfdnR3",
        "outputId": "46a232cb-d32a-4b4f-e296-909b9f8f77ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** RULES ********\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74-0nHK2wBK5",
        "outputId": "e3caa1a4-ae90-437a-e5a0-e1bfdf4bf3b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** RULES ********\n",
            "CTE 10 1: 21.88933047997554 , 1.1869436201780417\n",
            "HE 10 1: 57.44125326370757 , 25.46652030735456\n",
            "[5, 6, 7, 5, 5, 7, 2, 2]\n",
            "['<=', 5.014780044555664, '<=', 8.457788467407227, '<=', 6.801446199417114, '<=', 4.407367944717407, '<=', 0.42133453488349915, '<=', 4.016632080078125, '<=', 0.710400253534317, '<=', -0.7180337011814117]\n",
            "[2, 7, 4, 7, 3, 7, 6, 1, 1, 7, 7, 4, 2, 7, 7]\n",
            "['<=', 5.0200934410095215, '<=', 5.228757619857788, '<=', 0.7042066156864166, '>', 2.3984490633010864, '>', -4.647291898727417, '>', 2.8580180406570435, '>', 0.23533541709184647, '<=', -0.8676009178161621, '>', -1.7780373096466064, '<=', 5.001114130020142, '>', 3.032026529312134, '<=', 0.6818224787712097, '<=', 4.339860677719116, '>', 3.0972129106521606, '<=', 4.9778783321380615]\n"
          ]
        }
      ],
      "source": [
        "print(\"****** RULES ********\")\n",
        "\n",
        "\n",
        "df_op = pd.read_csv(\"./rules_cte_10_1.csv\")\n",
        "df_op = df_op[df_op.index == 0]\n",
        "train_f1 = (df_op['train_f1']).values[0]\n",
        "test_f1 = (df_op['test_f1']).values[0]\n",
        "\n",
        "print(\"CTE 10 1:\", train_f1,\",\",test_f1)\n",
        "\n",
        "\n",
        "\n",
        "df_op = pd.read_csv(\"./rules_he_10_1.csv\")\n",
        "df_op = df_op[df_op.index == 0]\n",
        "train_f1 = (df_op['train_f1']).values[0]\n",
        "test_f1 = (df_op['test_f1']).values[0]\n",
        "\n",
        "print(\"HE 10 1:\", train_f1,\",\",test_f1)\n",
        "\n",
        "\n",
        "\n",
        "#print(\"****** CTE 1 RULES ********\")\n",
        "df_op = pd.read_csv(\"./rules_cte_10_1.csv\")\n",
        "df_op = df_op[df_op.index == 0]\n",
        "layer_10_cte_rule_neurons = df_op['neurons']\n",
        "layer_10_cte_rule_signature = df_op['signature']\n",
        "\n",
        "\n",
        "#print(\"****** HE 1 RULES ********\")\n",
        "df_op = pd.read_csv(\"./rules_he_10_1.csv\")\n",
        "df_op = df_op[df_op.index == 0]\n",
        "layer_10_he_rule_neurons = df_op['neurons']\n",
        "layer_10_he_rule_signature = df_op['signature']\n",
        "\n",
        "print(layer_10_cte_rule_neurons.array[0])\n",
        "print(layer_10_cte_rule_signature.array[0])\n",
        "print(layer_10_he_rule_neurons.array[0])\n",
        "print(layer_10_he_rule_signature.array[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"****** RULES ********\")\n",
        "\n",
        "\n",
        "df_op = pd.read_csv(\"/content/ProphecyPlus/rules_cte_10_0.csv\")\n",
        "df_op = df_op[df_op.index == 0]\n",
        "train_f1 = (df_op['train_f1']).values[0]\n",
        "test_f1 = (df_op['test_f1']).values[0]\n",
        "\n",
        "print(\"CTE 10 0:\", train_f1,\",\",test_f1)\n",
        "\n",
        "\n",
        "\n",
        "df_op = pd.read_csv(\"/content/ProphecyPlus/rules_he_10_0.csv\")\n",
        "df_op = df_op[df_op.index == 0]\n",
        "train_f1 = (df_op['train_f1']).values[0]\n",
        "test_f1 = (df_op['test_f1']).values[0]\n",
        "\n",
        "print(\"HE 10 0:\", train_f1,\",\",test_f1)\n",
        "\n",
        "\n",
        "\n",
        "#print(\"****** CTE 1 RULES ********\")\n",
        "df_op = pd.read_csv(\"/content/ProphecyPlus/rules_cte_10_0.csv\")\n",
        "df_op = df_op[df_op.index == 0]\n",
        "layer_10_cte_rule_neurons0 = df_op['neurons']\n",
        "layer_10_cte_rule_signature0 = df_op['signature']\n",
        "\n",
        "\n",
        "#print(\"****** HE 1 RULES ********\")\n",
        "df_op = pd.read_csv(\"/content/ProphecyPlus/rules_he_10_0.csv\")\n",
        "df_op = df_op[df_op.index == 0]\n",
        "layer_10_he_rule_neurons0 = df_op['neurons']\n",
        "layer_10_he_rule_signature0 = df_op['signature']\n",
        "\n",
        "print(layer_10_cte_rule_neurons0.array[0])\n",
        "print(layer_10_cte_rule_signature0.array[0])\n",
        "print(layer_10_he_rule_neurons0.array[0])\n",
        "print(layer_10_he_rule_signature0.array[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV5yrClddWvf",
        "outputId": "92762970-0014-4379-8588-3afc6a9d5473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** RULES ********\n",
            "CTE 10 0: 25.007303534910896 , 68.69615163699024\n",
            "HE 10 0: 15.027061445399555 , 15.384615384615383\n",
            "[5, 6, 5, 2, 7, 5, 7, 6]\n",
            "['<=', 5.014780044555664, '>', 8.457788467407227, '<=', 0.45192472636699677, '>', -0.7284570932388306, '>', 3.306133508682251, '<=', 0.23589569330215454, '>', 3.5442665815353394, '>', 8.710090160369873]\n",
            "[2, 7, 4, 7, 1, 1, 5]\n",
            "['<=', 5.0200934410095215, '<=', 5.228757619857788, '>', 0.7042066156864166, '<=', 3.8628121614456177, '<=', -1.4256365299224854, '<=', -1.727737545967102, '>', -3.598828911781311]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prophecy helper methods**"
      ],
      "metadata": {
        "id": "QTgDsZQIpC2Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjjG8U57K9ok",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def check_pattern(layer_vals: list, neuron_ids: list, neuron_sig: list) -> bool:\n",
        "    \"\"\"\n",
        "        Check if the provided layer values satisfy the provided neuron signature.\n",
        "    :param layer_vals:\n",
        "    :param neuron_ids:\n",
        "    :param neuron_sig:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    found = True\n",
        "    oper = -1\n",
        "    # layer_vals = (layer_vals).flatten()\n",
        "\n",
        "    for ind in range(0, len(neuron_sig)):\n",
        "        if ind % 2 == 0:\n",
        "            op = neuron_sig[ind]\n",
        "            if op == '<=':\n",
        "                oper = 0\n",
        "            else:\n",
        "                oper = 1\n",
        "        else:\n",
        "            v = int(neuron_ids[(int)(ind / 2)])\n",
        "            vsig = float(neuron_sig[ind])\n",
        "            val = float(layer_vals[v])\n",
        "            # print(v,vsig,val,oper)\n",
        "            if oper == 0:\n",
        "                if val > vsig:\n",
        "                    # print(v,val,vsig,oper)\n",
        "                    found = False\n",
        "                    break\n",
        "            else:\n",
        "                if val <= vsig:\n",
        "                    # print(v,val,vsig,oper)\n",
        "                    found = False\n",
        "                    break\n",
        "            oper = -1\n",
        "\n",
        "    return found\n",
        "\n",
        "def get_suffix_cluster(neuron_ids, neuron_sig, suffixes, VAL=False):\n",
        "    # Get the cluster of inputs that such that all inputs in the cluster\n",
        "    # have provided on/off signature for the provided neurons.\n",
        "    #\n",
        "    # The returned cluster is an array of indices (into mnist.train.images).\n",
        "    if (VAL == False):\n",
        "        return np.where((suffixes[:, neuron_ids] == neuron_sig).all(axis=1))[0]\n",
        "\n",
        "    matched_ids = []\n",
        "    # print(len(suffixes))\n",
        "    for indx in range(0, len(suffixes)):\n",
        "        if (check_pattern(suffixes[indx], neuron_ids, neuron_sig) == True):\n",
        "            matched_ids.append(indx)\n",
        "    # print(matched_ids)\n",
        "    return matched_ids\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get fingerprints (neuron values) after layer_10 on train data**"
      ],
      "metadata": {
        "id": "5dCXw8cUpGjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend\n",
        "\n",
        "func = None\n",
        "\n",
        "\n",
        "for layer in model.layers:\n",
        "    print(layer.name)\n",
        "    if (layer.name == 'layer_10'):\n",
        "      func = backend.function(model.input, [layer.output])\n",
        "\n",
        "\n",
        "fingerprint_10 = []\n",
        "\n",
        "if (func != None):\n",
        "  fingerprint_10 = func(x_train)"
      ],
      "metadata": {
        "id": "Xhpgv_IBY6Cs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd14edab-3c0f-47b7-b355-5b3b219d6594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_0\n",
            "layer_0_\n",
            "layer_1_\n",
            "layer_2_\n",
            "layer_3__const2\n",
            "layer_3_\n",
            "layer_4_\n",
            "layer_6_\n",
            "layer_7_\n",
            "layer_8__const2\n",
            "layer_8_\n",
            "layer_9_\n",
            "layer_10\n",
            "layer_11_const2\n",
            "layer_11\n",
            "layer_12\n",
            "layer_13\n",
            "layer_14_const2\n",
            "layer_14\n",
            "tf.Tensor(\n",
            "[[[[-0.26016584 -0.43339378  2.2210205  ...  2.5751863   4.813346\n",
            "    -0.2545323 ]\n",
            "   [ 0.98201674 -0.43722454  3.0293176  ...  4.171374    4.887096\n",
            "    -0.29122013]]]\n",
            "\n",
            "\n",
            " [[[-0.18652447 -0.4324616   2.2048576  ...  2.6458187   4.772342\n",
            "    -0.25256512]\n",
            "   [ 1.0005517  -0.4410913   2.9933293  ...  4.2622576   4.9028606\n",
            "    -0.29046923]]]\n",
            "\n",
            "\n",
            " [[[-0.11948881 -0.43224463  2.210664   ...  2.6556752   4.703244\n",
            "    -0.24672768]\n",
            "   [ 1.0262357  -0.44452363  3.0207863  ...  4.361699    4.8975844\n",
            "    -0.2941592 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 1.9117573  -0.47347075  2.9767928  ...  3.8442156   6.0059633\n",
            "    -0.35950464]\n",
            "   [ 0.7352615  -0.43866083  2.328126   ...  3.0062077   4.824507\n",
            "    -0.26145965]]]\n",
            "\n",
            "\n",
            " [[[ 1.97178    -0.47032604  3.008598   ...  3.872708    5.998828\n",
            "    -0.35775897]\n",
            "   [ 0.70467544 -0.44175935  2.330661   ...  3.012251    4.8048143\n",
            "    -0.25541168]]]\n",
            "\n",
            "\n",
            " [[[ 2.1173859  -0.46397308  3.0816064  ...  3.8387425   6.0247927\n",
            "    -0.35299283]\n",
            "   [ 0.71075135 -0.44885433  2.3026514  ...  3.055552    4.8334317\n",
            "    -0.25397968]]]], shape=(5908, 1, 2, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.24709961  0.         -0.07541189  0.         -0.32499272 -0.5012685\n",
            " -0.4468012  -0.00124541], shape=(8,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-4.457205  -1.948548  -3.0069826 ... -1.2161914  8.097293   3.6387482]\n",
            " [-4.3902693 -2.0272279 -3.0420167 ... -1.389712   7.9507484  3.773254 ]\n",
            " [-4.354619  -2.0635738 -3.1056032 ... -1.5577652  7.80088    3.8919835]\n",
            " ...\n",
            " [ 1.4038738 -2.4899793 -2.1124203 ...  2.1309512  4.436715   7.201006 ]\n",
            " [ 1.564157  -2.5073762 -2.081811  ...  2.1876616  4.3028235  7.332969 ]\n",
            " [ 1.8199339 -2.5622702 -2.1004162 ...  2.2579951  3.9769657  7.4903245]], shape=(5908, 8), dtype=float32) tf.Tensor(\n",
            "[ 0.18295312  0.         -0.00550821  0.13051267  0.02680667  0.13877484\n",
            " -0.28092673  0.17636964], shape=(8,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train))\n",
        "x_train_flat = []\n",
        "for indx in range(0, len(x_train)):\n",
        "  x_train_flat.append((x_train[indx]).flatten())\n",
        "\n",
        "x_train_flat = np.array(x_train_flat)\n",
        "print(np.shape(x_train))\n",
        "print(np.shape(x_train_flat))\n",
        "\n",
        "print(np.shape(fingerprint_10[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf6kwuV5Romd",
        "outputId": "05cdb602-7ba7-4d36-8f9f-0d556082ce46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5908\n",
            "(5908, 8, 16, 1)\n",
            "(5908, 128)\n",
            "(5908, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get the values to set the lower and upper bounds for the input vars and inner layer variables, based on the rules.**\n",
        "\n",
        "**For each rule, the longest sequence of consecutive images satisfying the rule are considered.**"
      ],
      "metadata": {
        "id": "rHWttWoppZav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length = len(x_train_flat[0])\n",
        "\n",
        "\n",
        "x_train_min = np.zeros(length)\n",
        "x_train_max = np.zeros(length)\n",
        "\n",
        "for indx in range(0,length):\n",
        "  x_train_min[indx] = np.min(x_train_flat[:,indx])\n",
        "  x_train_max[indx] = np.max(x_train_flat[:,indx])\n",
        "\n",
        "print(x_train_min)\n",
        "print(x_train_max)\n",
        "\n",
        "\n",
        "print(\"CTE RULE:\")\n",
        "###########\n",
        "rule_neurons_list_cte = []\n",
        "rule_neurons = (layer_10_cte_rule_neurons.array[0]).split(\",\")\n",
        "for indx in range(0, len(rule_neurons)):\n",
        "    rule_neurons[indx] = (rule_neurons[indx]).strip()\n",
        "    rule_neurons[indx] = (rule_neurons[indx]).replace(\"[\", \"\")\n",
        "    rule_neurons[indx] = (rule_neurons[indx]).replace(\"]\",\"\")\n",
        "    rule_neurons_list_cte.append(int(rule_neurons[indx]))\n",
        "\n",
        "print(rule_neurons_list_cte)\n",
        "\n",
        "rule_sig_list_cte = []\n",
        "rule_sig = (layer_10_cte_rule_signature.array[0]).split(\",\")\n",
        "for indx in range(0, len(rule_sig)):\n",
        "    rule_sig[indx] = (rule_sig[indx]).strip()\n",
        "    rule_sig[indx] = (rule_sig[indx]).replace(\"[\", \"\")\n",
        "    rule_sig[indx] = (rule_sig[indx]).replace(\"]\",\"\")\n",
        "    if (indx % 2 == 0):\n",
        "      rule_sig[indx] = (rule_sig[indx]).replace(\"'\", \"\")\n",
        "      rule_sig_list_cte.append(rule_sig[indx])\n",
        "    else:\n",
        "      rule_sig_list_cte.append(float(rule_sig[indx]))\n",
        "\n",
        "print(rule_sig_list_cte)\n",
        "\n",
        "fngprnt = fingerprint_10[0]\n",
        "indices = get_suffix_cluster(rule_neurons_list_cte, rule_sig_list_cte, fngprnt,VAL=True)\n",
        "print(\"indices:\", len(indices))\n",
        "\n",
        "print(\"SORTED:\")\n",
        "sorted_indices = sorted(indices)\n",
        "print(sorted_indices)\n",
        "consecutive_min= [sorted_indices[0]]\n",
        "consecutive_m = 0\n",
        "consecutive_max = []\n",
        "consecutive_l = 0\n",
        "consecutive_len = []\n",
        "for indx in range(1, len(sorted_indices)):\n",
        "  if (sorted_indices[indx] - sorted_indices[indx-1] == 1):\n",
        "    consecutive_m = sorted_indices[indx]\n",
        "    consecutive_l = consecutive_l + 1\n",
        "  else:\n",
        "    consecutive_max.append(sorted_indices[indx-1])\n",
        "    consecutive_min.append(sorted_indices[indx])\n",
        "    consecutive_len.append(consecutive_l)\n",
        "    consecutive_l = 0\n",
        "    consecutive_m = 0\n",
        "\n",
        "print(\"MIN:\", consecutive_min)\n",
        "print(\"MAX:\", consecutive_max)\n",
        "print(\"LEN:\",np.max(consecutive_len), consecutive_len.index(np.max(consecutive_len)))\n",
        "\n",
        "\n",
        "\n",
        "x_train_cte = []\n",
        "fngprnt_cte = []\n",
        "inp_ex = []\n",
        "finger_ex = []\n",
        "#for indx in range(0, len(indices)):\n",
        "#    if (indx == 0):\n",
        "for indx in range(consecutive_min[0], consecutive_max[0]):\n",
        "    if (indx == consecutive_min[0]):\n",
        "      inp_ex = x_train_flat[indx]\n",
        "      finger_ex = fingerprint_10[0][indx]\n",
        "\n",
        "    x_train_cte.append(x_train_flat[indx])\n",
        "    fngprnt_cte.append(fingerprint_10[0][indx])\n",
        "\n",
        "x_train_cte = np.array(x_train_cte)\n",
        "fngprnt_cte = np.array(fngprnt_cte)\n",
        "\n",
        "print(np.shape(x_train_cte))\n",
        "x_train_minCTE = np.zeros(length)\n",
        "x_train_maxCTE = np.zeros(length)\n",
        "\n",
        "for indx in range(0,length):\n",
        "  x_train_minCTE[indx] = np.min(x_train_cte[:,indx])\n",
        "  x_train_maxCTE[indx] = np.max(x_train_cte[:,indx])\n",
        "\n",
        "print(x_train_minCTE)\n",
        "print(x_train_maxCTE)\n",
        "\n",
        "print(np.shape(fngprnt_cte))\n",
        "fngprnt_min_cte = np.zeros(len(fngprnt_cte[0]))\n",
        "fngprnt_max_cte = np.zeros(len(fngprnt_cte[0]))\n",
        "\n",
        "for indx in range(0,len(fngprnt_cte[0])):\n",
        "  fngprnt_min_cte[indx] = np.min(fngprnt_cte[:,indx])\n",
        "  fngprnt_max_cte[indx] = np.max(fngprnt_cte[:,indx])\n",
        "\n",
        "print(fngprnt_min_cte)\n",
        "print(fngprnt_max_cte)\n",
        "\n",
        "\n",
        "print(\"INP:\", inp_ex)\n",
        "print(\"FINGER:\", finger_ex)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXYdHQ_a_nPb",
        "outputId": "44199d7e-5992-49b8-ed1b-f38085f282a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.36404144 0.36037263 0.32961474 0.32884881 0.35045381 0.35795037\n",
            " 0.37650888 0.39604205 0.39824793 0.39785348 0.39858877 0.3939185\n",
            " 0.38999694 0.3894244  0.39050054 0.38926164 0.22116843 0.18043045\n",
            " 0.17305645 0.20998392 0.2158835  0.27976984 0.33289101 0.30152229\n",
            " 0.2659735  0.27905178 0.28924632 0.36837278 0.39922832 0.3946672\n",
            " 0.35002872 0.38987247 0.14694968 0.19119753 0.22777076 0.25891353\n",
            " 0.21280254 0.26782514 0.29537377 0.29710095 0.27209903 0.23497817\n",
            " 0.21542777 0.23297335 0.22390472 0.30461282 0.32118758 0.35895374\n",
            " 0.18875804 0.23876379 0.23973269 0.20781633 0.25474686 0.2602271\n",
            " 0.27331495 0.2810566  0.29480124 0.26616498 0.23485562 0.21879404\n",
            " 0.21448185 0.22192287 0.21875766 0.25191674 0.2076957  0.22034888\n",
            " 0.20951095 0.18575751 0.25643574 0.25178271 0.25221163 0.27322495\n",
            " 0.28897059 0.28653301 0.26131855 0.23656173 0.22149778 0.22565679\n",
            " 0.19116307 0.21677581 0.20153761 0.20362094 0.17176585 0.19518229\n",
            " 0.24211857 0.24609949 0.2096718  0.2654316  0.27264476 0.25999923\n",
            " 0.2634421  0.24693436 0.23889974 0.22945389 0.23325291 0.19929343\n",
            " 0.19101754 0.16759919 0.20101486 0.18871017 0.21186619 0.23530561\n",
            " 0.19292662 0.25548407 0.26820619 0.2352405  0.25705614 0.25899778\n",
            " 0.242446   0.23841529 0.21619753 0.22136374 0.17934666 0.15252566\n",
            " 0.20985562 0.18765127 0.18849763 0.22920688 0.19357958 0.23993566\n",
            " 0.26425973 0.22496362 0.22671377 0.24039905 0.23843827 0.24517272\n",
            " 0.23009536 0.22158778]\n",
            "[0.81100069 0.84454465 0.85546301 0.84688457 0.83834635 0.82383961\n",
            " 0.81373889 0.83270144 0.8425877  0.84773476 0.84124732 0.83757085\n",
            " 0.82413067 0.81746706 0.7582644  0.71512714 0.85382966 0.84990809\n",
            " 0.84770221 0.83712086 0.83168275 0.81517119 0.77712546 0.70996668\n",
            " 0.77923177 0.79527612 0.81994485 0.83353248 0.84194432 0.84194432\n",
            " 0.84034926 0.83979779 0.81853554 0.78521561 0.80035999 0.79545803\n",
            " 0.78565411 0.78001685 0.71453929 0.72550169 0.70970244 0.70874311\n",
            " 0.72654335 0.74416169 0.83844975 0.83924824 0.84162071 0.84267961\n",
            " 0.74459635 0.74708372 0.75097848 0.75268459 0.74753753 0.73038067\n",
            " 0.71364698 0.688804   0.69887791 0.7056564  0.71672794 0.7052811\n",
            " 0.71784429 0.81598882 0.82383195 0.83091491 0.74637523 0.73264974\n",
            " 0.74317747 0.72240541 0.72134268 0.68442096 0.70907246 0.68799403\n",
            " 0.68037684 0.7096335  0.71674134 0.70816291 0.71427505 0.7098786\n",
            " 0.70939032 0.79419233 0.76434589 0.76214001 0.73007621 0.72044462\n",
            " 0.69207835 0.69372511 0.71518459 0.71470971 0.68533624 0.70399625\n",
            " 0.71640625 0.72008272 0.71717793 0.71718941 0.71493566 0.70763059\n",
            " 0.77572381 0.76010072 0.72186543 0.72184245 0.69747051 0.69774433\n",
            " 0.71690985 0.70415518 0.67041016 0.68732192 0.72522978 0.7376034\n",
            " 0.73037684 0.71796684 0.72412301 0.72436811 0.7761106  0.74459635\n",
            " 0.74821347 0.72125268 0.69902535 0.71271255 0.7064951  0.72891008\n",
            " 0.69427658 0.69985447 0.70892693 0.73319164 0.72779948 0.7253485\n",
            " 0.72289752 0.72081801]\n",
            "CTE RULE:\n",
            "[5, 6, 7, 5, 5, 7, 2, 2]\n",
            "['<=', 5.014780044555664, '<=', 8.457788467407227, '<=', 6.801446199417114, '<=', 4.407367944717407, '<=', 0.42133453488349915, '<=', 4.016632080078125, '<=', 0.710400253534317, '<=', -0.7180337011814117]\n",
            "indices: 358\n",
            "SORTED:\n",
            "[2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2518, 2519, 2520, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2615, 2616, 2617, 2672, 2675, 2683, 2684, 2685, 2686, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2814, 2815, 2816, 2817, 2818, 2819, 2830, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2950, 2951, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3160, 3161, 3162, 3175, 3176, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3293, 3294, 3295, 3305, 3306, 3307, 3359, 3360, 3361, 3370, 3371, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3639, 3640, 3641, 3642, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3764, 3765, 3766, 3767, 3769, 3770, 3771, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3834, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 4034, 4035, 4036, 4037, 4038, 4039, 4041, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4157, 4158, 4159, 4160, 4161, 4162, 4178, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4297, 4298, 4299, 4300, 4301, 4302, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4500, 4501, 4502, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4634, 4635, 4636, 4637, 4645, 4646, 4647, 4699, 4700, 4701, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4789, 4790, 4791, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4922, 4925, 4926]\n",
            "MIN: [2471, 2518, 2522, 2539, 2599, 2615, 2672, 2675, 2683, 2742, 2814, 2830, 2886, 2950, 3026, 3090, 3160, 3175, 3227, 3293, 3305, 3359, 3370, 3432, 3499, 3578, 3639, 3678, 3749, 3764, 3769, 3826, 3834, 3888, 3956, 4034, 4041, 4093, 4157, 4178, 4230, 4297, 4378, 4437, 4500, 4566, 4634, 4645, 4699, 4772, 4789, 4831, 4846, 4900, 4922, 4925]\n",
            "MAX: [2489, 2520, 2533, 2554, 2612, 2617, 2672, 2675, 2686, 2748, 2819, 2830, 2894, 2951, 3034, 3097, 3162, 3176, 3233, 3295, 3307, 3361, 3371, 3441, 3508, 3585, 3642, 3686, 3756, 3767, 3771, 3832, 3834, 3894, 3963, 4039, 4041, 4102, 4162, 4178, 4238, 4302, 4385, 4446, 4502, 4573, 4637, 4647, 4701, 4784, 4791, 4839, 4862, 4917, 4922]\n",
            "LEN: 18 0\n",
            "(18, 128)\n",
            "[0.68076363 0.67341069 0.6717582  0.64988128 0.54301854 0.52541169\n",
            " 0.41746324 0.57054994 0.67909199 0.66640625 0.66899893 0.63419501\n",
            " 0.62978324 0.61835938 0.61713388 0.61272212 0.56385187 0.50821461\n",
            " 0.45404795 0.40625383 0.39752604 0.36983762 0.42179075 0.42471661\n",
            " 0.52252604 0.54826134 0.56860447 0.58453585 0.67721163 0.72933517\n",
            " 0.73419501 0.63738128 0.49890089 0.42443321 0.3599303  0.30536918\n",
            " 0.33775084 0.3696404  0.31811428 0.34410233 0.4966299  0.52620251\n",
            " 0.5470435  0.56076134 0.57252604 0.57203585 0.58086703 0.59213388\n",
            " 0.41714154 0.32365579 0.2741862  0.32414407 0.36081495 0.31958487\n",
            " 0.32059398 0.34853516 0.40732996 0.49997702 0.52696653 0.54604013\n",
            " 0.54630055 0.56370251 0.57105545 0.57424173 0.29497932 0.24379596\n",
            " 0.28017961 0.34644991 0.29771752 0.28474839 0.32132927 0.33051854\n",
            " 0.33949334 0.38917739 0.500697   0.51955231 0.54139859 0.54899663\n",
            " 0.54847388 0.55562194 0.24154795 0.27296454 0.28586282 0.33836167\n",
            " 0.28546262 0.28431947 0.3058881  0.30836589 0.33292165 0.32322878\n",
            " 0.40956457 0.50637829 0.47104013 0.539928   0.5401731  0.55343712\n",
            " 0.25698912 0.26242532 0.33125191 0.29007736 0.2801777  0.28331036\n",
            " 0.29624885 0.2906116  0.29816559 0.30542471 0.33255974 0.45006127\n",
            " 0.46198683 0.45116996 0.52721163 0.54798369 0.26732537 0.29566674\n",
            " 0.35542279 0.27414599 0.25678424 0.26636604 0.28149893 0.28802083\n",
            " 0.31050092 0.30090571 0.30125804 0.34142731 0.44491422 0.42053271\n",
            " 0.44410233 0.47953048]\n",
            "[0.74442402 0.71899127 0.78370481 0.79477443 0.77684015 0.71368719\n",
            " 0.79669501 0.7864411  0.77487362 0.72635187 0.74232345 0.74914407\n",
            " 0.70575597 0.64742264 0.64326172 0.64130093 0.63492839 0.60649701\n",
            " 0.5778148  0.53492264 0.4541973  0.48606196 0.51434206 0.56434206\n",
            " 0.57953814 0.60527344 0.7226754  0.77257966 0.76997741 0.76936849\n",
            " 0.76280637 0.76767961 0.56961359 0.53407437 0.52726716 0.51777344\n",
            " 0.53836167 0.49112094 0.4601754  0.48321461 0.55331265 0.56336167\n",
            " 0.58051854 0.59154795 0.60429305 0.61458716 0.70355775 0.78394991\n",
            " 0.52993451 0.42699142 0.39326363 0.4979205  0.57739545 0.4936332\n",
            " 0.46238128 0.46483226 0.513894   0.55600873 0.56581265 0.57184436\n",
            " 0.5849303  0.59644799 0.5954676  0.60172143 0.38644301 0.36732728\n",
            " 0.40233226 0.65803271 0.58427543 0.5023246  0.39553271 0.45757506\n",
            " 0.52463618 0.53238549 0.54758157 0.56385187 0.57198415 0.57777459\n",
            " 0.58704236 0.59375191 0.37653186 0.39410233 0.6654182  0.66420037\n",
            " 0.53677045 0.38693513 0.40807292 0.41605775 0.47498468 0.50551854\n",
            " 0.54346086 0.54880706 0.56413718 0.57271561 0.57786267 0.58227443\n",
            " 0.36981464 0.61076134 0.68768957 0.66442249 0.55159505 0.40727443\n",
            " 0.39162837 0.42708716 0.48646408 0.47022442 0.52835669 0.54831687\n",
            " 0.55441751 0.55811887 0.5719401  0.57712737 0.51835937 0.6896829\n",
            " 0.71275084 0.60594363 0.53851294 0.40738358 0.39973958 0.43505476\n",
            " 0.45904565 0.50366115 0.49120711 0.53766659 0.56511757 0.55172143\n",
            " 0.55801164 0.57909007]\n",
            "(18, 8)\n",
            "[ -6.14425087  -1.69371748  -3.93599987  -2.91622853 -10.86739445\n",
            "  -1.19800925  -4.54303932  -0.32229012]\n",
            "[-5.33386803 -1.49559236 -1.53325868 -2.53319836 -3.43426418  0.30174276\n",
            "  3.75126338  1.63665307]\n",
            "INP: [0.70414369 0.69164369 0.67326134 0.65732996 0.72546722 0.66639859\n",
            " 0.43551624 0.66296722 0.73674173 0.67154565 0.68576134 0.70978094\n",
            " 0.65120251 0.61835938 0.61713388 0.61272212 0.6031633  0.58012408\n",
            " 0.55585938 0.514928   0.39752604 0.3901731  0.42179075 0.4526731\n",
            " 0.52252604 0.54826134 0.56860447 0.58453585 0.72546722 0.75732996\n",
            " 0.74948683 0.75414369 0.54605545 0.51002604 0.42448683 0.30536918\n",
            " 0.45953585 0.47179075 0.31811428 0.44213388 0.49850643 0.52620251\n",
            " 0.54801624 0.56076134 0.57252604 0.57203585 0.58576134 0.59213388\n",
            " 0.42081036 0.35120251 0.33796722 0.4654182  0.56247702 0.31958487\n",
            " 0.3281633  0.38845741 0.40732996 0.49997702 0.53036918 0.54997702\n",
            " 0.54630055 0.56370251 0.57105545 0.57424173 0.3406633  0.35512408\n",
            " 0.37473192 0.64188879 0.49875153 0.2896829  0.39433977 0.45757506\n",
            " 0.40414369 0.50978094 0.51076134 0.53576134 0.54139859 0.54899663\n",
            " 0.56370251 0.56590839 0.33870251 0.32889859 0.6654182  0.56296722\n",
            " 0.32179075 0.30340839 0.37448683 0.38306526 0.46958487 0.33502604\n",
            " 0.51174173 0.51027114 0.52228094 0.539928   0.5401731  0.55806526\n",
            " 0.30071232 0.61076134 0.6776731  0.38821232 0.28331036 0.28331036\n",
            " 0.35782016 0.37326134 0.3906633  0.35659467 0.43159467 0.51982996\n",
            " 0.46198683 0.53159467 0.54115349 0.552428   0.51835937 0.6896829\n",
            " 0.43943781 0.32081036 0.29997702 0.28796722 0.3396829  0.40634957\n",
            " 0.45904565 0.38821232 0.40904565 0.51125153 0.53429075 0.48380055\n",
            " 0.51909467 0.53600643]\n",
            "FINGER: [-5.4587107  -1.4955924  -3.1134267  -2.8413756  -4.5581875   0.30174276\n",
            "  2.0788455   0.24012923]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"HE RULE:\")\n",
        "###########\n",
        "rule_neurons_list_he = []\n",
        "rule_neurons = (layer_10_he_rule_neurons.array[0]).split(\",\")\n",
        "for indx in range(0, len(rule_neurons)):\n",
        "    rule_neurons[indx] = (rule_neurons[indx]).strip()\n",
        "    rule_neurons[indx] = (rule_neurons[indx]).replace(\"[\", \"\")\n",
        "    rule_neurons[indx] = (rule_neurons[indx]).replace(\"]\",\"\")\n",
        "    rule_neurons_list_he.append(int(rule_neurons[indx]))\n",
        "\n",
        "print(rule_neurons_list_he)\n",
        "\n",
        "rule_sig_list_he = []\n",
        "rule_sig = (layer_10_he_rule_signature.array[0]).split(\",\")\n",
        "for indx in range(0, len(rule_sig)):\n",
        "    rule_sig[indx] = (rule_sig[indx]).strip()\n",
        "    rule_sig[indx] = (rule_sig[indx]).replace(\"[\", \"\")\n",
        "    rule_sig[indx] = (rule_sig[indx]).replace(\"]\",\"\")\n",
        "    if (indx % 2 == 0):\n",
        "      rule_sig[indx] = (rule_sig[indx]).replace(\"'\", \"\")\n",
        "      rule_sig_list_he.append(rule_sig[indx])\n",
        "    else:\n",
        "      rule_sig_list_he.append(float(rule_sig[indx]))\n",
        "\n",
        "print(rule_sig_list_he)\n",
        "\n",
        "fngprnt = fingerprint_10[0]\n",
        "indices = get_suffix_cluster(rule_neurons_list_he, rule_sig_list_he, fngprnt,VAL=True)\n",
        "print(\"indices:\", len(indices))\n",
        "\n",
        "print(\"SORTED:\")\n",
        "sorted_indices = sorted(indices)\n",
        "print(sorted_indices)\n",
        "consecutive_min= [sorted_indices[0]]\n",
        "consecutive_m = 0\n",
        "consecutive_max = []\n",
        "consecutive_l = 0\n",
        "consecutive_len = []\n",
        "for indx in range(1, len(sorted_indices)):\n",
        "  if (sorted_indices[indx] - sorted_indices[indx-1] == 1):\n",
        "    consecutive_m = sorted_indices[indx]\n",
        "    consecutive_l = consecutive_l + 1\n",
        "  else:\n",
        "    consecutive_max.append(sorted_indices[indx-1])\n",
        "    consecutive_min.append(sorted_indices[indx])\n",
        "    consecutive_len.append(consecutive_l)\n",
        "    consecutive_l = 0\n",
        "    consecutive_m = 0\n",
        "\n",
        "print(\"MIN:\", consecutive_min)\n",
        "print(\"MAX:\", consecutive_max)\n",
        "\n",
        "max_indx = consecutive_len.index(np.max(consecutive_len))\n",
        "print(\"LEN:\",np.max(consecutive_len), max_indx)\n",
        "\n",
        "\n",
        "\n",
        "x_train_he = []\n",
        "fngprnt_he = []\n",
        "inp_he_ex = []\n",
        "finger_he_ex = []\n",
        "\n",
        "for indx in range(consecutive_min[max_indx], consecutive_max[max_indx]):\n",
        "    if (indx == consecutive_min[max_indx]):\n",
        "      inp_he_ex = x_train_flat[indx]\n",
        "      finger_he_ex = fingerprint_10[0][indx]\n",
        "\n",
        "    x_train_he.append(x_train_flat[indx])\n",
        "    fngprnt_he.append(fingerprint_10[0][indx])\n",
        "\n",
        "x_train_he = np.array(x_train_he)\n",
        "fngprnt_he = np.array(fngprnt_he)\n",
        "\n",
        "print(np.shape(x_train_he))\n",
        "x_train_minhe = np.zeros(length)\n",
        "x_train_maxhe = np.zeros(length)\n",
        "\n",
        "for indx in range(0,length):\n",
        "  x_train_minhe[indx] = np.min(x_train_he[:,indx])\n",
        "  x_train_maxhe[indx] = np.max(x_train_he[:,indx])\n",
        "\n",
        "print(x_train_minhe)\n",
        "print(x_train_maxhe)\n",
        "\n",
        "print(np.shape(fngprnt_he))\n",
        "fngprnt_min_he = np.zeros(len(fngprnt_he[0]))\n",
        "fngprnt_max_he = np.zeros(len(fngprnt_he[0]))\n",
        "\n",
        "for indx in range(0,len(fngprnt_he[0])):\n",
        "  fngprnt_min_he[indx] = np.min(fngprnt_he[:,indx])\n",
        "  fngprnt_max_he[indx] = np.max(fngprnt_he[:,indx])\n",
        "\n",
        "print(fngprnt_min_he)\n",
        "print(fngprnt_max_he)\n",
        "\n",
        "\n",
        "print(\"INP:\", inp_he_ex)\n",
        "print(\"FINGER:\", finger_he_ex)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yQlr8mZZszm",
        "outputId": "7d93d410-a94c-43ea-c569-97db62b11ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HE RULE:\n",
            "[2, 7, 4, 7, 3, 7, 6, 1, 1, 7, 7, 4, 2, 7, 7]\n",
            "['<=', 5.0200934410095215, '<=', 5.228757619857788, '<=', 0.7042066156864166, '>', 2.3984490633010864, '>', -4.647291898727417, '>', 2.8580180406570435, '>', 0.23533541709184647, '<=', -0.8676009178161621, '>', -1.7780373096466064, '<=', 5.001114130020142, '>', 3.032026529312134, '<=', 0.6818224787712097, '<=', 4.339860677719116, '>', 3.0972129106521606, '<=', 4.9778783321380615]\n",
            "indices: 1210\n",
            "SORTED:\n",
            "[161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 214, 215, 225, 234, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 327, 357, 365, 370, 371, 381, 382, 383, 386, 387, 388, 389, 390, 391, 392, 395, 396, 397, 442, 443, 444, 445, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 495, 500, 501, 509, 510, 511, 513, 514, 515, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 564, 565, 569, 570, 571, 583, 584, 585, 586, 587, 588, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 704, 705, 709, 710, 713, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 741, 742, 778, 781, 782, 788, 792, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 845, 846, 866, 867, 868, 869, 870, 871, 872, 873, 874, 907, 919, 920, 921, 922, 923, 927, 928, 929, 930, 931, 932, 933, 934, 937, 938, 939, 940, 942, 943, 944, 975, 980, 991, 992, 993, 994, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1081, 1120, 1121, 1122, 1123, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1144, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1303, 1304, 1305, 1306, 1307, 1310, 1313, 1314, 1315, 1316, 1318, 1321, 1322, 1323, 1324, 1325, 1379, 1380, 1381, 1387, 1388, 1389, 1390, 1391, 1392, 1394, 1395, 1396, 1436, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1504, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1535, 1564, 1565, 1572, 1576, 1577, 1580, 1588, 1589, 1590, 1591, 1592, 1595, 1596, 1597, 1598, 1599, 1600, 1635, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1710, 1711, 1712, 1713, 1714, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1731, 1732, 1733, 1734, 1735, 1765, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1904, 1910, 1911, 1917, 1918, 1919, 1920, 1921, 1922, 1924, 1931, 1932, 1933, 1938, 1939, 1940, 1941, 1942, 1972, 1977, 1978, 1979, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2042, 2049, 2050, 2051, 2052, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2068, 2069, 2070, 2071, 2072, 2102, 2118, 2119, 2120, 2121, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2193, 2194, 2195, 2196, 2197, 2198, 2200, 2201, 2202, 2203, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2303, 2305, 2308, 2309, 2451, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2753, 2754, 2755, 2756, 2759, 2761, 2763, 2764, 2789, 2790, 2792, 2793, 2794, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2859, 2862, 2863, 2864, 2865, 2866, 2867, 2891, 2892, 2893, 2894, 2895, 2896, 2898, 2899, 2900, 2901, 2902, 2904, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2970, 2971, 2972, 2973, 2974, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3101, 3102, 3103, 3106, 3107, 3108, 3109, 3112, 3113, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3207, 3208, 3209, 3210, 3211, 3212, 3239, 3246, 3247, 3277, 3278, 3279, 3302, 3305, 3306, 3307, 3308, 3310, 3311, 3312, 3313, 3314, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3369, 3374, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3442, 3443, 3446, 3447, 3448, 3449, 3450, 3451, 3479, 3480, 3481, 3483, 3484, 3485, 3510, 3511, 3512, 3513, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3582, 3583, 3584, 3585, 3587, 3592, 3593, 3595, 3615, 3617, 3618, 3619, 3621, 3622, 3623, 3624, 3625, 3651, 3652, 3653, 3655, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3730, 3731, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3796, 3797, 3800, 3801, 3802, 3803, 3804, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3935, 3936, 3937, 3938, 3939, 3940, 3965, 3966, 3967, 3970, 3971, 3972, 3973, 3975, 3976, 3977, 3978, 3979, 4004, 4005, 4006, 4007, 4008, 4009, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4099, 4100, 4101, 4102, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4133, 4134, 4135, 4136, 4138, 4139, 4140, 4141, 4142, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4200, 4201, 4202, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4239, 4240, 4244, 4245, 4246, 4247, 4248, 4249, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4304, 4305, 4306, 4307, 4308, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4376, 4377, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4445, 4446, 4447, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4476, 4477, 4479, 4480, 4481, 4483, 4484, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4578, 4579, 4585, 4618, 4619, 4643, 4644, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4712, 4713, 4714, 4716, 4717, 4718, 4720, 4721, 4722, 4723, 4724, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4923, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5121, 5166, 5186, 5187, 5188, 5189, 5191, 5192, 5257, 5258, 5259, 5334, 5369, 5374, 5396, 5397, 5398, 5399, 5400, 5401, 5403, 5404, 5508, 5509, 5510, 5511, 5512, 5514, 5515, 5516, 5538, 5540, 5541, 5542, 5601, 5602, 5671, 5674, 5675, 5738, 5739, 5740, 5782, 5783, 5785, 5805, 5806, 5807, 5808, 5811]\n",
            "MIN: [161, 167, 214, 225, 234, 242, 301, 327, 357, 365, 370, 381, 386, 395, 442, 447, 495, 500, 509, 513, 517, 564, 569, 583, 591, 650, 704, 709, 713, 726, 741, 778, 781, 788, 792, 794, 845, 866, 907, 919, 927, 937, 942, 975, 980, 991, 997, 1057, 1081, 1120, 1132, 1144, 1239, 1303, 1310, 1313, 1318, 1321, 1379, 1387, 1394, 1436, 1447, 1504, 1511, 1535, 1564, 1572, 1576, 1580, 1588, 1595, 1635, 1646, 1710, 1718, 1731, 1765, 1784, 1849, 1904, 1910, 1917, 1924, 1931, 1938, 1972, 1977, 1990, 2042, 2049, 2060, 2068, 2102, 2118, 2125, 2183, 2193, 2200, 2257, 2303, 2305, 2308, 2451, 2458, 2613, 2682, 2721, 2753, 2759, 2761, 2763, 2789, 2792, 2822, 2859, 2862, 2891, 2898, 2904, 2925, 2970, 2997, 3032, 3068, 3101, 3106, 3112, 3136, 3170, 3207, 3239, 3246, 3277, 3302, 3305, 3310, 3338, 3369, 3374, 3378, 3408, 3442, 3446, 3479, 3483, 3510, 3515, 3548, 3582, 3587, 3592, 3595, 3615, 3617, 3621, 3651, 3655, 3657, 3690, 3730, 3758, 3796, 3800, 3831, 3866, 3896, 3935, 3965, 3970, 3975, 4004, 4033, 4069, 4099, 4105, 4133, 4138, 4169, 4200, 4205, 4239, 4244, 4273, 4304, 4310, 4345, 4376, 4379, 4414, 4445, 4450, 4476, 4479, 4483, 4510, 4545, 4578, 4585, 4618, 4643, 4646, 4680, 4712, 4716, 4720, 4747, 4782, 4923, 5113, 5121, 5166, 5186, 5191, 5257, 5334, 5369, 5374, 5396, 5403, 5508, 5514, 5538, 5540, 5601, 5671, 5674, 5738, 5782, 5785, 5805, 5811]\n",
            "MAX: [165, 176, 215, 225, 234, 253, 318, 327, 357, 365, 371, 383, 392, 397, 445, 461, 495, 501, 511, 515, 527, 565, 571, 588, 603, 667, 705, 710, 713, 738, 742, 778, 782, 788, 792, 804, 846, 874, 907, 923, 934, 940, 944, 975, 980, 994, 1007, 1075, 1081, 1123, 1141, 1144, 1252, 1307, 1310, 1316, 1318, 1325, 1381, 1392, 1396, 1436, 1463, 1504, 1527, 1535, 1565, 1572, 1577, 1580, 1592, 1600, 1635, 1664, 1714, 1727, 1735, 1765, 1802, 1867, 1904, 1911, 1922, 1924, 1933, 1942, 1972, 1979, 2006, 2042, 2052, 2066, 2072, 2102, 2121, 2137, 2191, 2198, 2203, 2268, 2303, 2305, 2309, 2451, 2464, 2622, 2693, 2727, 2756, 2759, 2761, 2764, 2790, 2794, 2835, 2859, 2867, 2896, 2902, 2904, 2936, 2974, 3009, 3042, 3078, 3103, 3109, 3113, 3142, 3182, 3212, 3239, 3247, 3279, 3302, 3308, 3314, 3347, 3369, 3374, 3385, 3415, 3443, 3451, 3481, 3485, 3513, 3525, 3556, 3585, 3587, 3593, 3595, 3615, 3619, 3625, 3653, 3655, 3664, 3701, 3731, 3775, 3797, 3804, 3841, 3877, 3907, 3940, 3967, 3973, 3979, 4009, 4045, 4076, 4102, 4112, 4136, 4142, 4182, 4202, 4213, 4240, 4249, 4281, 4308, 4319, 4352, 4377, 4390, 4421, 4447, 4456, 4477, 4481, 4484, 4520, 4551, 4579, 4585, 4619, 4644, 4655, 4687, 4714, 4718, 4724, 4755, 4792, 4923, 5119, 5121, 5166, 5189, 5192, 5259, 5334, 5369, 5374, 5401, 5404, 5512, 5516, 5538, 5542, 5602, 5671, 5675, 5740, 5783, 5785, 5808]\n",
            "LEN: 18 47\n",
            "(18, 128)\n",
            "[0.54449678 0.54082031 0.53545688 0.53572112 0.47170075 0.43371055\n",
            " 0.43441138 0.44488549 0.53470435 0.60674977 0.53885761 0.53469095\n",
            " 0.53318589 0.53268038 0.52968941 0.52684972 0.44483379 0.43523093\n",
            " 0.36140663 0.36584138 0.39127413 0.40827014 0.43519646 0.4564051\n",
            " 0.46792471 0.48494945 0.63689874 0.69885685 0.56483801 0.53488626\n",
            " 0.53243528 0.5302294  0.54665097 0.45944585 0.3785769  0.39316981\n",
            " 0.4141525  0.43485945 0.45130783 0.46569585 0.46743451 0.47612592\n",
            " 0.48924824 0.49777497 0.66483992 0.6927811  0.59983724 0.53219018\n",
            " 0.36157514 0.38118298 0.40025467 0.41557521 0.42759842 0.44852367\n",
            " 0.46128408 0.46792471 0.47753332 0.47939262 0.48715725 0.49573376\n",
            " 0.49841835 0.50145144 0.52965112 0.68519646 0.38616728 0.40598001\n",
            " 0.41388825 0.42784161 0.44011183 0.45385838 0.46406824 0.47491766\n",
            " 0.47154948 0.48037301 0.48445925 0.48366077 0.49607269 0.50032744\n",
            " 0.49969171 0.50431411 0.4087163  0.41529373 0.42614124 0.43763787\n",
            " 0.44788986 0.46034965 0.46682751 0.47603592 0.47851371 0.48127298\n",
            " 0.48355928 0.49182751 0.48895144 0.49648629 0.50039062 0.49883004\n",
            " 0.42068589 0.43152765 0.44011183 0.44592907 0.45920075 0.45979818\n",
            " 0.47393727 0.47244945 0.47733801 0.48169998 0.48538986 0.48661535\n",
            " 0.49303002 0.48054726 0.49451785 0.49096392 0.42123928 0.44523016\n",
            " 0.44991    0.45602022 0.45940947 0.46259574 0.47540594 0.48053002\n",
            " 0.47954963 0.48331419 0.47881051 0.48404948 0.49037607 0.493053\n",
            " 0.48390587 0.49047373]\n",
            "[0.57665058 0.57172947 0.56583372 0.56778493 0.55334138 0.53498583\n",
            " 0.49964193 0.59670075 0.63003408 0.65874119 0.66165365 0.54794156\n",
            " 0.54184858 0.5420458  0.541749   0.53782744 0.47509766 0.59102711\n",
            " 0.60552619 0.59871132 0.55076785 0.44155369 0.46263212 0.47709291\n",
            " 0.49253408 0.67170075 0.70381434 0.71333104 0.70677849 0.55923139\n",
            " 0.55625957 0.55334712 0.61951785 0.61161726 0.46387293 0.41459291\n",
            " 0.43469669 0.45132123 0.46945849 0.47489277 0.48714767 0.49470358\n",
            " 0.50067019 0.54277918 0.70753294 0.71480162 0.71462354 0.58700406\n",
            " 0.38219018 0.39964767 0.41655944 0.42816713 0.45025467 0.46083984\n",
            " 0.47687845 0.4799977  0.49572036 0.49572036 0.50185355 0.50824908\n",
            " 0.51337316 0.52434704 0.70071615 0.70571959 0.41228745 0.42704312\n",
            " 0.42893114 0.45375881 0.46111175 0.47246477 0.48817019 0.48465456\n",
            " 0.49470358 0.50479473 0.50618298 0.51142961 0.51339805 0.51385761\n",
            " 0.51608647 0.62606273 0.4405867  0.44885685 0.46184704 0.45760761\n",
            " 0.47562423 0.48430798 0.48882315 0.49558249 0.49915173 0.50716337\n",
            " 0.50356924 0.51021178 0.5158184  0.52199372 0.5141525  0.51852022\n",
            " 0.45743528 0.46199066 0.47105928 0.47508234 0.48464116 0.48919654\n",
            " 0.49351448 0.50944776 0.5071404  0.50750804 0.5069604  0.51823491\n",
            " 0.51437653 0.51819278 0.51655944 0.52073568 0.46092984 0.48302313\n",
            " 0.47828202 0.48856273 0.49280216 0.50018191 0.51165365 0.50970435\n",
            " 0.50270565 0.5052428  0.51537607 0.51658241 0.51568053 0.51660156\n",
            " 0.51666092 0.51945849]\n",
            "(18, 8)\n",
            "[-7.69653749 -1.22859621  2.1027112  -4.57374811 -7.43925762  4.436728\n",
            "  1.31280446  3.55540657]\n",
            "[-7.06307793 -0.90087962  3.67797828 -4.37260485 -3.82612038  6.19288492\n",
            "  4.40823746  4.15130138]\n",
            "INP: [0.54861175 0.54395489 0.54150391 0.53831763 0.53268038 0.52434704\n",
            " 0.44665097 0.44616077 0.53929802 0.63194508 0.63488626 0.54101371\n",
            " 0.53635685 0.53268038 0.541749   0.53782744 0.45326861 0.55620979\n",
            " 0.60008234 0.57018038 0.48635685 0.41969018 0.44616077 0.45841567\n",
            " 0.47189606 0.48586665 0.68464116 0.69885685 0.64003332 0.53488626\n",
            " 0.53243528 0.5302294  0.54665097 0.55326861 0.38513136 0.4047392\n",
            " 0.42483724 0.44321959 0.45130783 0.4672392  0.47214116 0.48366077\n",
            " 0.4924843  0.49861175 0.691749   0.7052294  0.59983724 0.53219018\n",
            " 0.38219018 0.39836665 0.40669998 0.42140587 0.44836665 0.45400391\n",
            " 0.4674843  0.47067057 0.47753332 0.4802294  0.48905293 0.49787646\n",
            " 0.50277842 0.52434704 0.69591567 0.70571959 0.41086665 0.42704312\n",
            " 0.42385685 0.45375881 0.46111175 0.47189606 0.47214116 0.47630783\n",
            " 0.48096469 0.48219018 0.49003332 0.48366077 0.49616077 0.50032744\n",
            " 0.50719018 0.62606273 0.42851371 0.44885685 0.46184704 0.45277842\n",
            " 0.46307253 0.47067057 0.48268038 0.48611175 0.47851371 0.48415097\n",
            " 0.48807253 0.50302351 0.49591567 0.49714116 0.5044941  0.51429802\n",
            " 0.45743528 0.45375881 0.45155293 0.47508234 0.48464116 0.47508234\n",
            " 0.48439606 0.47900391 0.48488626 0.48169998 0.50571959 0.49616077\n",
            " 0.49910195 0.49861175 0.50326861 0.49934704 0.45964116 0.46527842\n",
            " 0.47777842 0.48856273 0.47581763 0.48537646 0.49297449 0.48292547\n",
            " 0.49076861 0.49493528 0.49346469 0.4922392  0.49787646 0.49934704\n",
            " 0.48390587 0.50351371]\n",
            "FINGER: [-7.360227  -0.9008796  3.6779783 -4.5182266 -3.8784993  6.192885\n",
            "  4.4082375  3.8341336]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CTE 0 RULE:\")\n",
        "###########\n",
        "rule_neurons_list_cte0 = []\n",
        "rule_neurons = (layer_10_cte_rule_neurons0.array[0]).split(\",\")\n",
        "for indx in range(0, len(rule_neurons)):\n",
        "    rule_neurons[indx] = (rule_neurons[indx]).strip()\n",
        "    rule_neurons[indx] = (rule_neurons[indx]).replace(\"[\", \"\")\n",
        "    rule_neurons[indx] = (rule_neurons[indx]).replace(\"]\",\"\")\n",
        "    rule_neurons_list_cte0.append(int(rule_neurons[indx]))\n",
        "\n",
        "print(rule_neurons_list_cte0)\n",
        "\n",
        "rule_sig_list_cte0 = []\n",
        "rule_sig = (layer_10_cte_rule_signature0.array[0]).split(\",\")\n",
        "for indx in range(0, len(rule_sig)):\n",
        "    rule_sig[indx] = (rule_sig[indx]).strip()\n",
        "    rule_sig[indx] = (rule_sig[indx]).replace(\"[\", \"\")\n",
        "    rule_sig[indx] = (rule_sig[indx]).replace(\"]\",\"\")\n",
        "    if (indx % 2 == 0):\n",
        "      rule_sig[indx] = (rule_sig[indx]).replace(\"'\", \"\")\n",
        "      rule_sig_list_cte0.append(rule_sig[indx])\n",
        "    else:\n",
        "      rule_sig_list_cte0.append(float(rule_sig[indx]))\n",
        "\n",
        "print(rule_sig_list_cte0)\n",
        "\n",
        "fngprnt = fingerprint_10[0]\n",
        "indices = get_suffix_cluster(rule_neurons_list_cte0, rule_sig_list_cte0, fngprnt,VAL=True)\n",
        "print(\"indices:\", len(indices))\n",
        "\n",
        "print(\"SORTED:\")\n",
        "sorted_indices = sorted(indices)\n",
        "print(sorted_indices)\n",
        "consecutive_min= [sorted_indices[0]]\n",
        "consecutive_m = 0\n",
        "consecutive_max = []\n",
        "consecutive_l = 0\n",
        "consecutive_len = []\n",
        "for indx in range(1, len(sorted_indices)):\n",
        "  if (sorted_indices[indx] - sorted_indices[indx-1] == 1):\n",
        "    consecutive_m = sorted_indices[indx]\n",
        "    consecutive_l = consecutive_l + 1\n",
        "  else:\n",
        "    consecutive_max.append(sorted_indices[indx-1])\n",
        "    consecutive_min.append(sorted_indices[indx])\n",
        "    consecutive_len.append(consecutive_l)\n",
        "    consecutive_l = 0\n",
        "    consecutive_m = 0\n",
        "\n",
        "print(\"MIN:\", consecutive_min)\n",
        "print(\"MAX:\", consecutive_max)\n",
        "max_indx = consecutive_len.index(np.max(consecutive_len))\n",
        "print(\"LEN:\",np.max(consecutive_len), max_indx)\n",
        "\n",
        "\n",
        "\n",
        "x_train_cte0 = []\n",
        "fngprnt_cte0 = []\n",
        "inp_ex0 = []\n",
        "finger_ex0 = []\n",
        "for indx in range(consecutive_min[max_indx], consecutive_max[max_indx]):\n",
        "    if (indx == consecutive_min[max_indx]):\n",
        "      inp_ex0 = x_train_flat[indx]\n",
        "      finger_ex0 = fingerprint_10[0][indx]\n",
        "\n",
        "    x_train_cte0.append(x_train_flat[indx])\n",
        "    fngprnt_cte0.append(fingerprint_10[0][indx])\n",
        "\n",
        "x_train_cte0 = np.array(x_train_cte0)\n",
        "fngprnt_cte0 = np.array(fngprnt_cte0)\n",
        "\n",
        "print(np.shape(x_train_cte0))\n",
        "x_train_minCTE0 = np.zeros(length)\n",
        "x_train_maxCTE0 = np.zeros(length)\n",
        "\n",
        "for indx in range(0,length):\n",
        "  x_train_minCTE0[indx] = np.min(x_train_cte0[:,indx])\n",
        "  x_train_maxCTE0[indx] = np.max(x_train_cte0[:,indx])\n",
        "\n",
        "print(x_train_minCTE0)\n",
        "print(x_train_maxCTE0)\n",
        "\n",
        "print(np.shape(fngprnt_cte0))\n",
        "fngprnt_min_cte0 = np.zeros(len(fngprnt_cte0[0]))\n",
        "fngprnt_max_cte0 = np.zeros(len(fngprnt_cte0[0]))\n",
        "\n",
        "for indx in range(0,len(fngprnt_cte0[0])):\n",
        "  fngprnt_min_cte0[indx] = np.min(fngprnt_cte0[:,indx])\n",
        "  fngprnt_max_cte0[indx] = np.max(fngprnt_cte0[:,indx])\n",
        "\n",
        "print(fngprnt_min_cte0)\n",
        "print(fngprnt_max_cte0)\n",
        "\n",
        "\n",
        "print(\"INP:\", inp_ex0)\n",
        "print(\"FINGER:\", finger_ex0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq3mUKFeeaBX",
        "outputId": "4ebf3000-5065-4810-b47b-02dd35c9d17c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CTE 0 RULE:\n",
            "[5, 6, 5, 2, 7, 5, 7, 6]\n",
            "['<=', 5.014780044555664, '>', 8.457788467407227, '<=', 0.45192472636699677, '>', -0.7284570932388306, '>', 3.306133508682251, '<=', 0.23589569330215454, '>', 3.5442665815353394, '>', 8.710090160369873]\n",
            "indices: 428\n",
            "SORTED:\n",
            "[2955, 3165, 3166, 3168, 3169, 3298, 3299, 3300, 3301, 3363, 3364, 3365, 3366, 3367, 3368, 3689, 4167, 4503, 4639, 4640, 4703, 4704, 4705, 4707, 4708, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5055, 5056, 5059, 5060, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5134, 5135, 5136, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5194, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5213, 5214, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229, 5230, 5231, 5266, 5267, 5268, 5269, 5270, 5271, 5276, 5277, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5358, 5359, 5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5416, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439, 5440, 5441, 5442, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489, 5490, 5491, 5492, 5493, 5494, 5495, 5496, 5498, 5499, 5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509, 5510, 5543, 5544, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5609, 5610, 5611, 5612, 5613, 5614, 5618, 5619, 5620, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5744, 5745, 5746, 5747, 5748, 5754, 5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769, 5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821, 5822, 5823, 5827, 5828, 5829, 5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849, 5883, 5890, 5893, 5894, 5895, 5896, 5897, 5898, 5899, 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907]\n",
            "MIN: [2955, 3165, 3168, 3298, 3363, 3689, 4167, 4503, 4639, 4703, 4707, 4933, 4954, 5007, 5055, 5059, 5064, 5124, 5134, 5141, 5194, 5197, 5213, 5266, 5276, 5284, 5345, 5358, 5407, 5416, 5430, 5482, 5498, 5543, 5546, 5561, 5609, 5618, 5627, 5685, 5695, 5744, 5754, 5760, 5813, 5827, 5883, 5890, 5893]\n",
            "MAX: [2955, 3166, 3169, 3301, 3368, 3689, 4167, 4503, 4640, 4705, 4708, 4946, 4961, 5026, 5056, 5060, 5092, 5130, 5136, 5158, 5194, 5207, 5231, 5271, 5277, 5303, 5356, 5374, 5414, 5416, 5442, 5496, 5510, 5544, 5556, 5578, 5614, 5620, 5644, 5691, 5714, 5748, 5754, 5779, 5823, 5849, 5883, 5890]\n",
            "LEN: 28 16\n",
            "(28, 128)\n",
            "[0.51478439 0.52551509 0.52385302 0.53364354 0.4821921  0.44997894\n",
            " 0.44459252 0.43659812 0.43454925 0.496268   0.55208908 0.5668907\n",
            " 0.5635589  0.54674479 0.53835976 0.53115234 0.5230373  0.54689798\n",
            " 0.52606464 0.506947   0.49086244 0.46013327 0.43841146 0.43405331\n",
            " 0.41571117 0.37007698 0.44287684 0.44875345 0.44950789 0.46887063\n",
            " 0.47891965 0.51565755 0.52729013 0.52708525 0.51659773 0.50400582\n",
            " 0.49556717 0.47508425 0.45483303 0.44175092 0.41111366 0.3832644\n",
            " 0.36787109 0.32492341 0.33813189 0.36764897 0.40614085 0.41697687\n",
            " 0.51454504 0.51135876 0.51027305 0.50168122 0.49554228 0.49077053\n",
            " 0.47496553 0.46160386 0.44812347 0.4335095  0.38042854 0.33165403\n",
            " 0.30948223 0.26985486 0.29473614 0.3114028  0.51443972 0.5060432\n",
            " 0.5053462  0.49919577 0.49605737 0.48992992 0.47615656 0.47395067\n",
            " 0.44317364 0.43845167 0.35175207 0.30150697 0.28680109 0.29845282\n",
            " 0.28112937 0.25175398 0.5130553  0.51052964 0.5102922  0.50345627\n",
            " 0.49770987 0.48635302 0.48096661 0.43140893 0.40531556 0.40959712\n",
            " 0.43187615 0.33189913 0.28704619 0.26841873 0.25921032 0.28014897\n",
            " 0.51746706 0.51540671 0.50976371 0.50939223 0.49581801 0.49149625\n",
            " 0.4728305  0.4098403  0.40798483 0.40430836 0.39945427 0.29905599\n",
            " 0.29145795 0.24170305 0.23287952 0.24916131 0.52597848 0.5187117\n",
            " 0.51232001 0.50592065 0.49971469 0.49481273 0.48399203 0.42320772\n",
            " 0.41754366 0.34450636 0.38617302 0.38042854 0.28717831 0.28105086\n",
            " 0.2353305  0.23399203]\n",
            "[0.59635991 0.68880974 0.67817096 0.67600528 0.70774165 0.66680836\n",
            " 0.64257047 0.57483915 0.58366268 0.59273131 0.60689913 0.62244562\n",
            " 0.61694815 0.63459521 0.64404105 0.65678615 0.75825674 0.75725146\n",
            " 0.75616383 0.74426126 0.62182713 0.55433517 0.5379136  0.59273131\n",
            " 0.53394799 0.54096775 0.59474954 0.58361673 0.52434896 0.61405484\n",
            " 0.62640165 0.62449449 0.77073185 0.73163105 0.5744064  0.56781556\n",
            " 0.55591873 0.54156327 0.52636719 0.50970052 0.49036458 0.47416131\n",
            " 0.49443742 0.57912646 0.58929036 0.58279718 0.50588044 0.49341873\n",
            " 0.57710248 0.56953125 0.5603305  0.55629596 0.54058287 0.5376685\n",
            " 0.526394   0.51019263 0.49660501 0.48018344 0.46458908 0.56782322\n",
            " 0.59282322 0.61295573 0.60069508 0.57826478 0.56621285 0.55458027\n",
            " 0.55286458 0.54390893 0.54698223 0.53837699 0.52761949 0.5259038\n",
            " 0.49523974 0.48399203 0.49327895 0.46008732 0.44430722 0.56169577\n",
            " 0.62105737 0.6202742  0.5598403  0.5605756  0.5513691  0.56645795\n",
            " 0.54575674 0.53273974 0.52391621 0.51448568 0.49585823 0.49107498\n",
            " 0.48396523 0.48347695 0.46017731 0.44756242 0.47650697 0.61939913\n",
            " 0.56694815 0.56645795 0.5626685  0.56729856 0.54513442 0.52955538\n",
            " 0.52980047 0.53029067 0.511803   0.48502796 0.50416858 0.47912071\n",
            " 0.47576019 0.45243566 0.45957223 0.43480201 0.57492341 0.56659007\n",
            " 0.56876915 0.56533778 0.55257927 0.53787339 0.52523552 0.5296358\n",
            " 0.51740388 0.49998468 0.48896867 0.49107307 0.48137063 0.48517731\n",
            " 0.4526865  0.45339116]\n",
            "(28, 8)\n",
            "[ -7.42527676  -2.84805083  -0.70769686  -4.62243032  -0.49835262\n",
            " -10.27838326   9.33499908   4.41888285]\n",
            "[-6.29369736 -1.50755715  2.60037875 -3.30963945  6.5540576  -1.26007533\n",
            " 13.32176399  8.72059727]\n",
            "INP: [0.59635991 0.5855756  0.58312462 0.66523246 0.6853305  0.66449717\n",
            " 0.63729128 0.53606579 0.50028148 0.56400697 0.60689913 0.62185011\n",
            " 0.61694815 0.63459521 0.64194815 0.65371285 0.75763442 0.75199717\n",
            " 0.75616383 0.69537952 0.5733207  0.55273246 0.53606579 0.50567364\n",
            " 0.46768344 0.54096775 0.56694815 0.46204619 0.50542854 0.52969324\n",
            " 0.54709521 0.57430109 0.63900697 0.58018344 0.57111481 0.56131089\n",
            " 0.55591873 0.53680109 0.5225854  0.49219324 0.48361481 0.44562462\n",
            " 0.36792854 0.33067364 0.38361481 0.43385991 0.46988932 0.49341873\n",
            " 0.56204619 0.55689913 0.5603305  0.55199717 0.5358207  0.53704619\n",
            " 0.52185011 0.48900697 0.49660501 0.48018344 0.38042854 0.33165403\n",
            " 0.37895795 0.42920305 0.3983207  0.41964422 0.56621285 0.55028148\n",
            " 0.54145795 0.54390893 0.54268344 0.52969324 0.52430109 0.48876187\n",
            " 0.44317364 0.47724226 0.35175207 0.30150697 0.28680109 0.48998736\n",
            " 0.56547756 0.57160501 0.5598403  0.5605756  0.54243834 0.56645795\n",
            " 0.53067364 0.52135991 0.50469324 0.43140893 0.43631089 0.48778148\n",
            " 0.43753638 0.33189913 0.28704619 0.26841873 0.47650697 0.61939913\n",
            " 0.56694815 0.56645795 0.54930109 0.54488932 0.54513442 0.52871285\n",
            " 0.4728305  0.4098403  0.45640893 0.47748736 0.47601677 0.29905599\n",
            " 0.29145795 0.24170305 0.23287952 0.37037952 0.56131089 0.56596775\n",
            " 0.55640893 0.54145795 0.5478305  0.53116383 0.50763442 0.43067364\n",
            " 0.43214422 0.42871285 0.4603305  0.38042854 0.31645795 0.28263442\n",
            " 0.2353305  0.24023246]\n",
            "FINGER: [-7.2899795 -2.7850852 -0.6331256 -3.561718   3.4586718 -8.670766\n",
            " 12.458647   6.1008577]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"HE RULE:\")\n",
        "###########\n",
        "rule_neurons_list_he0 = []\n",
        "rule_neurons = (layer_10_he_rule_neurons0.array[0]).split(\",\")\n",
        "for indx in range(0, len(rule_neurons)):\n",
        "    rule_neurons[indx] = (rule_neurons[indx]).strip()\n",
        "    rule_neurons[indx] = (rule_neurons[indx]).replace(\"[\", \"\")\n",
        "    rule_neurons[indx] = (rule_neurons[indx]).replace(\"]\",\"\")\n",
        "    rule_neurons_list_he0.append(int(rule_neurons[indx]))\n",
        "\n",
        "print(rule_neurons_list_he0)\n",
        "\n",
        "rule_sig_list_he0 = []\n",
        "rule_sig = (layer_10_he_rule_signature0.array[0]).split(\",\")\n",
        "for indx in range(0, len(rule_sig)):\n",
        "    rule_sig[indx] = (rule_sig[indx]).strip()\n",
        "    rule_sig[indx] = (rule_sig[indx]).replace(\"[\", \"\")\n",
        "    rule_sig[indx] = (rule_sig[indx]).replace(\"]\",\"\")\n",
        "    if (indx % 2 == 0):\n",
        "      rule_sig[indx] = (rule_sig[indx]).replace(\"'\", \"\")\n",
        "      rule_sig_list_he0.append(rule_sig[indx])\n",
        "    else:\n",
        "      rule_sig_list_he0.append(float(rule_sig[indx]))\n",
        "\n",
        "print(rule_sig_list_he0)\n",
        "\n",
        "fngprnt = fingerprint_10[0]\n",
        "indices = get_suffix_cluster(rule_neurons_list_he0, rule_sig_list_he0, fngprnt,VAL=True)\n",
        "print(\"indices:\", len(indices))\n",
        "\n",
        "print(\"SORTED:\")\n",
        "sorted_indices = sorted(indices)\n",
        "print(sorted_indices)\n",
        "consecutive_min= [sorted_indices[0]]\n",
        "consecutive_m = 0\n",
        "consecutive_max = []\n",
        "consecutive_l = 0\n",
        "consecutive_len = []\n",
        "for indx in range(1, len(sorted_indices)):\n",
        "  if (sorted_indices[indx] - sorted_indices[indx-1] == 1):\n",
        "    consecutive_m = sorted_indices[indx]\n",
        "    consecutive_l = consecutive_l + 1\n",
        "  else:\n",
        "    consecutive_max.append(sorted_indices[indx-1])\n",
        "    consecutive_min.append(sorted_indices[indx])\n",
        "    consecutive_len.append(consecutive_l)\n",
        "    consecutive_l = 0\n",
        "    consecutive_m = 0\n",
        "\n",
        "print(\"MIN:\", consecutive_min)\n",
        "print(\"MAX:\", consecutive_max)\n",
        "\n",
        "max_indx = consecutive_len.index(np.max(consecutive_len))\n",
        "print(\"LEN:\",np.max(consecutive_len), max_indx)\n",
        "\n",
        "\n",
        "\n",
        "x_train_he0 = []\n",
        "fngprnt_he0 = []\n",
        "inp_he_ex0 = []\n",
        "finger_he_ex0 = []\n",
        "\n",
        "for indx in range(consecutive_min[max_indx], consecutive_max[max_indx]):\n",
        "    if (indx == consecutive_min[max_indx]):\n",
        "      inp_he_ex0 = x_train_flat[indx]\n",
        "      finger_he_ex0 = fingerprint_10[0][indx]\n",
        "\n",
        "    x_train_he0.append(x_train_flat[indx])\n",
        "    fngprnt_he0.append(fingerprint_10[0][indx])\n",
        "\n",
        "x_train_he0 = np.array(x_train_he0)\n",
        "fngprnt_he0 = np.array(fngprnt_he0)\n",
        "\n",
        "print(np.shape(x_train_he0))\n",
        "x_train_minhe0 = np.zeros(length)\n",
        "x_train_maxhe0 = np.zeros(length)\n",
        "\n",
        "for indx in range(0,length):\n",
        "  x_train_minhe0[indx] = np.min(x_train_he0[:,indx])\n",
        "  x_train_maxhe0[indx] = np.max(x_train_he0[:,indx])\n",
        "\n",
        "print(x_train_minhe0)\n",
        "print(x_train_maxhe0)\n",
        "\n",
        "print(np.shape(fngprnt_he0))\n",
        "fngprnt_min_he0 = np.zeros(len(fngprnt_he0[0]))\n",
        "fngprnt_max_he0 = np.zeros(len(fngprnt_he0[0]))\n",
        "\n",
        "for indx in range(0,len(fngprnt_he0[0])):\n",
        "  fngprnt_min_he0[indx] = np.min(fngprnt_he0[:,indx])\n",
        "  fngprnt_max_he0[indx] = np.max(fngprnt_he0[:,indx])\n",
        "\n",
        "print(fngprnt_min_he0)\n",
        "print(fngprnt_max_he0)\n",
        "\n",
        "\n",
        "print(\"INP:\", inp_he_ex0)\n",
        "print(\"FINGER:\", finger_he_ex0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMC6HO9XgQLN",
        "outputId": "50c6b4c7-0b53-49f5-ef0c-6f17c98985c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HE RULE:\n",
            "[2, 7, 4, 7, 1, 1, 5]\n",
            "['<=', 5.0200934410095215, '<=', 5.228757619857788, '>', 0.7042066156864166, '<=', 3.8628121614456177, '<=', -1.4256365299224854, '<=', -1.727737545967102, '>', -3.598828911781311]\n",
            "indices: 236\n",
            "SORTED:\n",
            "[2598, 2602, 2603, 2605, 2606, 2607, 2610, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2814, 2815, 2816, 2817, 2818, 2882, 2887, 2888, 2889, 2950, 2951, 2952, 2953, 2954, 3021, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3159, 3160, 3161, 3162, 3163, 3164, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3358, 3359, 3360, 3361, 3362, 3366, 3370, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3573, 3574, 3575, 3576, 3578, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3749, 3750, 3751, 3753, 3754, 3820, 3821, 3888, 3889, 3890, 3891, 3892, 3893, 3956, 3957, 3963, 4024, 4025, 4026, 4093, 4094, 4095, 4096, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4166, 4231, 4233, 4234, 4235, 4236, 4296, 4297, 4369, 4370, 4436, 4438, 4439, 4440, 4441, 4442, 4443, 4499, 4500, 4501, 4502, 4503, 4505, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4633, 4634, 4635, 4636, 4637, 4638, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4905, 5131, 5132, 5133, 5134, 5199, 5273, 5274, 5275, 5343, 5344, 5346, 5347, 5415, 5480, 5481, 5616, 5617, 5618, 5619, 5683, 5684, 5749, 5751, 5752, 5753, 5754, 5888, 5889, 5891, 5892, 5893]\n",
            "MIN: [2598, 2602, 2605, 2610, 2671, 2742, 2814, 2882, 2887, 2950, 3021, 3090, 3159, 3226, 3293, 3358, 3366, 3370, 3432, 3498, 3573, 3578, 3638, 3678, 3749, 3753, 3820, 3888, 3956, 3963, 4024, 4093, 4156, 4166, 4231, 4233, 4296, 4369, 4436, 4438, 4499, 4505, 4565, 4633, 4699, 4770, 4905, 5131, 5199, 5273, 5343, 5346, 5415, 5480, 5616, 5683, 5749, 5751, 5888, 5891]\n",
            "MAX: [2598, 2603, 2607, 2610, 2678, 2748, 2818, 2882, 2889, 2954, 3021, 3096, 3164, 3233, 3301, 3362, 3366, 3370, 3440, 3508, 3576, 3578, 3644, 3686, 3751, 3754, 3821, 3893, 3957, 3963, 4026, 4096, 4164, 4166, 4231, 4236, 4297, 4370, 4436, 4443, 4503, 4505, 4573, 4638, 4707, 4779, 4905, 5134, 5199, 5275, 5344, 5347, 5415, 5481, 5619, 5684, 5749, 5754, 5889]\n",
            "LEN: 10 19\n",
            "(10, 128)\n",
            "[0.57662952 0.59623736 0.63618834 0.61803194 0.60402688 0.58520795\n",
            " 0.52903837 0.60336435 0.50329159 0.5796875  0.60003064 0.63025237\n",
            " 0.63624579 0.57552083 0.57772672 0.57546147 0.65898246 0.60550896\n",
            " 0.52736481 0.51337508 0.49006778 0.450607   0.46296147 0.57413641\n",
            " 0.41278148 0.45885417 0.48973652 0.50836397 0.53434436 0.68011834\n",
            " 0.67662952 0.66697304 0.50028148 0.50262714 0.48792126 0.46365656\n",
            " 0.43858571 0.40726677 0.64033586 0.40899969 0.40713848 0.4346201\n",
            " 0.45511834 0.4801777  0.49463848 0.50248162 0.50854971 0.52113971\n",
            " 0.46443206 0.4603305  0.45101103 0.42174479 0.41194278 0.39874579\n",
            " 0.62591146 0.40321691 0.40520833 0.4221201  0.44488358 0.45805951\n",
            " 0.47472618 0.48655599 0.49550207 0.50315755 0.44863473 0.42542126\n",
            " 0.41880553 0.40360754 0.40256778 0.43565602 0.40630362 0.40696806\n",
            " 0.40628638 0.41859873 0.435679   0.45422794 0.45836397 0.47462661\n",
            " 0.47702206 0.4854339  0.42150161 0.41474226 0.40696806 0.40623277\n",
            " 0.40403837 0.52198989 0.41064453 0.40623277 0.41155025 0.4227654\n",
            " 0.42613932 0.44463848 0.45251225 0.46302658 0.46884383 0.47315602\n",
            " 0.41081495 0.41756089 0.40892884 0.40640319 0.40653148 0.41880362\n",
            " 0.40672296 0.40125613 0.41596201 0.41873277 0.42685547 0.4349437\n",
            " 0.45013978 0.45398284 0.45869524 0.46770833 0.42248009 0.40059551\n",
            " 0.41204044 0.39576631 0.40648935 0.40924479 0.40150697 0.39864622\n",
            " 0.39586972 0.40732422 0.42172756 0.42515893 0.44121285 0.44417892\n",
            " 0.45966414 0.46329657]\n",
            "[0.58492073 0.63124426 0.64940449 0.63007812 0.61439185 0.60017616\n",
            " 0.58522518 0.62830308 0.57462661 0.59815602 0.62144033 0.64548292\n",
            " 0.66145067 0.58473499 0.58443053 0.5832644  0.69355852 0.64512868\n",
            " 0.53865847 0.52052122 0.50066828 0.46677198 0.63988205 0.63704044\n",
            " 0.43767616 0.47977367 0.50213695 0.51904871 0.57195351 0.70483303\n",
            " 0.69895067 0.68154871 0.52391429 0.51188151 0.49695351 0.47438151\n",
            " 0.4479339  0.5960095  0.65802122 0.64212814 0.42934283 0.45483303\n",
            " 0.47321538 0.49649969 0.50948989 0.5210095  0.52860754 0.53841146\n",
            " 0.49574717 0.47732269 0.45943053 0.44719861 0.4462546  0.6577742\n",
            " 0.66509076 0.59619524 0.44233303 0.45213695 0.46905063 0.49208793\n",
            " 0.4957644  0.5080193  0.51316636 0.51635263 0.47491383 0.46163641\n",
            " 0.44548292 0.43124426 0.65164675 0.65990924 0.66653837 0.47462661\n",
            " 0.42836435 0.4462546  0.46243107 0.47002911 0.48816636 0.49797028\n",
            " 0.51292126 0.50851141 0.45798292 0.43863932 0.44178347 0.50557024\n",
            " 0.64961512 0.66296147 0.66751877 0.42762906 0.43252911 0.44502911\n",
            " 0.46120558 0.48007812 0.47762714 0.49674479 0.49797028 0.50703891\n",
            " 0.44425743 0.43998736 0.43912952 0.60745826 0.66155599 0.66247128\n",
            " 0.6475203  0.42040441 0.43792318 0.44429381 0.46733494 0.48130362\n",
            " 0.48890165 0.47836435 0.49846239 0.49258004 0.44408892 0.44398935\n",
            " 0.44212814 0.62012868 0.68109873 0.66927658 0.66207108 0.44380362\n",
            " 0.43865847 0.45924479 0.4582644  0.47003102 0.47640165 0.48056832\n",
            " 0.48106043 0.489321  ]\n",
            "(10, 8)\n",
            "[-6.94442987 -1.87823415 -1.70118594 -4.15793371  1.40696526 -1.82176363\n",
            "  6.54738855  1.76062691]\n",
            "[-6.56526709 -1.73704898 -1.10832715 -3.63316441  2.12218642  0.49739575\n",
            "  8.06357765  3.816432  ]\n",
            "INP: [0.58448989 0.62468597 0.64208793 0.63007812 0.61439185 0.60017616\n",
            " 0.58522518 0.60630362 0.5080193  0.59110754 0.61610754 0.63767616\n",
            " 0.66145067 0.58473499 0.58399969 0.5832644  0.69355852 0.62493107\n",
            " 0.53816636 0.5202742  0.50017616 0.46267616 0.63988205 0.62444087\n",
            " 0.43767616 0.47517616 0.50213695 0.51904871 0.54698989 0.70483303\n",
            " 0.69895067 0.68154871 0.51708793 0.50262714 0.48792126 0.46365656\n",
            " 0.44208793 0.5960095  0.6457644  0.40899969 0.42934283 0.45483303\n",
            " 0.47321538 0.49649969 0.50948989 0.5210095  0.52860754 0.53841146\n",
            " 0.48179381 0.4702742  0.45336244 0.42174479 0.4462546  0.6577742\n",
            " 0.62591146 0.41659773 0.44233303 0.45213695 0.46659773 0.49208793\n",
            " 0.4957644  0.5080193  0.51316636 0.51635263 0.45532322 0.42542126\n",
            " 0.4205193  0.40360754 0.65164675 0.62934283 0.40630362 0.42615656\n",
            " 0.42738205 0.4462546  0.46243107 0.47002911 0.48816636 0.49797028\n",
            " 0.51292126 0.5082644  0.42909773 0.42468597 0.41733303 0.4587546\n",
            " 0.57909773 0.52198989 0.41512714 0.42664675 0.43252911 0.44502911\n",
            " 0.46120558 0.48007812 0.47762714 0.49674479 0.49797028 0.50703891\n",
            " 0.42370558 0.42321538 0.42002911 0.42174479 0.4077742  0.41880362\n",
            " 0.4330193  0.41978401 0.42002911 0.44429381 0.4582644  0.48130362\n",
            " 0.48890165 0.47787224 0.49478401 0.49110754 0.42248009 0.42419577\n",
            " 0.42591146 0.40287224 0.41512714 0.40924479 0.41341146 0.44380362\n",
            " 0.42640165 0.45924479 0.4582644  0.4587546  0.47640165 0.48056832\n",
            " 0.47517616 0.48081342]\n",
            "FINGER: [-6.565267   -1.737049   -1.701186   -3.6331644   1.7123669   0.49739575\n",
            "  6.5473886   1.7606269 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROVE RULES USING MARABOU**"
      ],
      "metadata": {
        "id": "ZaPHj6E3XUDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MARABOU SETUP:**\n",
        "\n",
        "IMPORTANT: INSTALL AND BUILD MARABOU from https://github.com/NeuralNetworkVerification/Marabou.\n",
        "\n",
        "The following code assumes /content/drive/MyDrive/Marabou_bld to be the Marabou build directory"
      ],
      "metadata": {
        "id": "suPjrBxzqQY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Marabou_bld')\n",
        "!pwd\n",
        "!ls -lt\n",
        "path = os.environ['PATH']\n",
        "print(path)\n",
        "#os.environ['PATH'] = '/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/content/Marabou/build/Marabou:/content/Marabou/build/Marabou/build:/content/Marabou/build/Marabou/build/bin:/content/Marabou/build/Marabou:/content/Marabou/build/Marabou/build:/content/Marabou/build/Marabou/build/bin'\n",
        "print(os.environ['PATH'])\n",
        "os.environ['PATH'] = path + ':/content/drive/MyDrive/Marabou_bld:/content/drive/MyDrive/Marabou_bld/build:/content/drive/MyDrive/Marabou_bld/build/bin'\n",
        "print(os.environ['PATH'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48YOwg_eVUVH",
        "outputId": "407a7517-083f-4421-c7b6-4a43bfe335ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1ViAk8CjCXd",
        "outputId": "fe4cefc2-c011-48a7-edb7-39bd5b0dffa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.25.5)\n",
            "Requirement already satisfied: onnx-tf in /usr/local/lib/python3.10/dist-packages (1.10.0)\n",
            "Requirement already satisfied: onnx>=1.10.2 in /usr/local/lib/python3.10/dist-packages (from onnx-tf) (1.17.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from onnx-tf) (6.0.2)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (from onnx-tf) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.10.2->onnx-tf) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.10.2->onnx-tf) (4.25.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons->onnx-tf) (24.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons->onnx-tf) (2.13.3)\n",
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.10/dist-packages (2.11.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (3.31.2)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.20.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install onnx\n",
        "! pip install onnx-tf\n",
        "\n",
        "! pip install pybind11\n",
        "! pip install cmake\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "import onnx\n",
        "!pip install onnxruntime\n",
        "import onnxruntime\n",
        "\n",
        "#import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model = onnx.load(\"/content/ProphecyPlus/dataset_models/kj_tiny_taxinet/KJ_TinyTaxiNet.onnx\")\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "print(onnx.helper.printable_graph(onnx_model.graph))\n",
        "\n",
        "input_names = [input.name for input in onnx_model.graph.input]\n",
        "print(\"Input layer names:\", input_names)\n",
        "\n",
        "onnx_outputs = [i.name for i in onnx_model.graph.output]\n",
        "print(\"output names:\", onnx_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjrAHYkcWFG7",
        "outputId": "7e8d0dcb-ff4b-4f12-fe21-234cacb6ef14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "graph tf2onnx (\n",
            "  %X_place:0[FLOAT, unk__11x8x16x1]\n",
            ") optional inputs with matching initializers (\n",
            "  %dense_2/kernel:0[FLOAT, 8x2]\n",
            "  %dense_2/bias:0[FLOAT, 2]\n",
            "  %dense_1/kernel:0[FLOAT, 8x8]\n",
            "  %dense_1/bias:0[FLOAT, 8]\n",
            "  %dense/kernel:0[FLOAT, 16x8]\n",
            "  %dense/bias:0[FLOAT, 8]\n",
            "  %conv2d/kernel:0[FLOAT, 8x1x8x8]\n",
            "  %conv2d/bias:0[FLOAT, 8]\n",
            "  %Reshape/shape:0[INT32, 2]\n",
            ") {\n",
            "  %conv2d/Conv2D__6:0 = Transpose[perm = [0, 3, 1, 2]](%X_place:0)\n",
            "  %conv2d/Conv2D:0 = Conv[dilations = [1, 1], kernel_shape = [8, 8], pads = [0, 0, 0, 0], strides = [8, 8]](%conv2d/Conv2D__6:0, %conv2d/kernel:0)\n",
            "  %conv2d/Conv2D__8:0 = Transpose[perm = [0, 2, 3, 1]](%conv2d/Conv2D:0)\n",
            "  %conv2d/BiasAdd:0 = Add(%conv2d/Conv2D__8:0, %conv2d/bias:0)\n",
            "  %relu:0 = Relu(%conv2d/BiasAdd:0)\n",
            "  %Reshape__10:0 = Cast[to = 7](%Reshape/shape:0)\n",
            "  %Reshape:0 = Reshape(%relu:0, %Reshape__10:0)\n",
            "  %dense/MatMul:0 = MatMul(%Reshape:0, %dense/kernel:0)\n",
            "  %dense/BiasAdd:0 = Add(%dense/MatMul:0, %dense/bias:0)\n",
            "  %relu_1:0 = Relu(%dense/BiasAdd:0)\n",
            "  %dense_1/MatMul:0 = MatMul(%relu_1:0, %dense_1/kernel:0)\n",
            "  %dense_1/BiasAdd:0 = Add(%dense_1/MatMul:0, %dense_1/bias:0)\n",
            "  %relu_2:0 = Relu(%dense_1/BiasAdd:0)\n",
            "  %dense_2/MatMul:0 = MatMul(%relu_2:0, %dense_2/kernel:0)\n",
            "  %dense_2/BiasAdd_raw_output___3:0 = Add(%dense_2/MatMul:0, %dense_2/bias:0)\n",
            "  %dense_2/BiasAdd:0 = Identity(%dense_2/BiasAdd_raw_output___3:0)\n",
            "  return %dense_2/BiasAdd:0\n",
            "}\n",
            "Input layer names: ['X_place:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'dense/kernel:0', 'dense/bias:0', 'conv2d/kernel:0', 'conv2d/bias:0', 'Reshape/shape:0']\n",
            "output names: ['dense_2/BiasAdd:0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROVE RULE FOR SAT OF SAFETY PROPERTY |cte| <= 4.3**"
      ],
      "metadata": {
        "id": "0Re1JhvGwBBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from maraboupy import Marabou\n",
        "from maraboupy.MarabouCore import *\n",
        "from maraboupy.MarabouPythonic import *\n",
        "\n",
        "options = Marabou.createOptions(verbosity = 1, numWorkers=1, numBlasThreads=1,snc=True)\n",
        "\n",
        "filename = \"/content/ProphecyPlus/dataset_models/kj_tiny_taxinet/KJ_TinyTaxiNet.onnx\"\n",
        "network_a = Marabou.read_onnx(filename)\n",
        "\n",
        "\n",
        "print(\"INPUT VARS\")\n",
        "invars = network_a.inputVars[0][0].flatten()\n",
        "\n",
        "\n",
        "\n",
        "for indx in range(0,len(invars)):\n",
        "    i = invars[indx]\n",
        "    v = Var(i)\n",
        "    network_a.setLowerBound(i,x_train_minCTE[i])\n",
        "    network_a.setUpperBound(i,x_train_maxCTE[i])\n",
        "    #network_a.setLowerBound(i,inp_ex[indx])\n",
        "    #network_a.setUpperBound(i,inp_ex[indx])\n",
        "\n",
        "\n",
        "print(\"LAYER VARS MAP\")\n",
        "print(network_a.layerNameToVariables)\n",
        "\n",
        "dense_10_neurons = network_a.layerNameToVariables[\"dense_1/BiasAdd:0\"][0]\n",
        "print(np.shape(dense_10_neurons))\n",
        "print(len(dense_10_neurons))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for indx in range(0, len(dense_10_neurons)):\n",
        "    neuron_indx = dense_10_neurons[indx] - dense_10_neurons[0]\n",
        "    network_a.setLowerBound(dense_10_neurons[indx], fngprnt_min_cte[neuron_indx])\n",
        "    network_a.setUpperBound(dense_10_neurons[indx], fngprnt_max_cte[neuron_indx])\n",
        "    #network_a.setLowerBound(dense_10_neurons[indx], finger_ex[neuron_indx] - 1.0)\n",
        "    #network_a.setUpperBound(dense_10_neurons[indx], finger_ex[neuron_indx] + 1.0)\n",
        "\n",
        "    if (neuron_indx in rule_neurons_list_cte):\n",
        "      v = Var(dense_10_neurons[indx])\n",
        "\n",
        "      for indx in range(0, len(rule_neurons_list_cte)):\n",
        "        if (rule_neurons_list_cte[indx] == neuron_indx):\n",
        "          index = indx\n",
        "          sig_indx = (2*index) + 1\n",
        "          print(\"N:\", neuron_indx, index)\n",
        "          print(\"OP:\", rule_sig_list_cte[sig_indx-1])\n",
        "          print(\"VAL:\", rule_sig_list_cte[sig_indx])\n",
        "          if (rule_sig_list_cte[sig_indx-1] == '<='):\n",
        "            network_a.addConstraint(float(rule_sig_list_cte[sig_indx]) >= v)\n",
        "          else:\n",
        "            network_a.addConstraint(v >= float(rule_sig_list_cte[sig_indx]))\n",
        "\n",
        "\n",
        "\n",
        "print(\"OUTPUT VARS\")\n",
        "outvars = network_a.outputVars[0].flatten()\n",
        "print(outvars)\n",
        "label_var = Var(outvars[0])\n",
        "\n",
        "# |cte| >= 4.3 should be UNSAT\n",
        "# CONSTRAINT 1\n",
        "network_a.addConstraint(label_var >= 4.3)\n",
        "# CONSTRAINT 2\n",
        "#network_a.addConstraint(-4.3 >= label_var)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjcPH45YWfjT",
        "outputId": "1f198516-d938-4111-d39c-8992be8b21dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT VARS\n",
            "LAYER VARS MAP\n",
            "{'X_place:0': array([[[[  0],\n",
            "         [  1],\n",
            "         [  2],\n",
            "         [  3],\n",
            "         [  4],\n",
            "         [  5],\n",
            "         [  6],\n",
            "         [  7],\n",
            "         [  8],\n",
            "         [  9],\n",
            "         [ 10],\n",
            "         [ 11],\n",
            "         [ 12],\n",
            "         [ 13],\n",
            "         [ 14],\n",
            "         [ 15]],\n",
            "\n",
            "        [[ 16],\n",
            "         [ 17],\n",
            "         [ 18],\n",
            "         [ 19],\n",
            "         [ 20],\n",
            "         [ 21],\n",
            "         [ 22],\n",
            "         [ 23],\n",
            "         [ 24],\n",
            "         [ 25],\n",
            "         [ 26],\n",
            "         [ 27],\n",
            "         [ 28],\n",
            "         [ 29],\n",
            "         [ 30],\n",
            "         [ 31]],\n",
            "\n",
            "        [[ 32],\n",
            "         [ 33],\n",
            "         [ 34],\n",
            "         [ 35],\n",
            "         [ 36],\n",
            "         [ 37],\n",
            "         [ 38],\n",
            "         [ 39],\n",
            "         [ 40],\n",
            "         [ 41],\n",
            "         [ 42],\n",
            "         [ 43],\n",
            "         [ 44],\n",
            "         [ 45],\n",
            "         [ 46],\n",
            "         [ 47]],\n",
            "\n",
            "        [[ 48],\n",
            "         [ 49],\n",
            "         [ 50],\n",
            "         [ 51],\n",
            "         [ 52],\n",
            "         [ 53],\n",
            "         [ 54],\n",
            "         [ 55],\n",
            "         [ 56],\n",
            "         [ 57],\n",
            "         [ 58],\n",
            "         [ 59],\n",
            "         [ 60],\n",
            "         [ 61],\n",
            "         [ 62],\n",
            "         [ 63]],\n",
            "\n",
            "        [[ 64],\n",
            "         [ 65],\n",
            "         [ 66],\n",
            "         [ 67],\n",
            "         [ 68],\n",
            "         [ 69],\n",
            "         [ 70],\n",
            "         [ 71],\n",
            "         [ 72],\n",
            "         [ 73],\n",
            "         [ 74],\n",
            "         [ 75],\n",
            "         [ 76],\n",
            "         [ 77],\n",
            "         [ 78],\n",
            "         [ 79]],\n",
            "\n",
            "        [[ 80],\n",
            "         [ 81],\n",
            "         [ 82],\n",
            "         [ 83],\n",
            "         [ 84],\n",
            "         [ 85],\n",
            "         [ 86],\n",
            "         [ 87],\n",
            "         [ 88],\n",
            "         [ 89],\n",
            "         [ 90],\n",
            "         [ 91],\n",
            "         [ 92],\n",
            "         [ 93],\n",
            "         [ 94],\n",
            "         [ 95]],\n",
            "\n",
            "        [[ 96],\n",
            "         [ 97],\n",
            "         [ 98],\n",
            "         [ 99],\n",
            "         [100],\n",
            "         [101],\n",
            "         [102],\n",
            "         [103],\n",
            "         [104],\n",
            "         [105],\n",
            "         [106],\n",
            "         [107],\n",
            "         [108],\n",
            "         [109],\n",
            "         [110],\n",
            "         [111]],\n",
            "\n",
            "        [[112],\n",
            "         [113],\n",
            "         [114],\n",
            "         [115],\n",
            "         [116],\n",
            "         [117],\n",
            "         [118],\n",
            "         [119],\n",
            "         [120],\n",
            "         [121],\n",
            "         [122],\n",
            "         [123],\n",
            "         [124],\n",
            "         [125],\n",
            "         [126],\n",
            "         [127]]]]), 'conv2d/Conv2D__6:0': array([[[[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,\n",
            "           12,  13,  14,  15],\n",
            "         [ 16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "           28,  29,  30,  31],\n",
            "         [ 32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,\n",
            "           44,  45,  46,  47],\n",
            "         [ 48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,\n",
            "           60,  61,  62,  63],\n",
            "         [ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,\n",
            "           76,  77,  78,  79],\n",
            "         [ 80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
            "           92,  93,  94,  95],\n",
            "         [ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
            "          108, 109, 110, 111],\n",
            "         [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
            "          124, 125, 126, 127]]]]), 'conv2d/Conv2D:0': array([[[[128, 129]],\n",
            "\n",
            "        [[130, 131]],\n",
            "\n",
            "        [[132, 133]],\n",
            "\n",
            "        [[134, 135]],\n",
            "\n",
            "        [[136, 137]],\n",
            "\n",
            "        [[138, 139]],\n",
            "\n",
            "        [[140, 141]],\n",
            "\n",
            "        [[142, 143]]]]), 'conv2d/Conv2D__8:0': array([[[[128, 130, 132, 134, 136, 138, 140, 142],\n",
            "         [129, 131, 133, 135, 137, 139, 141, 143]]]]), 'conv2d/BiasAdd:0': array([[[[128, 130, 132, 134, 136, 138, 140, 142],\n",
            "         [129, 131, 133, 135, 137, 139, 141, 143]]]]), 'relu:0': array([[[[144, 145, 146, 147, 148, 149, 150, 151],\n",
            "         [152, 153, 154, 155, 156, 157, 158, 159]]]]), 'Reshape:0': array([[144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
            "        157, 158, 159]]), 'dense/MatMul:0': array([[160, 161, 162, 163, 164, 165, 166, 167]]), 'dense/BiasAdd:0': array([[160, 161, 162, 163, 164, 165, 166, 167]]), 'relu_1:0': array([[168, 169, 170, 171, 172, 173, 174, 175]]), 'dense_1/MatMul:0': array([[176, 177, 178, 179, 180, 181, 182, 183]]), 'dense_1/BiasAdd:0': array([[176, 177, 178, 179, 180, 181, 182, 183]]), 'relu_2:0': array([[184, 185, 186, 187, 188, 189, 190, 191]]), 'dense_2/MatMul:0': array([[192, 193]]), 'dense_2/BiasAdd_raw_output___3:0': array([[192, 193]]), 'dense_2/BiasAdd:0': array([[192, 193]])}\n",
            "(8,)\n",
            "8\n",
            "N: 2 6\n",
            "OP: <=\n",
            "VAL: 0.710400253534317\n",
            "N: 2 7\n",
            "OP: <=\n",
            "VAL: -0.7180337011814117\n",
            "N: 5 0\n",
            "OP: <=\n",
            "VAL: 5.014780044555664\n",
            "N: 5 3\n",
            "OP: <=\n",
            "VAL: 4.407367944717407\n",
            "N: 5 4\n",
            "OP: <=\n",
            "VAL: 0.42133453488349915\n",
            "N: 6 1\n",
            "OP: <=\n",
            "VAL: 8.457788467407227\n",
            "N: 7 2\n",
            "OP: <=\n",
            "VAL: 6.801446199417114\n",
            "N: 7 5\n",
            "OP: <=\n",
            "VAL: 4.016632080078125\n",
            "OUTPUT VARS\n",
            "[192 193]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtvxH-twzS7A",
        "outputId": "b7acd12f-4345-4503-a436-3fa15164adcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unsat\n",
            "sat_unsat: unsat\n",
            "vals: {}\n",
            "stats: <maraboupy.MarabouCore.Statistics object at 0x7bb4aef9e1f0>\n"
          ]
        }
      ],
      "source": [
        "#print(network_a.getInputQuery())\n",
        "sat_unsat,vals, stats = network_a.solve(options = options)\n",
        "\n",
        "print(\"sat_unsat:\", sat_unsat)\n",
        "print(\"vals:\", vals)\n",
        "print(\"stats:\", stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROVE RULE FOR VIOLATION OF SAFETY PROPERTY |cte| <= 4.3**"
      ],
      "metadata": {
        "id": "tqT_A_dQv67G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from maraboupy import Marabou\n",
        "from maraboupy.MarabouCore import *\n",
        "from maraboupy.MarabouPythonic import *\n",
        "\n",
        "options = Marabou.createOptions(verbosity = 1, numWorkers=1, numBlasThreads=1,snc=True)\n",
        "\n",
        "filename = \"/content/ProphecyPlus/dataset_models/kj_tiny_taxinet/KJ_TinyTaxiNet.onnx\"\n",
        "network_a = Marabou.read_onnx(filename)\n",
        "\n",
        "\n",
        "print(\"INPUT VARS\")\n",
        "invars = network_a.inputVars[0][0].flatten()\n",
        "\n",
        "\n",
        "\n",
        "for indx in range(0,len(invars)):\n",
        "    i = invars[indx]\n",
        "    v = Var(i)\n",
        "    network_a.setLowerBound(i,x_train_minCTE0[i])\n",
        "    network_a.setUpperBound(i,x_train_maxCTE0[i])\n",
        "    #network_a.setLowerBound(i,inp_ex0[indx])\n",
        "    #network_a.setUpperBound(i,inp_ex0[indx])\n",
        "\n",
        "\n",
        "print(\"LAYER VARS MAP\")\n",
        "print(network_a.layerNameToVariables)\n",
        "\n",
        "dense_10_neurons = network_a.layerNameToVariables[\"dense_1/BiasAdd:0\"][0]\n",
        "print(np.shape(dense_10_neurons))\n",
        "print(len(dense_10_neurons))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for indx in range(0, len(dense_10_neurons)):\n",
        "    neuron_indx = dense_10_neurons[indx] - dense_10_neurons[0]\n",
        "    network_a.setLowerBound(dense_10_neurons[indx], fngprnt_min_cte0[neuron_indx])\n",
        "    network_a.setUpperBound(dense_10_neurons[indx], fngprnt_max_cte0[neuron_indx])\n",
        "   # network_a.setLowerBound(dense_10_neurons[indx], finger_ex0[neuron_indx] - 1.0)\n",
        "   # network_a.setUpperBound(dense_10_neurons[indx], finger_ex0[neuron_indx] + 1.0)\n",
        "\n",
        "    if (neuron_indx in rule_neurons_list_cte0):\n",
        "      v = Var(dense_10_neurons[indx])\n",
        "\n",
        "      for indx in range(0, len(rule_neurons_list_cte0)):\n",
        "        if (rule_neurons_list_cte0[indx] == neuron_indx):\n",
        "          index = indx\n",
        "          sig_indx = (2*index) + 1\n",
        "          print(\"N:\", neuron_indx, index)\n",
        "          print(\"OP:\", rule_sig_list_cte0[sig_indx-1])\n",
        "          print(\"VAL:\", rule_sig_list_cte0[sig_indx])\n",
        "          if (rule_sig_list_cte0[sig_indx-1] == '<='):\n",
        "            network_a.addConstraint(float(rule_sig_list_cte0[sig_indx]) >= v)\n",
        "          else:\n",
        "            network_a.addConstraint(v >= float(rule_sig_list_cte0[sig_indx]))\n",
        "\n",
        "\n",
        "\n",
        "print(\"OUTPUT VARS\")\n",
        "outvars = network_a.outputVars[0].flatten()\n",
        "print(outvars)\n",
        "label_var = Var(outvars[0])\n",
        "\n",
        "# |cte| < 4.3  should be UNSAT\n",
        "\n",
        "network_a.addConstraint(4.3 >= label_var)\n",
        "network_a.addConstraint(label_var >= -4.3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7AS-tpDkH9o",
        "outputId": "9611b171-d235-4a08-d6e5-542dbcedcca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT VARS\n",
            "LAYER VARS MAP\n",
            "{'X_place:0': array([[[[  0],\n",
            "         [  1],\n",
            "         [  2],\n",
            "         [  3],\n",
            "         [  4],\n",
            "         [  5],\n",
            "         [  6],\n",
            "         [  7],\n",
            "         [  8],\n",
            "         [  9],\n",
            "         [ 10],\n",
            "         [ 11],\n",
            "         [ 12],\n",
            "         [ 13],\n",
            "         [ 14],\n",
            "         [ 15]],\n",
            "\n",
            "        [[ 16],\n",
            "         [ 17],\n",
            "         [ 18],\n",
            "         [ 19],\n",
            "         [ 20],\n",
            "         [ 21],\n",
            "         [ 22],\n",
            "         [ 23],\n",
            "         [ 24],\n",
            "         [ 25],\n",
            "         [ 26],\n",
            "         [ 27],\n",
            "         [ 28],\n",
            "         [ 29],\n",
            "         [ 30],\n",
            "         [ 31]],\n",
            "\n",
            "        [[ 32],\n",
            "         [ 33],\n",
            "         [ 34],\n",
            "         [ 35],\n",
            "         [ 36],\n",
            "         [ 37],\n",
            "         [ 38],\n",
            "         [ 39],\n",
            "         [ 40],\n",
            "         [ 41],\n",
            "         [ 42],\n",
            "         [ 43],\n",
            "         [ 44],\n",
            "         [ 45],\n",
            "         [ 46],\n",
            "         [ 47]],\n",
            "\n",
            "        [[ 48],\n",
            "         [ 49],\n",
            "         [ 50],\n",
            "         [ 51],\n",
            "         [ 52],\n",
            "         [ 53],\n",
            "         [ 54],\n",
            "         [ 55],\n",
            "         [ 56],\n",
            "         [ 57],\n",
            "         [ 58],\n",
            "         [ 59],\n",
            "         [ 60],\n",
            "         [ 61],\n",
            "         [ 62],\n",
            "         [ 63]],\n",
            "\n",
            "        [[ 64],\n",
            "         [ 65],\n",
            "         [ 66],\n",
            "         [ 67],\n",
            "         [ 68],\n",
            "         [ 69],\n",
            "         [ 70],\n",
            "         [ 71],\n",
            "         [ 72],\n",
            "         [ 73],\n",
            "         [ 74],\n",
            "         [ 75],\n",
            "         [ 76],\n",
            "         [ 77],\n",
            "         [ 78],\n",
            "         [ 79]],\n",
            "\n",
            "        [[ 80],\n",
            "         [ 81],\n",
            "         [ 82],\n",
            "         [ 83],\n",
            "         [ 84],\n",
            "         [ 85],\n",
            "         [ 86],\n",
            "         [ 87],\n",
            "         [ 88],\n",
            "         [ 89],\n",
            "         [ 90],\n",
            "         [ 91],\n",
            "         [ 92],\n",
            "         [ 93],\n",
            "         [ 94],\n",
            "         [ 95]],\n",
            "\n",
            "        [[ 96],\n",
            "         [ 97],\n",
            "         [ 98],\n",
            "         [ 99],\n",
            "         [100],\n",
            "         [101],\n",
            "         [102],\n",
            "         [103],\n",
            "         [104],\n",
            "         [105],\n",
            "         [106],\n",
            "         [107],\n",
            "         [108],\n",
            "         [109],\n",
            "         [110],\n",
            "         [111]],\n",
            "\n",
            "        [[112],\n",
            "         [113],\n",
            "         [114],\n",
            "         [115],\n",
            "         [116],\n",
            "         [117],\n",
            "         [118],\n",
            "         [119],\n",
            "         [120],\n",
            "         [121],\n",
            "         [122],\n",
            "         [123],\n",
            "         [124],\n",
            "         [125],\n",
            "         [126],\n",
            "         [127]]]]), 'conv2d/Conv2D__6:0': array([[[[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,\n",
            "           12,  13,  14,  15],\n",
            "         [ 16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "           28,  29,  30,  31],\n",
            "         [ 32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,\n",
            "           44,  45,  46,  47],\n",
            "         [ 48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,\n",
            "           60,  61,  62,  63],\n",
            "         [ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,\n",
            "           76,  77,  78,  79],\n",
            "         [ 80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
            "           92,  93,  94,  95],\n",
            "         [ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
            "          108, 109, 110, 111],\n",
            "         [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
            "          124, 125, 126, 127]]]]), 'conv2d/Conv2D:0': array([[[[128, 129]],\n",
            "\n",
            "        [[130, 131]],\n",
            "\n",
            "        [[132, 133]],\n",
            "\n",
            "        [[134, 135]],\n",
            "\n",
            "        [[136, 137]],\n",
            "\n",
            "        [[138, 139]],\n",
            "\n",
            "        [[140, 141]],\n",
            "\n",
            "        [[142, 143]]]]), 'conv2d/Conv2D__8:0': array([[[[128, 130, 132, 134, 136, 138, 140, 142],\n",
            "         [129, 131, 133, 135, 137, 139, 141, 143]]]]), 'conv2d/BiasAdd:0': array([[[[128, 130, 132, 134, 136, 138, 140, 142],\n",
            "         [129, 131, 133, 135, 137, 139, 141, 143]]]]), 'relu:0': array([[[[144, 145, 146, 147, 148, 149, 150, 151],\n",
            "         [152, 153, 154, 155, 156, 157, 158, 159]]]]), 'Reshape:0': array([[144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
            "        157, 158, 159]]), 'dense/MatMul:0': array([[160, 161, 162, 163, 164, 165, 166, 167]]), 'dense/BiasAdd:0': array([[160, 161, 162, 163, 164, 165, 166, 167]]), 'relu_1:0': array([[168, 169, 170, 171, 172, 173, 174, 175]]), 'dense_1/MatMul:0': array([[176, 177, 178, 179, 180, 181, 182, 183]]), 'dense_1/BiasAdd:0': array([[176, 177, 178, 179, 180, 181, 182, 183]]), 'relu_2:0': array([[184, 185, 186, 187, 188, 189, 190, 191]]), 'dense_2/MatMul:0': array([[192, 193]]), 'dense_2/BiasAdd_raw_output___3:0': array([[192, 193]]), 'dense_2/BiasAdd:0': array([[192, 193]])}\n",
            "(8,)\n",
            "8\n",
            "N: 2 3\n",
            "OP: >\n",
            "VAL: -0.7284570932388306\n",
            "N: 5 0\n",
            "OP: <=\n",
            "VAL: 5.014780044555664\n",
            "N: 5 2\n",
            "OP: <=\n",
            "VAL: 0.45192472636699677\n",
            "N: 5 5\n",
            "OP: <=\n",
            "VAL: 0.23589569330215454\n",
            "N: 6 1\n",
            "OP: >\n",
            "VAL: 8.457788467407227\n",
            "N: 6 7\n",
            "OP: >\n",
            "VAL: 8.710090160369873\n",
            "N: 7 4\n",
            "OP: >\n",
            "VAL: 3.306133508682251\n",
            "N: 7 6\n",
            "OP: >\n",
            "VAL: 3.5442665815353394\n",
            "OUTPUT VARS\n",
            "[192 193]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sat_unsat,vals, stats = network_a.solve(options = options)\n",
        "\n",
        "print(\"sat_unsat:\", sat_unsat)\n",
        "print(\"vals:\", vals)\n",
        "print(\"stats:\", stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y91MR0uMk1go",
        "outputId": "766bc4ea-8a3f-4a50-e761-52c9bcbb04d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sat\n",
            "input 0 = 0.5147843903186273\n",
            "input 1 = 0.5255150888480392\n",
            "input 2 = 0.5238530177696078\n",
            "input 3 = 0.5336435355392156\n",
            "input 4 = 0.4821920955882354\n",
            "input 5 = 0.4499789368872549\n",
            "input 6 = 0.444592524509804\n",
            "input 7 = 0.4424343055754672\n",
            "input 8 = 0.4345492493872549\n",
            "input 9 = 0.49626799938725497\n",
            "input 10 = 0.5520890778186274\n",
            "input 11 = 0.5668907015931373\n",
            "input 12 = 0.563558900122549\n",
            "input 13 = 0.5467447916666667\n",
            "input 14 = 0.5383597579656864\n",
            "input 15 = 0.6567523192466\n",
            "input 16 = 0.5230373008578431\n",
            "input 17 = 0.5468979779411766\n",
            "input 18 = 0.5260646446078432\n",
            "input 19 = 0.5069469975490197\n",
            "input 20 = 0.4908624387254903\n",
            "input 21 = 0.4652581232179744\n",
            "input 22 = 0.4384114583333334\n",
            "input 23 = 0.43405330882352955\n",
            "input 24 = 0.4157111672794117\n",
            "input 25 = 0.3700769761029412\n",
            "input 26 = 0.4428768382352942\n",
            "input 27 = 0.44875344669117645\n",
            "input 28 = 0.5243489583333334\n",
            "input 29 = 0.6140548406862746\n",
            "input 30 = 0.6264016544117648\n",
            "input 31 = 0.6244944852941178\n",
            "input 32 = 0.5272901348039216\n",
            "input 33 = 0.5270852481617647\n",
            "input 34 = 0.5165977328431373\n",
            "input 35 = 0.5040058210784315\n",
            "input 36 = 0.4955671721813726\n",
            "input 37 = 0.5415632659313726\n",
            "input 38 = 0.4548330269607843\n",
            "input 39 = 0.44175091911764713\n",
            "input 40 = 0.4903645833333333\n",
            "input 41 = 0.38326439950980395\n",
            "input 42 = 0.36787109374999993\n",
            "input 43 = 0.32492340686274507\n",
            "input 44 = 0.33813189338235294\n",
            "input 45 = 0.36764897365196075\n",
            "input 46 = 0.40614085477941186\n",
            "input 47 = 0.49341873468137254\n",
            "input 48 = 0.514545036764706\n",
            "input 49 = 0.5113587622549021\n",
            "input 50 = 0.5102730545343137\n",
            "input 51 = 0.5016812193627451\n",
            "input 52 = 0.49554227941176465\n",
            "input 53 = 0.4907705269607844\n",
            "input 54 = 0.5263939950980392\n",
            "input 55 = 0.4616038602941177\n",
            "input 56 = 0.448123468137255\n",
            "input 57 = 0.4335094975490196\n",
            "input 58 = 0.3804285386029411\n",
            "input 59 = 0.3316540287990196\n",
            "input 60 = 0.30948223039215683\n",
            "input 61 = 0.26985485600490194\n",
            "input 62 = 0.6006950827205882\n",
            "input 63 = 0.5782647824754901\n",
            "input 64 = 0.5144397212009804\n",
            "input 65 = 0.5060431985294117\n",
            "input 66 = 0.5053462009803921\n",
            "input 67 = 0.4991957720588236\n",
            "input 68 = 0.49605736825980395\n",
            "input 69 = 0.4899299172794118\n",
            "input 70 = 0.47615655637254894\n",
            "input 71 = 0.4739506740196078\n",
            "input 72 = 0.49523973651960784\n",
            "input 73 = 0.43845166973039207\n",
            "input 74 = 0.3517520680147058\n",
            "input 75 = 0.3015069699754902\n",
            "input 76 = 0.28680108762254897\n",
            "input 77 = 0.298452818627451\n",
            "input 78 = 0.621057368259804\n",
            "input 79 = 0.6202742034313726\n",
            "input 80 = 0.5130553002450979\n",
            "input 81 = 0.5105296415441176\n",
            "input 82 = 0.5102922028186274\n",
            "input 83 = 0.5034562653186274\n",
            "input 84 = 0.4977098651960784\n",
            "input 85 = 0.4863530177696078\n",
            "input 86 = 0.48096660539215697\n",
            "input 87 = 0.5144856770833333\n",
            "input 88 = 0.40531556372549016\n",
            "input 89 = 0.4095971200980392\n",
            "input 90 = 0.43187614889705883\n",
            "input 91 = 0.3318991268382353\n",
            "input 92 = 0.28704618566176465\n",
            "input 93 = 0.2684187346813725\n",
            "input 94 = 0.25921032475490197\n",
            "input 95 = 0.6193991268382353\n",
            "input 96 = 0.5174670649509803\n",
            "input 97 = 0.5154067095588235\n",
            "input 98 = 0.5097637101715686\n",
            "input 99 = 0.5093922334558822\n",
            "input 100 = 0.4958180147058825\n",
            "input 101 = 0.4914962469362746\n",
            "input 102 = 0.47283049938725485\n",
            "input 103 = 0.4098403033088235\n",
            "input 104 = 0.4079848345588235\n",
            "input 105 = 0.4043083639705882\n",
            "input 106 = 0.3994542738970588\n",
            "input 107 = 0.29905598958333335\n",
            "input 108 = 0.29145795036764705\n",
            "input 109 = 0.24170304840686274\n",
            "input 110 = 0.232879518995098\n",
            "input 111 = 0.2491613051470588\n",
            "input 112 = 0.5259784773284313\n",
            "input 113 = 0.5187117034313726\n",
            "input 114 = 0.5123200061274509\n",
            "input 115 = 0.505920649509804\n",
            "input 116 = 0.4997146905637255\n",
            "input 117 = 0.4948127297794118\n",
            "input 118 = 0.5252355238970587\n",
            "input 119 = 0.529635799632353\n",
            "input 120 = 0.41754365808823535\n",
            "input 121 = 0.3445063572303922\n",
            "input 122 = 0.3861730238970589\n",
            "input 123 = 0.3804285386029412\n",
            "input 124 = 0.28717830882352935\n",
            "input 125 = 0.2810508578431372\n",
            "input 126 = 0.45268650428921564\n",
            "input 127 = 0.23399203431372545\n",
            "output 0 = 5.987022038467982\n",
            "output 1 = 4.420915880128053\n",
            "sat_unsat: sat\n",
            "vals: {0: 0.5147843903186273, 1: 0.5255150888480392, 2: 0.5238530177696078, 3: 0.5336435355392156, 4: 0.4821920955882354, 5: 0.4499789368872549, 6: 0.444592524509804, 7: 0.4424343055754672, 8: 0.4345492493872549, 9: 0.49626799938725497, 10: 0.5520890778186274, 11: 0.5668907015931373, 12: 0.563558900122549, 13: 0.5467447916666667, 14: 0.5383597579656864, 15: 0.6567523192466, 16: 0.5230373008578431, 17: 0.5468979779411766, 18: 0.5260646446078432, 19: 0.5069469975490197, 20: 0.4908624387254903, 21: 0.4652581232179744, 22: 0.4384114583333334, 23: 0.43405330882352955, 24: 0.4157111672794117, 25: 0.3700769761029412, 26: 0.4428768382352942, 27: 0.44875344669117645, 28: 0.5243489583333334, 29: 0.6140548406862746, 30: 0.6264016544117648, 31: 0.6244944852941178, 32: 0.5272901348039216, 33: 0.5270852481617647, 34: 0.5165977328431373, 35: 0.5040058210784315, 36: 0.4955671721813726, 37: 0.5415632659313726, 38: 0.4548330269607843, 39: 0.44175091911764713, 40: 0.4903645833333333, 41: 0.38326439950980395, 42: 0.36787109374999993, 43: 0.32492340686274507, 44: 0.33813189338235294, 45: 0.36764897365196075, 46: 0.40614085477941186, 47: 0.49341873468137254, 48: 0.514545036764706, 49: 0.5113587622549021, 50: 0.5102730545343137, 51: 0.5016812193627451, 52: 0.49554227941176465, 53: 0.4907705269607844, 54: 0.5263939950980392, 55: 0.4616038602941177, 56: 0.448123468137255, 57: 0.4335094975490196, 58: 0.3804285386029411, 59: 0.3316540287990196, 60: 0.30948223039215683, 61: 0.26985485600490194, 62: 0.6006950827205882, 63: 0.5782647824754901, 64: 0.5144397212009804, 65: 0.5060431985294117, 66: 0.5053462009803921, 67: 0.4991957720588236, 68: 0.49605736825980395, 69: 0.4899299172794118, 70: 0.47615655637254894, 71: 0.4739506740196078, 72: 0.49523973651960784, 73: 0.43845166973039207, 74: 0.3517520680147058, 75: 0.3015069699754902, 76: 0.28680108762254897, 77: 0.298452818627451, 78: 0.621057368259804, 79: 0.6202742034313726, 80: 0.5130553002450979, 81: 0.5105296415441176, 82: 0.5102922028186274, 83: 0.5034562653186274, 84: 0.4977098651960784, 85: 0.4863530177696078, 86: 0.48096660539215697, 87: 0.5144856770833333, 88: 0.40531556372549016, 89: 0.4095971200980392, 90: 0.43187614889705883, 91: 0.3318991268382353, 92: 0.28704618566176465, 93: 0.2684187346813725, 94: 0.25921032475490197, 95: 0.6193991268382353, 96: 0.5174670649509803, 97: 0.5154067095588235, 98: 0.5097637101715686, 99: 0.5093922334558822, 100: 0.4958180147058825, 101: 0.4914962469362746, 102: 0.47283049938725485, 103: 0.4098403033088235, 104: 0.4079848345588235, 105: 0.4043083639705882, 106: 0.3994542738970588, 107: 0.29905598958333335, 108: 0.29145795036764705, 109: 0.24170304840686274, 110: 0.232879518995098, 111: 0.2491613051470588, 112: 0.5259784773284313, 113: 0.5187117034313726, 114: 0.5123200061274509, 115: 0.505920649509804, 116: 0.4997146905637255, 117: 0.4948127297794118, 118: 0.5252355238970587, 119: 0.529635799632353, 120: 0.41754365808823535, 121: 0.3445063572303922, 122: 0.3861730238970589, 123: 0.3804285386029412, 124: 0.28717830882352935, 125: 0.2810508578431372, 126: 0.45268650428921564, 127: 0.23399203431372545, 128: 1.7026255290555237, 129: 0.06380862997679115, 130: -0.4257192972385067, 131: -0.4262072259134206, 132: 2.7371226229372234, 133: 1.7911873752370393, 134: -0.18358311621007956, 135: -0.1740050944842071, 136: 0.6981120931228904, 137: 0.7822357459653931, 138: 3.432545835106488, 139: 1.1939006091969622, 140: 4.586646947146778, 141: 4.581186061030413, 142: -0.29365391564486526, 143: -0.2127177915307471, 144: 1.7026255290555237, 145: 0.0, 146: 2.7371226229372234, 147: 0.0, 148: 0.6981120931228912, 149: 3.432545835106488, 150: 4.586646947146778, 151: 0.0, 152: 0.06380862997678992, 153: 0.0, 154: 1.7911873752370393, 155: 0.0, 156: 0.7822357459653916, 157: 1.193900609196962, 158: 4.581186061030413, 159: 0.0, 160: 2.059767821926803, 161: -2.5726818877765876, 162: -1.191563748414286, 163: 0.658833735325026, 164: 4.145549485013955, 165: 2.6600068082556505, 166: 3.4207853449131664, 167: 6.543625424021027, 168: 2.059767821926815, 169: 0.0, 170: 0.0, 171: 0.6588337353250258, 172: 4.145549485013967, 173: 2.6600068082556403, 174: 3.4207853449131425, 175: 6.543625424021027, 176: -6.6207183532783205, 177: -2.1066874754182527, 178: -6.679742176512316e-16, 179: -3.3096394538879395, 180: -3.019806626980426e-14, 181: -4.391019516906839, 182: 10.4726250749343, 183: 4.308803605705257, 184: 0.0, 185: 0.0, 186: 0.0, 187: 0.0, 188: 0.0, 189: 0.0, 190: 10.4726250749343, 191: 4.308803605705268, 192: 5.987022038467982, 193: 4.420915880128053}\n",
            "stats: <maraboupy.MarabouCore.Statistics object at 0x7bb4a39139f0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROVE RULE FOR SAT OF SAFETY PROPERTY |he| <= 8.03**"
      ],
      "metadata": {
        "id": "35c5ILxqv1nL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from maraboupy import Marabou\n",
        "from maraboupy.MarabouCore import *\n",
        "from maraboupy.MarabouPythonic import *\n",
        "\n",
        "options = Marabou.createOptions(verbosity = 1, numWorkers=1, numBlasThreads=1,snc=True)\n",
        "\n",
        "filename = \"/content/ProphecyPlus/dataset_models/kj_tiny_taxinet/KJ_TinyTaxiNet.onnx\"\n",
        "network_a = Marabou.read_onnx(filename)\n",
        "\n",
        "\n",
        "print(\"INPUT VARS\")\n",
        "invars = network_a.inputVars[0][0].flatten()\n",
        "\n",
        "\n",
        "\n",
        "for indx in range(0,len(invars)):\n",
        "    i = invars[indx]\n",
        "    v = Var(i)\n",
        "    network_a.setLowerBound(i,x_train_minhe[i])\n",
        "    network_a.setUpperBound(i,x_train_maxhe[i])\n",
        "    #network_a.setLowerBound(i,inp_he_ex[indx])\n",
        "    #network_a.setUpperBound(i,inp_he_ex[indx])\n",
        "\n",
        "\n",
        "print(\"LAYER VARS MAP\")\n",
        "print(network_a.layerNameToVariables)\n",
        "\n",
        "dense_10_neurons = network_a.layerNameToVariables[\"dense_1/BiasAdd:0\"][0]\n",
        "print(np.shape(dense_10_neurons))\n",
        "print(len(dense_10_neurons))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for indx in range(0, len(dense_10_neurons)):\n",
        "    neuron_indx = dense_10_neurons[indx] - dense_10_neurons[0]\n",
        "    network_a.setLowerBound(dense_10_neurons[indx], fngprnt_min_he[neuron_indx])\n",
        "    network_a.setUpperBound(dense_10_neurons[indx], fngprnt_max_he[neuron_indx])\n",
        "    #network_a.setLowerBound(dense_10_neurons[indx], finger_he_ex[neuron_indx] - 1.0)\n",
        "    #network_a.setUpperBound(dense_10_neurons[indx], finger_he_ex[neuron_indx] + 1.0)\n",
        "\n",
        "    if (neuron_indx in rule_neurons_list_he):\n",
        "      v = Var(dense_10_neurons[indx])\n",
        "\n",
        "      for indx in range(0, len(rule_neurons_list_he)):\n",
        "        if (rule_neurons_list_he[indx] == neuron_indx):\n",
        "          index = indx\n",
        "          sig_indx = (2*index) + 1\n",
        "          print(\"N:\", neuron_indx, index)\n",
        "          print(\"OP:\", rule_sig_list_he[sig_indx-1])\n",
        "          print(\"VAL:\", rule_sig_list_he[sig_indx])\n",
        "          if (rule_sig_list_he[sig_indx-1] == '<='):\n",
        "            network_a.addConstraint(float(rule_sig_list_he[sig_indx]) >= v)\n",
        "          else:\n",
        "            network_a.addConstraint(v >= float(rule_sig_list_he[sig_indx]))\n",
        "\n",
        "\n",
        "\n",
        "print(\"OUTPUT VARS\")\n",
        "outvars = network_a.outputVars[0].flatten()\n",
        "print(outvars)\n",
        "label_var = Var(outvars[1])\n",
        "\n",
        "# |he| >= 8.03 should be UNSAT\n",
        "# CONSTRAINT 1\n",
        "#network_a.addConstraint(label_var >= 8.03)\n",
        "# CONSTRAINT 2\n",
        "network_a.addConstraint(-8.0 >= label_var)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gK_jn6ealZg",
        "outputId": "bf2b7b1b-f5db-4cf9-84b6-492b9be23508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT VARS\n",
            "LAYER VARS MAP\n",
            "{'X_place:0': array([[[[  0],\n",
            "         [  1],\n",
            "         [  2],\n",
            "         [  3],\n",
            "         [  4],\n",
            "         [  5],\n",
            "         [  6],\n",
            "         [  7],\n",
            "         [  8],\n",
            "         [  9],\n",
            "         [ 10],\n",
            "         [ 11],\n",
            "         [ 12],\n",
            "         [ 13],\n",
            "         [ 14],\n",
            "         [ 15]],\n",
            "\n",
            "        [[ 16],\n",
            "         [ 17],\n",
            "         [ 18],\n",
            "         [ 19],\n",
            "         [ 20],\n",
            "         [ 21],\n",
            "         [ 22],\n",
            "         [ 23],\n",
            "         [ 24],\n",
            "         [ 25],\n",
            "         [ 26],\n",
            "         [ 27],\n",
            "         [ 28],\n",
            "         [ 29],\n",
            "         [ 30],\n",
            "         [ 31]],\n",
            "\n",
            "        [[ 32],\n",
            "         [ 33],\n",
            "         [ 34],\n",
            "         [ 35],\n",
            "         [ 36],\n",
            "         [ 37],\n",
            "         [ 38],\n",
            "         [ 39],\n",
            "         [ 40],\n",
            "         [ 41],\n",
            "         [ 42],\n",
            "         [ 43],\n",
            "         [ 44],\n",
            "         [ 45],\n",
            "         [ 46],\n",
            "         [ 47]],\n",
            "\n",
            "        [[ 48],\n",
            "         [ 49],\n",
            "         [ 50],\n",
            "         [ 51],\n",
            "         [ 52],\n",
            "         [ 53],\n",
            "         [ 54],\n",
            "         [ 55],\n",
            "         [ 56],\n",
            "         [ 57],\n",
            "         [ 58],\n",
            "         [ 59],\n",
            "         [ 60],\n",
            "         [ 61],\n",
            "         [ 62],\n",
            "         [ 63]],\n",
            "\n",
            "        [[ 64],\n",
            "         [ 65],\n",
            "         [ 66],\n",
            "         [ 67],\n",
            "         [ 68],\n",
            "         [ 69],\n",
            "         [ 70],\n",
            "         [ 71],\n",
            "         [ 72],\n",
            "         [ 73],\n",
            "         [ 74],\n",
            "         [ 75],\n",
            "         [ 76],\n",
            "         [ 77],\n",
            "         [ 78],\n",
            "         [ 79]],\n",
            "\n",
            "        [[ 80],\n",
            "         [ 81],\n",
            "         [ 82],\n",
            "         [ 83],\n",
            "         [ 84],\n",
            "         [ 85],\n",
            "         [ 86],\n",
            "         [ 87],\n",
            "         [ 88],\n",
            "         [ 89],\n",
            "         [ 90],\n",
            "         [ 91],\n",
            "         [ 92],\n",
            "         [ 93],\n",
            "         [ 94],\n",
            "         [ 95]],\n",
            "\n",
            "        [[ 96],\n",
            "         [ 97],\n",
            "         [ 98],\n",
            "         [ 99],\n",
            "         [100],\n",
            "         [101],\n",
            "         [102],\n",
            "         [103],\n",
            "         [104],\n",
            "         [105],\n",
            "         [106],\n",
            "         [107],\n",
            "         [108],\n",
            "         [109],\n",
            "         [110],\n",
            "         [111]],\n",
            "\n",
            "        [[112],\n",
            "         [113],\n",
            "         [114],\n",
            "         [115],\n",
            "         [116],\n",
            "         [117],\n",
            "         [118],\n",
            "         [119],\n",
            "         [120],\n",
            "         [121],\n",
            "         [122],\n",
            "         [123],\n",
            "         [124],\n",
            "         [125],\n",
            "         [126],\n",
            "         [127]]]]), 'conv2d/Conv2D__6:0': array([[[[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,\n",
            "           12,  13,  14,  15],\n",
            "         [ 16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "           28,  29,  30,  31],\n",
            "         [ 32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,\n",
            "           44,  45,  46,  47],\n",
            "         [ 48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,\n",
            "           60,  61,  62,  63],\n",
            "         [ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,\n",
            "           76,  77,  78,  79],\n",
            "         [ 80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
            "           92,  93,  94,  95],\n",
            "         [ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
            "          108, 109, 110, 111],\n",
            "         [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
            "          124, 125, 126, 127]]]]), 'conv2d/Conv2D:0': array([[[[128, 129]],\n",
            "\n",
            "        [[130, 131]],\n",
            "\n",
            "        [[132, 133]],\n",
            "\n",
            "        [[134, 135]],\n",
            "\n",
            "        [[136, 137]],\n",
            "\n",
            "        [[138, 139]],\n",
            "\n",
            "        [[140, 141]],\n",
            "\n",
            "        [[142, 143]]]]), 'conv2d/Conv2D__8:0': array([[[[128, 130, 132, 134, 136, 138, 140, 142],\n",
            "         [129, 131, 133, 135, 137, 139, 141, 143]]]]), 'conv2d/BiasAdd:0': array([[[[128, 130, 132, 134, 136, 138, 140, 142],\n",
            "         [129, 131, 133, 135, 137, 139, 141, 143]]]]), 'relu:0': array([[[[144, 145, 146, 147, 148, 149, 150, 151],\n",
            "         [152, 153, 154, 155, 156, 157, 158, 159]]]]), 'Reshape:0': array([[144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
            "        157, 158, 159]]), 'dense/MatMul:0': array([[160, 161, 162, 163, 164, 165, 166, 167]]), 'dense/BiasAdd:0': array([[160, 161, 162, 163, 164, 165, 166, 167]]), 'relu_1:0': array([[168, 169, 170, 171, 172, 173, 174, 175]]), 'dense_1/MatMul:0': array([[176, 177, 178, 179, 180, 181, 182, 183]]), 'dense_1/BiasAdd:0': array([[176, 177, 178, 179, 180, 181, 182, 183]]), 'relu_2:0': array([[184, 185, 186, 187, 188, 189, 190, 191]]), 'dense_2/MatMul:0': array([[192, 193]]), 'dense_2/BiasAdd_raw_output___3:0': array([[192, 193]]), 'dense_2/BiasAdd:0': array([[192, 193]])}\n",
            "(8,)\n",
            "8\n",
            "N: 1 7\n",
            "OP: <=\n",
            "VAL: -0.8676009178161621\n",
            "N: 1 8\n",
            "OP: >\n",
            "VAL: -1.7780373096466064\n",
            "N: 2 0\n",
            "OP: <=\n",
            "VAL: 5.0200934410095215\n",
            "N: 2 12\n",
            "OP: <=\n",
            "VAL: 4.339860677719116\n",
            "N: 3 4\n",
            "OP: >\n",
            "VAL: -4.647291898727417\n",
            "N: 4 2\n",
            "OP: <=\n",
            "VAL: 0.7042066156864166\n",
            "N: 4 11\n",
            "OP: <=\n",
            "VAL: 0.6818224787712097\n",
            "N: 6 6\n",
            "OP: >\n",
            "VAL: 0.23533541709184647\n",
            "N: 7 1\n",
            "OP: <=\n",
            "VAL: 5.228757619857788\n",
            "N: 7 3\n",
            "OP: >\n",
            "VAL: 2.3984490633010864\n",
            "N: 7 5\n",
            "OP: >\n",
            "VAL: 2.8580180406570435\n",
            "N: 7 9\n",
            "OP: <=\n",
            "VAL: 5.001114130020142\n",
            "N: 7 10\n",
            "OP: >\n",
            "VAL: 3.032026529312134\n",
            "N: 7 13\n",
            "OP: >\n",
            "VAL: 3.0972129106521606\n",
            "N: 7 14\n",
            "OP: <=\n",
            "VAL: 4.9778783321380615\n",
            "OUTPUT VARS\n",
            "[192 193]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(network_a.getInputQuery())\n",
        "sat_unsat,vals, stats = network_a.solve(options = options)\n",
        "\n",
        "print(\"sat_unsat:\", sat_unsat)\n",
        "print(\"vals:\", vals)\n",
        "print(\"stats:\", stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdpjifFXcC6r",
        "outputId": "83d27dd6-c7d9-4055-ef90-7fbc6670254a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sat\n",
            "input 0 = 0.5766505821078431\n",
            "input 1 = 0.5717294730392156\n",
            "input 2 = 0.5354568780637257\n",
            "input 3 = 0.5357211243872549\n",
            "input 4 = 0.5533413756127451\n",
            "input 5 = 0.5349858302696079\n",
            "input 6 = 0.4996419270833333\n",
            "input 7 = 0.4895349274293377\n",
            "input 8 = 0.534704350490196\n",
            "input 9 = 0.6067497702205883\n",
            "input 10 = 0.5388576133578432\n",
            "input 11 = 0.5346909466911766\n",
            "input 12 = 0.5418485753676471\n",
            "input 13 = 0.5326803768382353\n",
            "input 14 = 0.5296894148284315\n",
            "input 15 = 0.526849724264706\n",
            "input 16 = 0.44483379289215685\n",
            "input 17 = 0.4352309283088235\n",
            "input 18 = 0.3614066329656863\n",
            "input 19 = 0.5987113204656863\n",
            "input 20 = 0.5507678462009804\n",
            "input 21 = 0.4415536917892158\n",
            "input 22 = 0.4626321231617647\n",
            "input 23 = 0.47709290747549027\n",
            "input 24 = 0.49253408394607845\n",
            "input 25 = 0.4963957291806721\n",
            "input 26 = 0.6368987438725491\n",
            "input 27 = 0.6988568474264707\n",
            "input 28 = 0.5648380055147059\n",
            "input 29 = 0.5348862591911765\n",
            "input 30 = 0.5324352787990196\n",
            "input 31 = 0.5302293964460785\n",
            "input 32 = 0.5466509650735294\n",
            "input 33 = 0.4594458486519608\n",
            "input 34 = 0.3785768995098039\n",
            "input 35 = 0.39316980698529413\n",
            "input 36 = 0.41415249693627454\n",
            "input 37 = 0.4348594515931372\n",
            "input 38 = 0.45130782781862755\n",
            "input 39 = 0.4656958486519609\n",
            "input 40 = 0.46743451286764715\n",
            "input 41 = 0.476125919117647\n",
            "input 42 = 0.48924823835784315\n",
            "input 43 = 0.4977749693627451\n",
            "input 44 = 0.6648399203431373\n",
            "input 45 = 0.6927810968137255\n",
            "input 46 = 0.5998372395833333\n",
            "input 47 = 0.5870040594362745\n",
            "input 48 = 0.36157513786764706\n",
            "input 49 = 0.381182981004902\n",
            "input 50 = 0.40025467218137256\n",
            "input 51 = 0.4155752144607844\n",
            "input 52 = 0.42759842218137256\n",
            "input 53 = 0.44852366727941184\n",
            "input 54 = 0.4768784466911766\n",
            "input 55 = 0.4679247089460785\n",
            "input 56 = 0.4775333180147059\n",
            "input 57 = 0.4793926164215686\n",
            "input 58 = 0.48715724571078434\n",
            "input 59 = 0.49573376225490184\n",
            "input 60 = 0.49841835171568627\n",
            "input 61 = 0.5014514399509804\n",
            "input 62 = 0.529651118259804\n",
            "input 63 = 0.6851964613970588\n",
            "input 64 = 0.38616727941176465\n",
            "input 65 = 0.40598000919117644\n",
            "input 66 = 0.41388825061274526\n",
            "input 67 = 0.42784160539215677\n",
            "input 68 = 0.4401118259803921\n",
            "input 69 = 0.4538583792892158\n",
            "input 70 = 0.46406824448529416\n",
            "input 71 = 0.47491766237745114\n",
            "input 72 = 0.4715494791666667\n",
            "input 73 = 0.48037300857843135\n",
            "input 74 = 0.4844592524509803\n",
            "input 75 = 0.483660768995098\n",
            "input 76 = 0.4960726868872549\n",
            "input 77 = 0.5003274356617647\n",
            "input 78 = 0.49969171262254897\n",
            "input 79 = 0.5043141084558823\n",
            "input 80 = 0.4087162990196078\n",
            "input 81 = 0.41529373468137254\n",
            "input 82 = 0.42614123774509804\n",
            "input 83 = 0.4376378676470587\n",
            "input 84 = 0.44788985906862755\n",
            "input 85 = 0.4603496476715687\n",
            "input 86 = 0.46682751225490204\n",
            "input 87 = 0.4760359221813726\n",
            "input 88 = 0.47851371017156863\n",
            "input 89 = 0.4812729779411764\n",
            "input 90 = 0.48355928308823526\n",
            "input 91 = 0.49182751225490196\n",
            "input 92 = 0.48895143995098034\n",
            "input 93 = 0.49648628982843146\n",
            "input 94 = 0.500390625\n",
            "input 95 = 0.5020240616262487\n",
            "input 96 = 0.4206858915441176\n",
            "input 97 = 0.431527650122549\n",
            "input 98 = 0.4401118259803921\n",
            "input 99 = 0.44592907475490207\n",
            "input 100 = 0.45920075061274523\n",
            "input 101 = 0.4597981770833335\n",
            "input 102 = 0.4739372702205884\n",
            "input 103 = 0.47244944852941173\n",
            "input 104 = 0.47733800551470595\n",
            "input 105 = 0.48169998468137254\n",
            "input 106 = 0.4853898590686274\n",
            "input 107 = 0.48661534926470584\n",
            "input 108 = 0.4930300245098038\n",
            "input 109 = 0.4805472579656863\n",
            "input 110 = 0.4945178462009804\n",
            "input 111 = 0.49096392463235294\n",
            "input 112 = 0.42123927696078434\n",
            "input 113 = 0.44523016237745094\n",
            "input 114 = 0.44991000306372564\n",
            "input 115 = 0.4560202205882353\n",
            "input 116 = 0.45940946691176476\n",
            "input 117 = 0.4625957414215687\n",
            "input 118 = 0.47540594362745103\n",
            "input 119 = 0.48053002450980387\n",
            "input 120 = 0.4795496323529411\n",
            "input 121 = 0.4833141850490196\n",
            "input 122 = 0.47881050857843144\n",
            "input 123 = 0.48404947916666663\n",
            "input 124 = 0.49037607230392155\n",
            "input 125 = 0.4930530024509804\n",
            "input 126 = 0.4839058670343137\n",
            "input 127 = 0.4904737285539216\n",
            "output 0 = -3.1394235984346555\n",
            "output 1 = -8.0\n",
            "sat_unsat: sat\n",
            "vals: {0: 0.5766505821078431, 1: 0.5717294730392156, 2: 0.5354568780637257, 3: 0.5357211243872549, 4: 0.5533413756127451, 5: 0.5349858302696079, 6: 0.4996419270833333, 7: 0.4895349274293377, 8: 0.534704350490196, 9: 0.6067497702205883, 10: 0.5388576133578432, 11: 0.5346909466911766, 12: 0.5418485753676471, 13: 0.5326803768382353, 14: 0.5296894148284315, 15: 0.526849724264706, 16: 0.44483379289215685, 17: 0.4352309283088235, 18: 0.3614066329656863, 19: 0.5987113204656863, 20: 0.5507678462009804, 21: 0.4415536917892158, 22: 0.4626321231617647, 23: 0.47709290747549027, 24: 0.49253408394607845, 25: 0.4963957291806721, 26: 0.6368987438725491, 27: 0.6988568474264707, 28: 0.5648380055147059, 29: 0.5348862591911765, 30: 0.5324352787990196, 31: 0.5302293964460785, 32: 0.5466509650735294, 33: 0.4594458486519608, 34: 0.3785768995098039, 35: 0.39316980698529413, 36: 0.41415249693627454, 37: 0.4348594515931372, 38: 0.45130782781862755, 39: 0.4656958486519609, 40: 0.46743451286764715, 41: 0.476125919117647, 42: 0.48924823835784315, 43: 0.4977749693627451, 44: 0.6648399203431373, 45: 0.6927810968137255, 46: 0.5998372395833333, 47: 0.5870040594362745, 48: 0.36157513786764706, 49: 0.381182981004902, 50: 0.40025467218137256, 51: 0.4155752144607844, 52: 0.42759842218137256, 53: 0.44852366727941184, 54: 0.4768784466911766, 55: 0.4679247089460785, 56: 0.4775333180147059, 57: 0.4793926164215686, 58: 0.48715724571078434, 59: 0.49573376225490184, 60: 0.49841835171568627, 61: 0.5014514399509804, 62: 0.529651118259804, 63: 0.6851964613970588, 64: 0.38616727941176465, 65: 0.40598000919117644, 66: 0.41388825061274526, 67: 0.42784160539215677, 68: 0.4401118259803921, 69: 0.4538583792892158, 70: 0.46406824448529416, 71: 0.47491766237745114, 72: 0.4715494791666667, 73: 0.48037300857843135, 74: 0.4844592524509803, 75: 0.483660768995098, 76: 0.4960726868872549, 77: 0.5003274356617647, 78: 0.49969171262254897, 79: 0.5043141084558823, 80: 0.4087162990196078, 81: 0.41529373468137254, 82: 0.42614123774509804, 83: 0.4376378676470587, 84: 0.44788985906862755, 85: 0.4603496476715687, 86: 0.46682751225490204, 87: 0.4760359221813726, 88: 0.47851371017156863, 89: 0.4812729779411764, 90: 0.48355928308823526, 91: 0.49182751225490196, 92: 0.48895143995098034, 93: 0.49648628982843146, 94: 0.500390625, 95: 0.5020240616262487, 96: 0.4206858915441176, 97: 0.431527650122549, 98: 0.4401118259803921, 99: 0.44592907475490207, 100: 0.45920075061274523, 101: 0.4597981770833335, 102: 0.4739372702205884, 103: 0.47244944852941173, 104: 0.47733800551470595, 105: 0.48169998468137254, 106: 0.4853898590686274, 107: 0.48661534926470584, 108: 0.4930300245098038, 109: 0.4805472579656863, 110: 0.4945178462009804, 111: 0.49096392463235294, 112: 0.42123927696078434, 113: 0.44523016237745094, 114: 0.44991000306372564, 115: 0.4560202205882353, 116: 0.45940946691176476, 117: 0.4625957414215687, 118: 0.47540594362745103, 119: 0.48053002450980387, 120: 0.4795496323529411, 121: 0.4833141850490196, 122: 0.47881050857843144, 123: 0.48404947916666663, 124: 0.49037607230392155, 125: 0.4930530024509804, 126: 0.4839058670343137, 127: 0.4904737285539216, 128: 0.791663094930653, 129: 0.8589810763854724, 130: -0.4304928285711829, 131: -0.46039695121453766, 132: 2.4037974122132693, 133: 2.850714734430481, 134: -0.1741438576915264, 135: -0.17137958520956567, 136: 0.6151743416709468, 137: 0.6482075654649616, 138: 2.4215631811691964, 139: 2.7036582566560368, 140: 4.3143315389404595, 141: 5.0228458455838645, 142: -0.2436407299400586, 143: -0.26077320401201615, 144: 0.791663094930653, 145: 0.0, 146: 2.4037974122132693, 147: 0.0, 148: 0.6151743416709468, 149: 2.4215631811691964, 150: 4.3143315389404595, 151: 0.0, 152: 0.8589810763854724, 153: 0.0, 154: 2.850714734430481, 155: 0.0, 156: 0.6482075654649616, 157: 2.7036582566560368, 158: 5.0228458455838645, 159: 0.0, 160: -2.2360303663764647, 161: -2.3333885058289914, 162: -2.642404637794404, 163: 3.437481074034986, 164: 0.9832867090858537, 165: -7.66053886991358e-15, 166: 6.002752798056827, 167: 4.660449162208533, 168: 0.0, 169: 0.0, 170: 0.0, 171: 3.437481074034986, 172: 0.9832867090858542, 173: 0.0, 174: 6.002752798056827, 175: 4.660449162208533, 176: -7.527507072919999, 177: -1.0721250899780006, 178: 3.5560617257786475, 179: -4.447291898727417, 180: -5.7842496080465295, 181: 4.742495087871088, 182: 2.3542674177125447, 183: 4.55684014022818, 184: 0.0, 185: 0.0, 186: 3.5560617257786182, 187: 0.0, 188: 0.0, 189: 4.742495087871088, 190: 2.3542674177125447, 191: 4.55684014022828, 192: -3.1394235984346555, 193: -8.0}\n",
            "stats: <maraboupy.MarabouCore.Statistics object at 0x7bb4ae152cf0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROVE RULE FOR VIOLATION OF SAFETY PROPERTY |he| <= 8.03**"
      ],
      "metadata": {
        "id": "Ve6Prb1pvnWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from maraboupy import Marabou\n",
        "from maraboupy.MarabouCore import *\n",
        "from maraboupy.MarabouPythonic import *\n",
        "\n",
        "options = Marabou.createOptions(verbosity = 1, numWorkers=1, numBlasThreads=1,snc=True)\n",
        "\n",
        "filename = \"/content/ProphecyPlus/dataset_models/kj_tiny_taxinet/KJ_TinyTaxiNet.onnx\"\n",
        "network_a = Marabou.read_onnx(filename)\n",
        "\n",
        "\n",
        "print(\"INPUT VARS\")\n",
        "invars = network_a.inputVars[0][0].flatten()\n",
        "\n",
        "\n",
        "\n",
        "for indx in range(0,len(invars)):\n",
        "    i = invars[indx]\n",
        "    v = Var(i)\n",
        "    network_a.setLowerBound(i,x_train_minhe0[i])\n",
        "    network_a.setUpperBound(i,x_train_maxhe0[i])\n",
        "    #network_a.setLowerBound(i,inp_he_ex0[indx])\n",
        "    #network_a.setUpperBound(i,inp_he_ex0[indx])\n",
        "\n",
        "\n",
        "print(\"LAYER VARS MAP\")\n",
        "print(network_a.layerNameToVariables)\n",
        "\n",
        "dense_10_neurons = network_a.layerNameToVariables[\"dense_1/BiasAdd:0\"][0]\n",
        "print(np.shape(dense_10_neurons))\n",
        "print(len(dense_10_neurons))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for indx in range(0, len(dense_10_neurons)):\n",
        "    neuron_indx = dense_10_neurons[indx] - dense_10_neurons[0]\n",
        "    network_a.setLowerBound(dense_10_neurons[indx], fngprnt_min_he0[neuron_indx])\n",
        "    network_a.setUpperBound(dense_10_neurons[indx], fngprnt_max_he0[neuron_indx])\n",
        "    #network_a.setLowerBound(dense_10_neurons[indx], finger_he_ex0[neuron_indx] - 1.0)\n",
        "    #network_a.setUpperBound(dense_10_neurons[indx], finger_he_ex0[neuron_indx] + 1.0)\n",
        "\n",
        "    if (neuron_indx in rule_neurons_list_he0):\n",
        "      v = Var(dense_10_neurons[indx])\n",
        "\n",
        "      for indx in range(0, len(rule_neurons_list_he0)):\n",
        "        if (rule_neurons_list_he0[indx] == neuron_indx):\n",
        "          index = indx\n",
        "          sig_indx = (2*index) + 1\n",
        "          print(\"N:\", neuron_indx, index)\n",
        "          print(\"OP:\", rule_sig_list_he0[sig_indx-1])\n",
        "          print(\"VAL:\", rule_sig_list_he0[sig_indx])\n",
        "          if (rule_sig_list_he0[sig_indx-1] == '<='):\n",
        "            network_a.addConstraint(float(rule_sig_list_he0[sig_indx]) >= v)\n",
        "          else:\n",
        "            network_a.addConstraint(v >= float(rule_sig_list_he0[sig_indx]))\n",
        "\n",
        "\n",
        "\n",
        "print(\"OUTPUT VARS\")\n",
        "outvars = network_a.outputVars[0].flatten()\n",
        "print(outvars)\n",
        "label_var = Var(outvars[1])\n",
        "\n",
        "# |he| < 8.03 should be UNSAT\n",
        "network_a.addConstraint(8.03 >= label_var)\n",
        "network_a.addConstraint(label_var >= -8.03)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BcSFiqNu1Bd",
        "outputId": "6d3f3d32-53d5-4c39-9381-4e242032c72a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT VARS\n",
            "LAYER VARS MAP\n",
            "{'X_place:0': array([[[[  0],\n",
            "         [  1],\n",
            "         [  2],\n",
            "         [  3],\n",
            "         [  4],\n",
            "         [  5],\n",
            "         [  6],\n",
            "         [  7],\n",
            "         [  8],\n",
            "         [  9],\n",
            "         [ 10],\n",
            "         [ 11],\n",
            "         [ 12],\n",
            "         [ 13],\n",
            "         [ 14],\n",
            "         [ 15]],\n",
            "\n",
            "        [[ 16],\n",
            "         [ 17],\n",
            "         [ 18],\n",
            "         [ 19],\n",
            "         [ 20],\n",
            "         [ 21],\n",
            "         [ 22],\n",
            "         [ 23],\n",
            "         [ 24],\n",
            "         [ 25],\n",
            "         [ 26],\n",
            "         [ 27],\n",
            "         [ 28],\n",
            "         [ 29],\n",
            "         [ 30],\n",
            "         [ 31]],\n",
            "\n",
            "        [[ 32],\n",
            "         [ 33],\n",
            "         [ 34],\n",
            "         [ 35],\n",
            "         [ 36],\n",
            "         [ 37],\n",
            "         [ 38],\n",
            "         [ 39],\n",
            "         [ 40],\n",
            "         [ 41],\n",
            "         [ 42],\n",
            "         [ 43],\n",
            "         [ 44],\n",
            "         [ 45],\n",
            "         [ 46],\n",
            "         [ 47]],\n",
            "\n",
            "        [[ 48],\n",
            "         [ 49],\n",
            "         [ 50],\n",
            "         [ 51],\n",
            "         [ 52],\n",
            "         [ 53],\n",
            "         [ 54],\n",
            "         [ 55],\n",
            "         [ 56],\n",
            "         [ 57],\n",
            "         [ 58],\n",
            "         [ 59],\n",
            "         [ 60],\n",
            "         [ 61],\n",
            "         [ 62],\n",
            "         [ 63]],\n",
            "\n",
            "        [[ 64],\n",
            "         [ 65],\n",
            "         [ 66],\n",
            "         [ 67],\n",
            "         [ 68],\n",
            "         [ 69],\n",
            "         [ 70],\n",
            "         [ 71],\n",
            "         [ 72],\n",
            "         [ 73],\n",
            "         [ 74],\n",
            "         [ 75],\n",
            "         [ 76],\n",
            "         [ 77],\n",
            "         [ 78],\n",
            "         [ 79]],\n",
            "\n",
            "        [[ 80],\n",
            "         [ 81],\n",
            "         [ 82],\n",
            "         [ 83],\n",
            "         [ 84],\n",
            "         [ 85],\n",
            "         [ 86],\n",
            "         [ 87],\n",
            "         [ 88],\n",
            "         [ 89],\n",
            "         [ 90],\n",
            "         [ 91],\n",
            "         [ 92],\n",
            "         [ 93],\n",
            "         [ 94],\n",
            "         [ 95]],\n",
            "\n",
            "        [[ 96],\n",
            "         [ 97],\n",
            "         [ 98],\n",
            "         [ 99],\n",
            "         [100],\n",
            "         [101],\n",
            "         [102],\n",
            "         [103],\n",
            "         [104],\n",
            "         [105],\n",
            "         [106],\n",
            "         [107],\n",
            "         [108],\n",
            "         [109],\n",
            "         [110],\n",
            "         [111]],\n",
            "\n",
            "        [[112],\n",
            "         [113],\n",
            "         [114],\n",
            "         [115],\n",
            "         [116],\n",
            "         [117],\n",
            "         [118],\n",
            "         [119],\n",
            "         [120],\n",
            "         [121],\n",
            "         [122],\n",
            "         [123],\n",
            "         [124],\n",
            "         [125],\n",
            "         [126],\n",
            "         [127]]]]), 'conv2d/Conv2D__6:0': array([[[[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,\n",
            "           12,  13,  14,  15],\n",
            "         [ 16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "           28,  29,  30,  31],\n",
            "         [ 32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,\n",
            "           44,  45,  46,  47],\n",
            "         [ 48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,\n",
            "           60,  61,  62,  63],\n",
            "         [ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,\n",
            "           76,  77,  78,  79],\n",
            "         [ 80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
            "           92,  93,  94,  95],\n",
            "         [ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
            "          108, 109, 110, 111],\n",
            "         [112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
            "          124, 125, 126, 127]]]]), 'conv2d/Conv2D:0': array([[[[128, 129]],\n",
            "\n",
            "        [[130, 131]],\n",
            "\n",
            "        [[132, 133]],\n",
            "\n",
            "        [[134, 135]],\n",
            "\n",
            "        [[136, 137]],\n",
            "\n",
            "        [[138, 139]],\n",
            "\n",
            "        [[140, 141]],\n",
            "\n",
            "        [[142, 143]]]]), 'conv2d/Conv2D__8:0': array([[[[128, 130, 132, 134, 136, 138, 140, 142],\n",
            "         [129, 131, 133, 135, 137, 139, 141, 143]]]]), 'conv2d/BiasAdd:0': array([[[[128, 130, 132, 134, 136, 138, 140, 142],\n",
            "         [129, 131, 133, 135, 137, 139, 141, 143]]]]), 'relu:0': array([[[[144, 145, 146, 147, 148, 149, 150, 151],\n",
            "         [152, 153, 154, 155, 156, 157, 158, 159]]]]), 'Reshape:0': array([[144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
            "        157, 158, 159]]), 'dense/MatMul:0': array([[160, 161, 162, 163, 164, 165, 166, 167]]), 'dense/BiasAdd:0': array([[160, 161, 162, 163, 164, 165, 166, 167]]), 'relu_1:0': array([[168, 169, 170, 171, 172, 173, 174, 175]]), 'dense_1/MatMul:0': array([[176, 177, 178, 179, 180, 181, 182, 183]]), 'dense_1/BiasAdd:0': array([[176, 177, 178, 179, 180, 181, 182, 183]]), 'relu_2:0': array([[184, 185, 186, 187, 188, 189, 190, 191]]), 'dense_2/MatMul:0': array([[192, 193]]), 'dense_2/BiasAdd_raw_output___3:0': array([[192, 193]]), 'dense_2/BiasAdd:0': array([[192, 193]])}\n",
            "(8,)\n",
            "8\n",
            "N: 1 4\n",
            "OP: <=\n",
            "VAL: -1.4256365299224854\n",
            "N: 1 5\n",
            "OP: <=\n",
            "VAL: -1.727737545967102\n",
            "N: 2 0\n",
            "OP: <=\n",
            "VAL: 5.0200934410095215\n",
            "N: 4 2\n",
            "OP: >\n",
            "VAL: 0.7042066156864166\n",
            "N: 5 6\n",
            "OP: >\n",
            "VAL: -3.598828911781311\n",
            "N: 7 1\n",
            "OP: <=\n",
            "VAL: 5.228757619857788\n",
            "N: 7 3\n",
            "OP: <=\n",
            "VAL: 3.8628121614456177\n",
            "OUTPUT VARS\n",
            "[192 193]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(network_a.getInputQuery())\n",
        "sat_unsat,vals, stats = network_a.solve(options = options)\n",
        "\n",
        "print(\"sat_unsat:\", sat_unsat)\n",
        "print(\"vals:\", vals)\n",
        "print(\"stats:\", stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz8ilFWuvUgN",
        "outputId": "79a50582-060d-4ec5-a03d-f0a6d90ea538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sat\n",
            "input 0 = 0.5849207261029412\n",
            "input 1 = 0.6312442555147059\n",
            "input 2 = 0.6361883425245098\n",
            "input 3 = 0.6180319393382353\n",
            "input 4 = 0.6040268841911764\n",
            "input 5 = 0.5852079503676471\n",
            "input 6 = 0.5290383731617647\n",
            "input 7 = 0.6033643535539216\n",
            "input 8 = 0.5032915900735295\n",
            "input 9 = 0.5796875\n",
            "input 10 = 0.6214403339460784\n",
            "input 11 = 0.6454829197303922\n",
            "input 12 = 0.6614506740196078\n",
            "input 13 = 0.584734987745098\n",
            "input 14 = 0.5844305300245098\n",
            "input 15 = 0.5832643995098039\n",
            "input 16 = 0.6589824601715686\n",
            "input 17 = 0.6055089613970588\n",
            "input 18 = 0.5273648131127451\n",
            "input 19 = 0.5133750765931373\n",
            "input 20 = 0.49006778492647063\n",
            "input 21 = 0.45060700061274517\n",
            "input 22 = 0.5366182675823071\n",
            "input 23 = 0.6370404411764706\n",
            "input 24 = 0.4127814797794117\n",
            "input 25 = 0.4588541666666667\n",
            "input 26 = 0.48973651960784315\n",
            "input 27 = 0.5083639705882353\n",
            "input 28 = 0.5442606927367828\n",
            "input 29 = 0.7048330269607843\n",
            "input 30 = 0.676629518995098\n",
            "input 31 = 0.6669730392156862\n",
            "input 32 = 0.5002814797794118\n",
            "input 33 = 0.5118815104166667\n",
            "input 34 = 0.4969535079656864\n",
            "input 35 = 0.463656556372549\n",
            "input 36 = 0.43858570772058825\n",
            "input 37 = 0.5960094975490196\n",
            "input 38 = 0.6403358609068628\n",
            "input 39 = 0.4089996936274509\n",
            "input 40 = 0.4293428308823529\n",
            "input 41 = 0.4548330269607843\n",
            "input 42 = 0.4732153799019607\n",
            "input 43 = 0.49649969362745094\n",
            "input 44 = 0.49463848039215685\n",
            "input 45 = 0.5024816176470589\n",
            "input 46 = 0.5286075367647058\n",
            "input 47 = 0.5384114583333333\n",
            "input 48 = 0.46443206188725483\n",
            "input 49 = 0.4603304993872549\n",
            "input 50 = 0.45101102941176474\n",
            "input 51 = 0.4217447916666666\n",
            "input 52 = 0.41194278492647063\n",
            "input 53 = 0.3987457873774509\n",
            "input 54 = 0.6259114583333333\n",
            "input 55 = 0.40321691176470587\n",
            "input 56 = 0.44233302696078425\n",
            "input 57 = 0.4221200980392157\n",
            "input 58 = 0.4448835784313726\n",
            "input 59 = 0.4920879289215685\n",
            "input 60 = 0.4747261795343136\n",
            "input 61 = 0.48655598958333335\n",
            "input 62 = 0.5131663602941177\n",
            "input 63 = 0.5163526348039216\n",
            "input 64 = 0.4749138327205882\n",
            "input 65 = 0.42542126225490196\n",
            "input 66 = 0.4188055300245099\n",
            "input 67 = 0.40360753676470584\n",
            "input 68 = 0.4025677849264706\n",
            "input 69 = 0.43565602022058825\n",
            "input 70 = 0.6416088616421488\n",
            "input 71 = 0.4069680606617647\n",
            "input 72 = 0.42836435355392166\n",
            "input 73 = 0.41859872855392155\n",
            "input 74 = 0.4356789981617647\n",
            "input 75 = 0.4542279411764706\n",
            "input 76 = 0.4583639705882353\n",
            "input 77 = 0.47462660845588234\n",
            "input 78 = 0.4770220588235293\n",
            "input 79 = 0.5085114123774511\n",
            "input 80 = 0.4579829197303922\n",
            "input 81 = 0.4147422640931372\n",
            "input 82 = 0.4069680606617647\n",
            "input 83 = 0.40623276654411766\n",
            "input 84 = 0.4040383731617647\n",
            "input 85 = 0.5219898897058823\n",
            "input 86 = 0.41064453125\n",
            "input 87 = 0.4276290594362746\n",
            "input 88 = 0.43252910539215683\n",
            "input 89 = 0.4227653952205882\n",
            "input 90 = 0.46120557598039214\n",
            "input 91 = 0.44463848039215687\n",
            "input 92 = 0.4525122549019608\n",
            "input 93 = 0.46302657781862744\n",
            "input 94 = 0.480350852350713\n",
            "input 95 = 0.5070389093137255\n",
            "input 96 = 0.4108149509803921\n",
            "input 97 = 0.41756089154411763\n",
            "input 98 = 0.4089288449754902\n",
            "input 99 = 0.4064031862745098\n",
            "input 100 = 0.59656817712464\n",
            "input 101 = 0.4188036151960784\n",
            "input 102 = 0.40672296262254903\n",
            "input 103 = 0.4012561274509804\n",
            "input 104 = 0.4379231770833334\n",
            "input 105 = 0.41873276654411773\n",
            "input 106 = 0.42685546875\n",
            "input 107 = 0.43494370404411764\n",
            "input 108 = 0.4501397824754902\n",
            "input 109 = 0.4539828431372549\n",
            "input 110 = 0.4586952359068628\n",
            "input 111 = 0.49258003982843146\n",
            "input 112 = 0.4224800857843137\n",
            "input 113 = 0.44398935355392155\n",
            "input 114 = 0.4421281403186274\n",
            "input 115 = 0.6201286764705882\n",
            "input 116 = 0.5530396400940005\n",
            "input 117 = 0.4092447916666666\n",
            "input 118 = 0.6620710784313725\n",
            "input 119 = 0.44380361519607836\n",
            "input 120 = 0.43865847120098045\n",
            "input 121 = 0.4592447916666666\n",
            "input 122 = 0.4217275582107843\n",
            "input 123 = 0.4700310202205883\n",
            "input 124 = 0.47640165441176463\n",
            "input 125 = 0.4805683210784313\n",
            "input 126 = 0.48106043198529413\n",
            "input 127 = 0.48932100183823524\n",
            "output 0 = 3.199553568954486\n",
            "output 1 = 8.03\n",
            "sat_unsat: sat\n",
            "vals: {0: 0.5849207261029412, 1: 0.6312442555147059, 2: 0.6361883425245098, 3: 0.6180319393382353, 4: 0.6040268841911764, 5: 0.5852079503676471, 6: 0.5290383731617647, 7: 0.6033643535539216, 8: 0.5032915900735295, 9: 0.5796875, 10: 0.6214403339460784, 11: 0.6454829197303922, 12: 0.6614506740196078, 13: 0.584734987745098, 14: 0.5844305300245098, 15: 0.5832643995098039, 16: 0.6589824601715686, 17: 0.6055089613970588, 18: 0.5273648131127451, 19: 0.5133750765931373, 20: 0.49006778492647063, 21: 0.45060700061274517, 22: 0.5366182675823071, 23: 0.6370404411764706, 24: 0.4127814797794117, 25: 0.4588541666666667, 26: 0.48973651960784315, 27: 0.5083639705882353, 28: 0.5442606927367828, 29: 0.7048330269607843, 30: 0.676629518995098, 31: 0.6669730392156862, 32: 0.5002814797794118, 33: 0.5118815104166667, 34: 0.4969535079656864, 35: 0.463656556372549, 36: 0.43858570772058825, 37: 0.5960094975490196, 38: 0.6403358609068628, 39: 0.4089996936274509, 40: 0.4293428308823529, 41: 0.4548330269607843, 42: 0.4732153799019607, 43: 0.49649969362745094, 44: 0.49463848039215685, 45: 0.5024816176470589, 46: 0.5286075367647058, 47: 0.5384114583333333, 48: 0.46443206188725483, 49: 0.4603304993872549, 50: 0.45101102941176474, 51: 0.4217447916666666, 52: 0.41194278492647063, 53: 0.3987457873774509, 54: 0.6259114583333333, 55: 0.40321691176470587, 56: 0.44233302696078425, 57: 0.4221200980392157, 58: 0.4448835784313726, 59: 0.4920879289215685, 60: 0.4747261795343136, 61: 0.48655598958333335, 62: 0.5131663602941177, 63: 0.5163526348039216, 64: 0.4749138327205882, 65: 0.42542126225490196, 66: 0.4188055300245099, 67: 0.40360753676470584, 68: 0.4025677849264706, 69: 0.43565602022058825, 70: 0.6416088616421488, 71: 0.4069680606617647, 72: 0.42836435355392166, 73: 0.41859872855392155, 74: 0.4356789981617647, 75: 0.4542279411764706, 76: 0.4583639705882353, 77: 0.47462660845588234, 78: 0.4770220588235293, 79: 0.5085114123774511, 80: 0.4579829197303922, 81: 0.4147422640931372, 82: 0.4069680606617647, 83: 0.40623276654411766, 84: 0.4040383731617647, 85: 0.5219898897058823, 86: 0.41064453125, 87: 0.4276290594362746, 88: 0.43252910539215683, 89: 0.4227653952205882, 90: 0.46120557598039214, 91: 0.44463848039215687, 92: 0.4525122549019608, 93: 0.46302657781862744, 94: 0.480350852350713, 95: 0.5070389093137255, 96: 0.4108149509803921, 97: 0.41756089154411763, 98: 0.4089288449754902, 99: 0.4064031862745098, 100: 0.59656817712464, 101: 0.4188036151960784, 102: 0.40672296262254903, 103: 0.4012561274509804, 104: 0.4379231770833334, 105: 0.41873276654411773, 106: 0.42685546875, 107: 0.43494370404411764, 108: 0.4501397824754902, 109: 0.4539828431372549, 110: 0.4586952359068628, 111: 0.49258003982843146, 112: 0.4224800857843137, 113: 0.44398935355392155, 114: 0.4421281403186274, 115: 0.6201286764705882, 116: 0.5530396400940005, 117: 0.4092447916666666, 118: 0.6620710784313725, 119: 0.44380361519607836, 120: 0.43865847120098045, 121: 0.4592447916666666, 122: 0.4217275582107843, 123: 0.4700310202205883, 124: 0.47640165441176463, 125: 0.4805683210784313, 126: 0.48106043198529413, 127: 0.48932100183823524, 128: 1.4977563185309226, 129: 2.220446049250313e-15, 130: -0.46507834931079406, 131: -0.4736065839736897, 132: 2.417945791202805, 133: 2.1850348974690696, 134: -0.20370111438967214, 135: -0.19288263173476308, 136: 0.6882729224993356, 137: 0.6579281873348657, 138: 2.4269980294604085, 139: 2.0098128731240545, 140: 4.94048921061258, 141: 4.874531154892445, 142: -0.24254889409771324, 143: -0.26556365630180334, 144: 1.4977563185309226, 145: 0.0, 146: 2.417945791202805, 147: 0.0, 148: 0.6882729224993341, 149: 2.4269980294604085, 150: 4.94048921061258, 151: 0.0, 152: 0.0, 153: 0.0, 154: 2.1850348974690696, 155: 0.0, 156: 0.6579281873348657, 157: 2.0098128731240545, 158: 4.874531154892445, 159: 0.0, 160: 0.5770615068698226, 161: -2.5976030961926067, 162: -1.9918184216832349, 163: 1.6904388543924322, 164: 2.7247319480396017, 165: 2.084397271987432, 166: 4.180913545574922, 167: 5.15753591658038, 168: 0.5770615068698239, 169: 0.0, 170: 0.0, 171: 1.6904388543924374, 172: 2.7247319480396017, 173: 2.0843972719874273, 174: 4.180913545574922, 175: 5.157535916580378, 176: -6.62526111051986, 177: -1.727737545967102, 178: -1.7011859416961828, 179: -3.6867061837926385, 180: 0.7042066156863847, 181: 3.099502419463661e-15, 182: 7.501971262346317, 183: 2.064566369902366, 184: 0.0, 185: 0.0, 186: 0.0, 187: 0.0, 188: 0.7042066156864166, 189: 0.0, 190: 7.501971262346313, 191: 2.064566369902366, 192: 3.199553568954486, 193: 8.03}\n",
            "stats: <maraboupy.MarabouCore.Statistics object at 0x7bb4ae19bab0>\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}