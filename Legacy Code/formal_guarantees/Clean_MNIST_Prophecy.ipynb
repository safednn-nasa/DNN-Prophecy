{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clean MNIST Prophecy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safednn-nasa/Prophecy/blob/master/Clean_MNIST_Prophecy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-JL_MKop5Qh",
        "colab_type": "code",
        "outputId": "484c49cf-728f-4715-9c5c-4e4731f87e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "import math\n",
        "import io\n",
        "import os\n",
        "from collections import namedtuple\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn import tree\n",
        "from tqdm import tqdm\n",
        "import operator\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RelAEm1T3JVq",
        "colab_type": "code",
        "outputId": "f4624755-9f2b-4bae-df04-583a13970598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "#!python \n",
        "!pip3 install -U pybind11"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6c30562ad531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip3 install -U pybind11'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     with temporary_clearer(), _display_stdin_widget(\n\u001b[0;32m--> 181\u001b[0;31m         delay_millis=500) as update_stdin_widget:\n\u001b[0m\u001b[1;32m    182\u001b[0m       \u001b[0;31m# TODO(b/115531839): Ensure that subprocesses are terminated upon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# interrupt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0mshell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0mdisplay_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_display_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'delayMillis'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdelay_millis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdisplay_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mecho_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_echo_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns8_eYDFReWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 50\n",
        "TEST_BATCH_SIZE = 1000  \n",
        "IMAGE_SIZE = 28\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "LAYER = 7\n",
        "LABEL = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO1Z5bHe06kz",
        "colab_type": "text"
      },
      "source": [
        "## **Load Input Data For MNIST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3Zs1CzKSqjM",
        "colab_type": "code",
        "outputId": "6f55c099-7831-4e07-eee9-a9928b397e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        }
      },
      "source": [
        "mnist = input_data.read_data_sets('MNIST_data')\n",
        "\n",
        "\n",
        "def read_inputs_from_file(inputFile, height, width):\n",
        "    global inputMatrix, labelMatrix, mnist_inp_images, mnist_inp_labels\n",
        "    with open(inputFile) as f:\n",
        "        lines = f.readlines()\n",
        "        print(len(lines), \"examples\")\n",
        "        inputMatrix = np.empty(len(lines),dtype=list)\n",
        "        labelMatrix = np.zeros(len(lines),dtype=int)\n",
        "        mnist_inp_images = np.zeros([8,784],dtype=float)\n",
        "        mnist_inp_labels = np.zeros(8,dtype=int)\n",
        "        for l in range(len(lines)):\n",
        "            if (l == 8):\n",
        "              break\n",
        "            k = [float(stringIn) for stringIn in lines[l].split(',')[1:]] #This is to remove the useless 1 at the start of each string. Not sure why that's there.\n",
        "            inputMatrix[l] = np.zeros((height, width, 1),dtype=float) #we're asuming that everything is 2D for now. The 1 is just to keep numpy happy.\n",
        "            labelMatrix[l] = lines[l].split(',')[0]\n",
        "            mnist_inp_labels[l] = labelMatrix[l]\n",
        "            count = 0\n",
        "            for i in range(height):\n",
        "                for j in range(width):\n",
        "                        print(count)\n",
        "                        inputMatrix[l][i][j] = k[count]\n",
        "                        print(k[count])\n",
        "                        pixel_num = (i *28) + j\n",
        "                        mnist_inp_images[l][pixel_num] = inputMatrix[l][i][j]\n",
        "                        count += 1\n",
        " \n",
        "#!wget https://raw.githubusercontent.com/safednn-nasa/prophecy_DNN/sym_convnn/master/MNIST_ITR_REL/mnist_inputs1.txt -O ./mnist_train_labels.txt\n",
        "\n",
        "#read_inputs_from_file('./mnist_train_labels.txt', 28, 28)\n",
        "\n",
        "   "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-fc392def7aa9>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C45xGa98mzt",
        "colab_type": "text"
      },
      "source": [
        "##**Architecture Of the MNIST Model (10 Layers) In TENSORFLOW** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39JpLmTitqdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def weight_variable(shape, name):\n",
        "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "  return tf.Variable(initial, name=name)\n",
        "\n",
        "def bias_variable(shape, name):\n",
        "  initial = tf.constant(0.1, shape=shape)\n",
        "  return tf.Variable(initial, name=name)\n",
        "\n",
        "def fc2d(x, W):\n",
        "  return tf.nn.fc2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
        "                        strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "def create_model():\n",
        "    x = tf.identity(tf.placeholder(tf.float32, shape=[None, 784]), name=\"input\")\n",
        "    y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "\n",
        "    W_fc1 = weight_variable([784, 10],name='w_fc1')\n",
        "    b_fc1 = bias_variable([10],name='b_fc1')\n",
        "    x_image = tf.reshape(x, [-1, 784])\n",
        "    h_fc1_relu_inp = tf.identity(tf.matmul(x_image, W_fc1) + b_fc1, name='h_fc1_relu_inp')\n",
        "    h_fc1 = tf.nn.relu(tf.matmul(x_image, W_fc1) + b_fc1)\n",
        "#    h_fc1_iden = tf.identity(h_fc1,name='h_fc1')\n",
        "    \n",
        "    \n",
        "    W_fc2 = weight_variable([10, 10],name='w_fc2')\n",
        "    b_fc2 = bias_variable([10],name='b_fc2')\n",
        "    h_fc2_relu_inp = tf.identity(tf.matmul(h_fc1, W_fc2) + b_fc2, name='h_fc2_relu_inp')\n",
        "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
        " #   h_fc2_iden = tf.identity(h_fc2,name='h_fc2')\n",
        "    \n",
        "    \n",
        "    W_fc3 = weight_variable([10, 10],name='w_fc3')\n",
        "    b_fc3 = bias_variable([10],name='b_fc3')\n",
        "    h_fc3_relu_inp = tf.identity(tf.matmul(h_fc2, W_fc3) + b_fc3, name='h_fc3_relu_inp')\n",
        "    h_fc3 = tf.nn.relu(tf.matmul(h_fc2, W_fc3) + b_fc3)\n",
        "  #  h_fc3_iden = tf.identity(h_fc3,name='h_fc3')\n",
        "\n",
        "    W_fc4 = weight_variable([10, 10],name='w_fc4')\n",
        "    b_fc4 = bias_variable([10],name='b_fc4')\n",
        "    h_fc4_relu_inp = tf.identity(tf.matmul(h_fc3, W_fc4) + b_fc4, name='h_fc4_relu_inp')\n",
        "    h_fc4 = tf.nn.relu(tf.matmul(h_fc3, W_fc4) + b_fc4)\n",
        "   # h_fc4_iden = tf.identity(h_fc4,name='h_fc4')\n",
        "    \n",
        "    \n",
        "    W_fc5 = weight_variable([10, 10],name='w_fc5')\n",
        "    b_fc5 = bias_variable([10],name='b_fc5')\n",
        "    h_fc5_relu_inp = tf.identity(tf.matmul(h_fc4, W_fc5) + b_fc5, name='h_fc5_relu_inp')\n",
        "    h_fc5 = tf.nn.relu(tf.matmul(h_fc4, W_fc5) + b_fc5)\n",
        "    #h_fc5_iden = tf.identity(h_fc5,name='h_fc5')\n",
        "\n",
        "   \n",
        "    W_fc6 = weight_variable([10, 10],name='w_fc6')\n",
        "    b_fc6 = bias_variable([10],name='b_fc6')\n",
        "    h_fc6_relu_inp = tf.identity(tf.matmul(h_fc5, W_fc6) + b_fc6, name='h_fc6_relu_inp')\n",
        "    h_fc6 = tf.nn.relu(tf.matmul(h_fc5, W_fc6) + b_fc6)\n",
        "   # h_fc6_iden = tf.identity(h_fc6,name='h_fc6')\n",
        "    \n",
        "    \n",
        "    W_fc7 = weight_variable([10, 10],name='w_fc7')\n",
        "    b_fc7 = bias_variable([10],name='b_fc7')\n",
        "    h_fc7_relu_inp = tf.identity(tf.matmul(h_fc6, W_fc7) + b_fc7, name='h_fc7_relu_inp')\n",
        "    h_fc7 = tf.nn.relu(tf.matmul(h_fc6, W_fc7) + b_fc7)\n",
        "   # h_fc7_iden = tf.identity(h_fc7,name='h_fc7')\n",
        "\n",
        "    W_fc8 = weight_variable([10, 10],name='w_fc8')\n",
        "    b_fc8 = bias_variable([10],name='b_fc8')\n",
        "    h_fc8_relu_inp = tf.identity(tf.matmul(h_fc7, W_fc8) + b_fc8, name='h_fc8_relu_inp')\n",
        "    h_fc8 = tf.nn.relu(tf.matmul(h_fc7, W_fc8) + b_fc8)\n",
        "   # h_fc8_iden = tf.identity(h_fc8,name='h_fc8')\n",
        "    \n",
        "    \n",
        "    W_fc9 = weight_variable([10, 10],name='w_fc9')\n",
        "    b_fc9 = bias_variable([10],name='b_fc9')\n",
        "    h_fc9_relu_inp = tf.identity(tf.matmul(h_fc8, W_fc9) + b_fc9, name='h_fc9_relu_inp')\n",
        "    h_fc9 = tf.nn.relu(tf.matmul(h_fc8, W_fc9) + b_fc9)\n",
        "   # h_fc9_iden = tf.identity(h_fc9,name='h_fc9')\n",
        "\n",
        "    W_fc10 = weight_variable([10, 10],name='w_fc10')\n",
        "    b_fc10 = bias_variable([10],name='b_fc10')\n",
        "    h_fc10_relu_inp = tf.identity(tf.matmul(h_fc9, W_fc10) + b_fc10, name='h_fc10_relu_inp')\n",
        "    h_fc10 = tf.nn.relu(tf.matmul(h_fc9, W_fc10) + b_fc10)\n",
        "    #h_fc10_iden = tf.identity(h_fc10,name='h_fc10')\n",
        "\n",
        "    W_fc11 = weight_variable([10, 10],name='w_fc11')\n",
        "    b_fc11 = bias_variable([10],name='b_fc11')\n",
        "    \n",
        "    y_fc = tf.matmul(h_fc10, W_fc11) + b_fc11\n",
        "    h_y_fc_iden = tf.identity(y_fc,name='y_fc')\n",
        "    \n",
        "    \n",
        "    prediction = tf.identity(tf.nn.softmax(y_fc), name=\"import/prediction\")\n",
        "    \n",
        "    \n",
        "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_fc))\n",
        "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "    correct_prediction = tf.equal(tf.argmax(y_fc, 1), tf.argmax(y_, 1))\n",
        "    \n",
        "    \n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    \n",
        "\n",
        "    return cross_entropy, accuracy, x, keep_prob, y_fc, y_, W_fc1, W_fc2, W_fc3, W_fc4, W_fc5, W_fc6, W_fc7, W_fc8, W_fc9, W_fc10, W_fc11, b_fc1, b_fc2, b_fc3, b_fc4, b_fc5, b_fc6, b_fc7, b_fc8, b_fc9, b_fc10, b_fc11, h_fc1, h_fc1_relu_inp, h_fc2, h_fc2_relu_inp, h_fc3, h_fc3_relu_inp, h_fc4, h_fc4_relu_inp, h_fc5, h_fc5_relu_inp, h_fc6, h_fc6_relu_inp, h_fc7, h_fc7_relu_inp, h_fc8, h_fc8_relu_inp, h_fc9, h_fc9_relu_inp, h_fc10, h_fc10_relu_inp, y_fc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv3U6Xs_T2el",
        "colab_type": "text"
      },
      "source": [
        "### **Train a new model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-2x6UTcuJUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.reset_default_graph()\n",
        "#sess = tf.InteractiveSession()\n",
        "#cross_entropy, accuracy, x, keep_prob, y_conv, y_ = create_model()\n",
        "#train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "#saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n",
        "#sess.run(tf.global_variables_initializer())\n",
        "#for i in range(0, 1200):\n",
        "#  batch = mnist.train.next_batch(BATCH_SIZE)\n",
        "#  train_step.run(feed_dict={x: batch[0], y_: np.eye(10)[batch[1]], keep_prob: 0.5})\n",
        "#  if i%100 == 0:\n",
        "#    test_accuracy = accuracy.eval(feed_dict={\n",
        "#        x:mnist.test.images, y_: np.eye(10)[mnist.test.labels], keep_prob: 1.0})\n",
        "#    print(\"step %d, test accuracy %g\"%(i, test_accuracy))    \n",
        "#ckpt_path_name = saver.save(sess, './checkpoints/mnist_invariant.ckpt', global_step=i)\n",
        "#print \"Checkpoint saved at: %s\" % ckpt_path_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylVCQ9lfTfWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import files\n",
        "#files.download(ckpt_path_name + '.data-00000-of-00001')\n",
        "#files.download(ckpt_path_name + '.index')\n",
        "#files.download(ckpt_path_name + '.meta')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy6_gvmpT4Wv",
        "colab_type": "text"
      },
      "source": [
        "### Restore a pretrained model from check point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMibD_lwWdVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!mkdir -p ./checkpoints\n",
        "#!wget https://github.com/safednn-nasa/prophecy_DNN/sym_convnn/raw/master/MNIST_ITR_REL/MNIST_conv_checkpoint/mnist_invariants.ckpt.index -O ./checkpoints/mnist_invariants.ckpt.index\n",
        "#!wget https://github.com/safednn-nasa/prophecy_DNN/sym_convnn/raw/master/MNIST_ITR_REL/MNIST_conv_checkpoint/mnist_invariants.ckpt.meta -O ./checkpoints/mnist_invariants.ckpt.meta\n",
        "#!wget https://github.com/safednn-nasa/prophecy_DNN/sym_convnn/raw/master/MNIST_ITR_REL/MNIST_conv_checkpoint/mnist_invariants.ckpt.data-00000-of-00001 -O ./checkpoints/mnist_invariants.ckpt.data-00000-of-00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwDRpu0_1QDv",
        "colab_type": "text"
      },
      "source": [
        "##**Restore pre-trained model from a .nn file with weights and biases**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JmFzaPPT6ql",
        "colab_type": "code",
        "outputId": "ad1ed1c2-06ad-486f-8bce-c9d2fd3c61ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "def read_weights_from_file(inputFile):\n",
        "    global weightMatrix, biasMatrix\n",
        "    \n",
        "    with open(inputFile) as f:\n",
        "        lines = f.readlines()\n",
        "        for indx in range(0,len(lines)):\n",
        "            #print(indx, lines[indx])\n",
        "            numberOfLayers = int(lines[0])\n",
        "            numberOfLayers = 11\n",
        "            weightMatrix = np.empty(numberOfLayers, dtype=list)\n",
        "            biasMatrix = np.empty(numberOfLayers, dtype=list)\n",
        "            currentLine = 2\n",
        "            for i in range(numberOfLayers):\n",
        "              dimensions = lines[currentLine].split(',')\n",
        "              dimensions = [int(stringDimension) for stringDimension in dimensions]\n",
        "              #print dimensions\n",
        "              currentLine += 1\n",
        "              weights = [float(stringWeight) for stringWeight in lines[currentLine].split(',')]\n",
        "              #print len(weights)\n",
        "              count = 0\n",
        "              weightMatrix[i] = np.zeros((dimensions[0], dimensions[1]), dtype=float)\n",
        "              for j in range(dimensions[1]):\n",
        "                 for k in range(dimensions[0]):\n",
        "                      weightMatrix[i][k][j] = weights[count]\n",
        "                      count += 1\n",
        "              currentLine += 1\n",
        "              biases = [float(stringBias) for stringBias in lines[currentLine].split(',')]\n",
        "              biasMatrix[i] = np.zeros(len(biases))\n",
        "              biasMatrix[i] = biases\n",
        "              currentLine += 2\n",
        "\n",
        "!wget https://raw.githubusercontent.com/safednn-nasa/prophecy_DNN/master/MNIST_ITR_REL/mnist_10_layer.txt -O ./mnist_10_layer.txt\n",
        "\n",
        "\n",
        "read_weights_from_file('./mnist_10_layer.txt')\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "cross_entropy, accuracy, x, keep_prob, y_fc, y_, W_fc1, W_fc2, W_fc3, W_fc4, W_fc5, W_fc6, W_fc7, W_fc8, W_fc9, W_fc10, W_fc11,b_fc1, b_fc2, b_fc3, b_fc4, b_fc5, b_fc6, b_fc7, b_fc8, b_fc9, b_fc10, b_fc11, h_fc1, h_fc1_relu_inp, h_fc2, h_fc2_relu_inp, h_fc3, h_fc3_relu_inp,h_fc4, h_fc4_relu_inp, h_fc5, h_fc5_relu_inp, h_fc6, h_fc6_relu_inp, h_fc7, h_fc7_relu_inp, h_fc8, h_fc8_relu_inp, h_fc9, h_fc9_relu_inp, h_fc10, h_fc10_relu_inp, y_fc = create_model()\n",
        "feed_dict = {x:mnist.test.images, y_: np.eye(10)[mnist.test.labels], keep_prob: 1.0, W_fc1: weightMatrix[0], b_fc1: biasMatrix[0], W_fc2: weightMatrix[1], b_fc2: biasMatrix[1], W_fc3: weightMatrix[2], b_fc3: biasMatrix[2], W_fc4: weightMatrix[3], b_fc4: biasMatrix[3], W_fc5: weightMatrix[4], b_fc5: biasMatrix[4], W_fc6: weightMatrix[5], b_fc6: biasMatrix[5],W_fc7: weightMatrix[6], b_fc7: biasMatrix[6],W_fc8: weightMatrix[7], b_fc8: biasMatrix[7],W_fc9: weightMatrix[8], b_fc9: biasMatrix[8], W_fc10: weightMatrix[9], b_fc10: biasMatrix[9], W_fc11: weightMatrix[10], b_fc11: biasMatrix[10]}\n",
        "  \n",
        "test_accuracy = accuracy.eval(feed_dict)\n",
        "print(\"Test accuracy %g\"%(test_accuracy))   \n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-07 23:33:49--  https://raw.githubusercontent.com/safednn-nasa/prophecy_DNN/master/MNIST_ITR_REL/mnist_10_layer.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 146765 (143K) [text/plain]\n",
            "Saving to: ‘./mnist_10_layer.txt’\n",
            "\n",
            "\r./mnist_10_layer.tx   0%[                    ]       0  --.-KB/s               \r./mnist_10_layer.tx 100%[===================>] 143.33K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-12-07 23:33:49 (3.78 MB/s) - ‘./mnist_10_layer.txt’ saved [146765/146765]\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-10-e53413d75d28>:99: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy 0.933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR9bYFN9E9lg",
        "colab_type": "text"
      },
      "source": [
        "## Get tensors from pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2F_QleSkgvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cross_entropy, accuracy, x, keep_prob, y_fc, y_, W_fc1, W_fc2, W_fc3, W_fc4, W_fc5, W_fc6, W_fc7, W_fc8, W_fc9, W_fc10, W_fc11,b_fc1, b_fc2, b_fc3, b_fc4, b_fc5, b_fc6, b_fc7, b_fc8, b_fc9, b_fc10, b_fc11, h_fc1, h_fc2, h_fc2_relu_inp, h_fc3, h_fc3_relu_inp,h_fc4, h_fc4_relu_inp, h_fc5, h_fc5_relu_inp, h_fc6, h_fc7, h_fc8, h_fc9, h_fc10, y_fc = create_model()\n",
        "\n",
        "cross_entropy, accuracy, x, keep_prob, y_fc, y_, W_fc1, W_fc2, W_fc3, W_fc4, W_fc5, W_fc6, W_fc7, W_fc8, W_fc9, W_fc10, W_fc11,b_fc1, b_fc2, b_fc3, b_fc4, b_fc5, b_fc6, b_fc7, b_fc8, b_fc9, b_fc10, b_fc11, h_fc1, h_fc1_relu_inp, h_fc2, h_fc2_relu_inp, h_fc3, h_fc3_relu_inp,h_fc4, h_fc4_relu_inp, h_fc5, h_fc5_relu_inp, h_fc6, h_fc6_relu_inp, h_fc7, h_fc7_relu_inp, h_fc8, h_fc8_relu_inp, h_fc9, h_fc9_relu_inp, h_fc10, h_fc10_relu_inp, y_fc = create_model()\n",
        "\n",
        "t_fc1 = h_fc1 #sess.graph.get_tensor_by_name('h_fc1:0')\n",
        "t_fc2 = h_fc2 #sess.graph.get_tensor_by_name('h_fc2:0')\n",
        "t_fc3 = h_fc3 #sess.graph.get_tensor_by_name('h_fc3:0')\n",
        "t_fc4 = h_fc4 #sess.graph.get_tensor_by_name('h_fc4:0')\n",
        "t_fc5 = h_fc5 #sess.graph.get_tensor_by_name('h_fc5:0')\n",
        "t_fc6 = h_fc6 #sess.graph.get_tensor_by_name('h_fc6:0')\n",
        "t_fc7 = h_fc7 #sess.graph.get_tensor_by_name('h_fc7:0')\n",
        "t_fc8 = h_fc8 #sess.graph.get_tensor_by_name('h_fc8:0')\n",
        "t_fc9 = h_fc9 #sess.graph.get_tensor_by_name('h_fc9:0')\n",
        "t_fc10 = h_fc10 #sess.graph.get_tensor_by_name('h_fc10:0')\n",
        "\n",
        "if (LAYER == 1):\n",
        "  curr_lay = t_fc1\n",
        "  prev_lay = t_fc1\n",
        "  curr_hlay = h_fc1\n",
        "  prev_hlay = h_fc1\n",
        "  inp_lay = h_fc1_relu_inp\n",
        "  \n",
        "if (LAYER == 2):\n",
        "  curr_lay = t_fc2\n",
        "  prev_lay = t_fc1\n",
        "  curr_hlay = h_fc2\n",
        "  prev_hlay = h_fc1\n",
        "  inp_lay = h_fc2_relu_inp\n",
        "  \n",
        "if (LAYER == 3):\n",
        "  curr_lay = t_fc3\n",
        "  prev_lay = t_fc2\n",
        "  curr_hlay = h_fc3\n",
        "  prev_hlay = h_fc2\n",
        "  inp_lay = h_fc3_relu_inp\n",
        "  \n",
        "if (LAYER == 4):\n",
        "  curr_lay = t_fc4\n",
        "  prev_lay = t_fc3\n",
        "  curr_hlay = h_fc4\n",
        "  prev_hlay = h_fc3\n",
        "  inp_lay = h_fc4_relu_inp\n",
        "\n",
        "if (LAYER == 5):\n",
        "  curr_lay = t_fc5\n",
        "  prev_lay = t_fc4\n",
        "  curr_hlay = h_fc5\n",
        "  prev_hlay = h_fc4\n",
        "  inp_lay = h_fc5_relu_inp\n",
        "  \n",
        "if (LAYER == 6):\n",
        "  curr_lay = t_fc6\n",
        "  prev_lay = t_fc5\n",
        "  curr_hlay = h_fc6\n",
        "  prev_hlay = h_fc5\n",
        "  inp_lay = h_fc6_relu_inp\n",
        "  \n",
        "if (LAYER == 7):\n",
        "  curr_lay = t_fc7\n",
        "  prev_lay = t_fc6\n",
        "  curr_hlay = h_fc7\n",
        "  prev_hlay = h_fc6\n",
        "  inp_lay = h_fc7_relu_inp\n",
        "  \n",
        "if (LAYER == 8):\n",
        "  curr_lay = t_fc8\n",
        "  prev_lay = t_fc7\n",
        "  curr_hlay = h_fc8\n",
        "  prev_hlay = h_fc7\n",
        "  inp_lay = h_fc8_relu_inp\n",
        "  \n",
        "if (LAYER == 9):\n",
        "  curr_lay = t_fc9\n",
        "  prev_lay = t_fc8\n",
        "  curr_hlay = h_fc9\n",
        "  prev_hlay = h_fc8\n",
        "  inp_lay = h_fc9_relu_inp\n",
        "  \n",
        "if (LAYER == 10):\n",
        "  curr_lay = t_fc10\n",
        "  prev_lay = t_fc9\n",
        "  curr_hlay = h_fc10\n",
        "  prev_hlay = h_fc9\n",
        "  inp_lay = h_fc10_relu_inp\n",
        "  \n",
        "t_label = tf.placeholder(tf.int32)\n",
        "t_neuron_id = tf.placeholder(tf.int32)\n",
        "t_grad = tf.gradients(y_fc[:, t_label], x)\n",
        "t_grad_neuron = tf.gradients(y_fc[:, t_label], curr_hlay)[0]\n",
        "t_grad_conductance = tf.gradients(inp_lay[:,t_neuron_id], x, grad_ys=t_grad_neuron[:, t_neuron_id])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL92gsWskV0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "\n",
        "def get_prediction(inps, tensor=y_fc, batch_size=100):\n",
        "  def get_prediction_batch(batch):\n",
        "    #feed = {x: np.array(batch), keep_prob:1.0}\n",
        "    feed = {x: np.array(batch), keep_prob:1.0, W_fc1: weightMatrix[0], b_fc1: biasMatrix[0], W_fc2: weightMatrix[1], b_fc2: biasMatrix[1], W_fc3: weightMatrix[2], b_fc3: biasMatrix[2], W_fc4: weightMatrix[3], b_fc4: biasMatrix[3], W_fc5: weightMatrix[4], b_fc5: biasMatrix[4], W_fc6: weightMatrix[5], b_fc6: biasMatrix[5],W_fc7: weightMatrix[6], b_fc7: biasMatrix[6],W_fc8: weightMatrix[7], b_fc8: biasMatrix[7],W_fc9: weightMatrix[8], b_fc9: biasMatrix[8], W_fc10: weightMatrix[9], b_fc10: biasMatrix[9], W_fc11: weightMatrix[10], b_fc11: biasMatrix[10]}\n",
        "    return sess.run(tensor, feed_dict=feed)\n",
        "  n = len(inps)\n",
        "  if n%batch_size == 0:\n",
        "    batches = [inps[i*batch_size:(i+1)*batch_size] for i in range(int(n/batch_size))]\n",
        "  else:\n",
        "    batches = [inps[i*batch_size:(i+1)*batch_size] for i in range(int(n/batch_size) +1)]    \n",
        "  batch_predictions = [get_prediction_batch(b) for b in tqdm(batches)]\n",
        "  return np.concatenate(tuple(batch_predictions), axis=0)\n",
        "\n",
        "def attribute(inp, label, baseline=None, steps=50, use_top_label=False):\n",
        "  def top_label(inp):\n",
        "    return np.argmax(get_prediction([inp])[0])\n",
        "  if baseline is None:\n",
        "    baseline = 0*inp\n",
        "  scaled_inputs = [baseline + (float(i)/steps)*(inp-baseline) for i in range(0, steps)]\n",
        "  #feed = {keep_prob:1.0}\n",
        "  feed = {keep_prob:1.0, W_fc1: weightMatrix[0], b_fc1: biasMatrix[0], W_fc2: weightMatrix[1], b_fc2: biasMatrix[1], W_fc3: weightMatrix[2], b_fc3: biasMatrix[2], W_fc4: weightMatrix[3], b_fc4: biasMatrix[3], W_fc5: weightMatrix[4], b_fc5: biasMatrix[4], W_fc6: weightMatrix[5], b_fc6: biasMatrix[5],W_fc7: weightMatrix[6], b_fc7: biasMatrix[6],W_fc8: weightMatrix[7], b_fc8: biasMatrix[7],W_fc9: weightMatrix[8], b_fc9: biasMatrix[8], W_fc10: weightMatrix[9], b_fc10: biasMatrix[9], W_fc11: weightMatrix[10], b_fc11: biasMatrix[10]}\n",
        "    \n",
        "  if use_top_label:\n",
        "    feed[x] = [inp]\n",
        "    logits = sess.run(y_fc, feed_dict=feed)[0]\n",
        "    label = np.argmax(logits)\n",
        "  feed[x] = scaled_inputs\n",
        "  feed[t_label] = label\n",
        "  grads, scores = sess.run([t_grad, y_fc], feed_dict=feed)  # shapes: <steps+1>, <steps+1, inp.shape>\n",
        "  integrated_gradients = (inp-baseline)*np.average(grads[0], axis=0)  # shape: <inp.shape>\n",
        "#  print \"FINAL SCORE\", scores[-1][label]\n",
        "#  print \"BASELINE SCORE\", scores[0][label]\n",
        "#  print \"SUM\", np.sum(integrated_gradients), \"DIFF\", scores[-1][label] - scores[0][label]\n",
        "  return integrated_gradients\n",
        "\n",
        "def conductance(inp, label, neuron_id=None, baseline=None, steps=50):\n",
        "  # neuron_id is the id of the neuron in layer t_fc1 through which conductance\n",
        "  # must be computed. If None, vanilla IG is computed.\n",
        "  if baseline is None:\n",
        "    baseline = 0*inp\n",
        "  scaled_inputs = [baseline + (float(i)/steps)*(inp-baseline) for i in range(0, steps)]\n",
        "  feed = {keep_prob:1.0, W_fc1: weightMatrix[0], b_fc1: biasMatrix[0], W_fc2: weightMatrix[1], b_fc2: biasMatrix[1], W_fc3: weightMatrix[2], b_fc3: biasMatrix[2], W_fc4: weightMatrix[3], b_fc4: biasMatrix[3], W_fc5: weightMatrix[4], b_fc5: biasMatrix[4], W_fc6: weightMatrix[5], b_fc6: biasMatrix[5],W_fc7: weightMatrix[6], b_fc7: biasMatrix[6],W_fc8: weightMatrix[7], b_fc8: biasMatrix[7],W_fc9: weightMatrix[8], b_fc9: biasMatrix[8], W_fc10: weightMatrix[9], b_fc10: biasMatrix[9], W_fc11: weightMatrix[10], b_fc11: biasMatrix[10]}\n",
        "  feed[x] = scaled_inputs\n",
        "  feed[t_label] = label\n",
        "  if neuron_id != None:\n",
        "    feed[t_neuron_id] = neuron_id\n",
        "    grads, scores = sess.run([t_grad_conductance, y_fc], feed_dict=feed)  # shapes: <steps+1>, <steps+1, inp.shape>\n",
        "    integrated_gradients = (inp-baseline)*np.average(grads[0], axis=0)  # shape: <inp.shape>\n",
        "    return integrated_gradients\n",
        "  grads, scores = sess.run([t_grad, y_fc], feed_dict=feed)  # shapes: <steps+1>, <steps+1, inp.shape>    \n",
        "  integrated_gradients = (inp-baseline)*np.average(grads[0], axis=0)  # shape: <inp.shape>\n",
        "  #print \"FINAL SCORE\", scores[-1][label]\n",
        "  #print \"BASELINE SCORE\", scores[0][label]\n",
        "  #print \"SUM\", np.sum(integrated_gradients), \"DIFF\", scores[-1][label] - scores[0][label]\n",
        "  return integrated_gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_uhUyYiBlL7",
        "colab_type": "text"
      },
      "source": [
        "## Extracting Invariant Candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzZVWWP5O8qV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPvUSoGyFkdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fingerprint_suffix(inps):\n",
        "  # Below t_fc1 is the final fully connected layer of size 1024.\n",
        "  return (get_prediction(inps, tensor=t_fc6)>0.0).astype('int')\n",
        "\n",
        "def fingerprint_signature(inps,ten = t_fc1):\n",
        "  # Below t_fc1 is the final fully connected layer of size 1024.\n",
        "  return (get_prediction(inps, tensor=ten)>0.0).astype('int')\n",
        "\n",
        "def fingerprint_prefix(inps):\n",
        "  return (get_prediction(inps, tensor=tf.reshape(t_fc1, [-1, 14*14*32]))>0.0).astype('int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QarR8VNHZNk",
        "colab_type": "code",
        "outputId": "3bd293f5-ff3b-4446-9e8a-ec3ca72cea98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# train_suffixes, train_predictions are in the same order\n",
        "# as mnist.train.images. Henceforth when we use the index i we will\n",
        "# be referring to mnist.train.images[i].\n",
        "train_suffixes = fingerprint_signature(mnist.train.images, curr_lay)\n",
        "print(\"Suffixes computed for all training data\")\n",
        "train_predictions = np.argmax(get_prediction(mnist.train.images), axis=1)\n",
        "print(\"Predictions computed for all training data\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 550/550 [00:00<00:00, 598.27it/s]\n",
            " 10%|█         | 56/550 [00:00<00:00, 553.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Suffixes computed for all training data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 550/550 [00:00<00:00, 598.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predictions computed for all training data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxgnKvn04Q92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def describe_input(i, training=True):\n",
        "  print(\"Input\", i)\n",
        "  print(\"Groundtruth\", mnist.train.labels[i])\n",
        "  print(\"Prediction\", train_predictions[i])\n",
        "  print(\"Fine-grained prediction\", 10*mnist.train.labels[i] + train_predictions[i])\n",
        "  show_mnist_img(mnist.train.images[i])\n",
        "  \n",
        "def describe_input_INP(i):\n",
        "  print(\"Input\", i)\n",
        "  print(\"Groundtruth\", mnist_inp_labels[i])\n",
        "  print(\"Prediction\", inp_predictions[i])\n",
        "  show_mnist_img(mnist_inp_images[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fcVM2dek4Qo",
        "colab_type": "text"
      },
      "source": [
        "### Build the Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_0fvX3aAWHN",
        "colab_type": "code",
        "outputId": "67ab2e58-b9f9-452e-b937-e5bfc64b7011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Basic decision tree\n",
        "basic_estimator = tree.DecisionTreeClassifier()\n",
        "basic_estimator.fit(train_suffixes, train_predictions)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "                       max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort=False,\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkvj66ZvX3B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fine-grained predictions decision tree\n",
        "#fine_grained_predictions = 10*mnist.train.labels + train_predictions\n",
        "#fine_grained_estimator = tree.DecisionTreeClassifier()\n",
        "#fine_grained_estimator.fit(train_suffixes, fine_grained_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfwDuDQtprdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decision tree per label\n",
        "def get_relative_predictions(label):\n",
        "  #print \"Create relative predictions for label:%d\" % label\n",
        "  res = np.zeros(train_predictions.shape)\n",
        "  for i in range(len(train_predictions)):\n",
        "    pred = train_predictions[i]\n",
        "    gt = mnist.train.labels[i]\n",
        "    if gt == label and pred == gt:\n",
        "      res[i] = 0\n",
        "    elif gt == label and pred != gt:\n",
        "      res[i] = 1\n",
        "    else:\n",
        "      res[i] = 2\n",
        "  #print \"Num correct: %d\" % np.sum(res == 0)\n",
        "  #print \"Num misclassified: %d\" % np.sum(res == 1)\n",
        "  #print \"Num others: %d\" % np.sum(res == 2)\n",
        "  return res\n",
        "\n",
        "def get_relative_estimator(label):\n",
        "  predictions = get_relative_predictions(label)\n",
        "  #print \"Creating decision tree for label:%d\" % label\n",
        "  estimator = tree.DecisionTreeClassifier()\n",
        "  estimator.fit(train_suffixes, predictions)\n",
        "  return estimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHJlFvwXzXgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SLOW; run only if you want to build relative estimators.\n",
        "#relative_estimators = [None for _ in range(10)]\n",
        "#for i in range(10):\n",
        "#  relative_estimators[i] = get_relative_estimator(i)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBK9KWLc6TSe",
        "colab_type": "code",
        "outputId": "84d05690-f0b2-48ed-f284-7def9590957d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/safednn-nasa/prophecy_DNN/master/marabou1.elf -O ./marabou1.elf\n",
        "!wget https://raw.githubusercontent.com/safednn-nasa/prophecy_DNN/master/MNIST_ITR_REL/mnist_10_layer.nnet -O ./mnist_10_layer.nnet\n",
        "!pwd\n",
        "!ls -lt ./marabou1.elf\n",
        "!chmod 777 ./marabou1.elf\n",
        "!ls -lt ./marabou1.elf"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-07 23:36:40--  https://raw.githubusercontent.com/safednn-nasa/prophecy_DNN/master/marabou1.elf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4231816 (4.0M) [application/octet-stream]\n",
            "Saving to: ‘./marabou1.elf’\n",
            "\n",
            "./marabou1.elf      100%[===================>]   4.04M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-12-07 23:36:41 (35.3 MB/s) - ‘./marabou1.elf’ saved [4231816/4231816]\n",
            "\n",
            "--2019-12-07 23:36:42--  https://raw.githubusercontent.com/safednn-nasa/prophecy_DNN/master/MNIST_ITR_REL/mnist_10_layer.nnet\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 159523 (156K) [text/plain]\n",
            "Saving to: ‘./mnist_10_layer.nnet’\n",
            "\n",
            "./mnist_10_layer.nn 100%[===================>] 155.78K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-12-07 23:36:42 (4.01 MB/s) - ‘./mnist_10_layer.nnet’ saved [159523/159523]\n",
            "\n",
            "/content\n",
            "-rw-r--r-- 1 root root 4231816 Dec  7 23:36 ./marabou1.elf\n",
            "-rwxrwxrwx 1 root root 4231816 Dec  7 23:36 ./marabou1.elf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy0b_gz46_55",
        "colab_type": "text"
      },
      "source": [
        "## Invoke Marabou For Verifying Likely Properties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLuPLXFC6_Rd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def invoke_marabou_chk(layer,neurons,signature,label):\n",
        "  #layer = 1\n",
        "  #neurons = [4, 8, 7, 1, 0, 2, 5, 3, 9, 6] \n",
        "  #signature = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "  #label = 6\n",
        "\n",
        "  for lab_indx in range(0,10):\n",
        "    if (lab_indx == label):\n",
        "      continue\n",
        "    strInp = \"\"\n",
        "    for i in range(0,784):\n",
        "      strInp = strInp + \"x\"+ str(i) + \" >= 0.0\" + \"\\n\"\n",
        "      strInp = strInp + \"x\"+ str(i) + \" <= 1.0\" + \"\\n\"\n",
        "    #print(strInp)\n",
        "\n",
        "    strInternal = \"\"\n",
        "    for i in range(0,len(neurons)):\n",
        "      strInternal = strInternal + \"ws_\"+ str(layer) + \"_\" + str(neurons[i])\n",
        "      if (signature[i] == 0):\n",
        "         strInternal = strInternal + \" <= 0.0\" + \"\\n\"\n",
        "      else:\n",
        "         strInternal = strInternal + \" >= 0.0\"  + \"\\n\"\n",
        "\n",
        "    strOP = \"-y\"+ str(lab_indx) + \" +y\" + str(label) + \" <= 0.00\" + \"\\n\"\n",
        "\n",
        "    #Write to a property file\n",
        "    file1 = open('property.txt',\"w\")\n",
        "    file1.writelines(strInp) \n",
        "    file1.writelines(strInternal) \n",
        "    file1.writelines(strOP) \n",
        "    file1.close() \n",
        "\n",
        "    #file1 = open('property.txt',\"r\")  \n",
        "    #print(\"PROPERTY FILE IS \")\n",
        "    #print(file1.read())\n",
        "    #file1.close()\n",
        "\n",
        "    !./marabou1.elf ./mnist_10_layer.nnet ./property.txt --summary-file=summary1.txt --verbosity=0\n",
        "    print(\"Summary Results from Marabou:\")\n",
        "    f = open('summary1.txt', 'r')\n",
        "    file_contents = f.read()\n",
        "    #print (file_contents)\n",
        "    f.close()\n",
        "    if (file_contents.find('UNSAT') == -1):\n",
        "        break\n",
        "  #f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK1Pr1agnsoi",
        "colab_type": "text"
      },
      "source": [
        "### Examine properties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbOQkQU0zBz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_decision_path(estimator, inp):\n",
        "  # Extract the decision path taken by an input as an ordered list of indices\n",
        "  # of the neurons that were evaluated.\n",
        "  # See: http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
        "  n_nodes = estimator.tree_.node_count\n",
        "  feature = estimator.tree_.feature\n",
        "\n",
        "  # First let's retrieve the decision path of each sample. The decision_path\n",
        "  # method allows to retrieve the node indicator functions. A non zero element of\n",
        "  # indicator matrix at the position (i, j) indicates that the sample i goes\n",
        "  # through the node j.\n",
        "  X_test = [inp]\n",
        "  node_indicator = estimator.decision_path(X_test)\n",
        "  # Similarly, we can also have the leaves ids reached by each sample.\n",
        "  leaf_id = estimator.apply(X_test)\n",
        "  # Now, it's possible to get the tests that were used to predict a sample or\n",
        "  # a group of samples. First, let's make it for the sample.\n",
        "  node_index = node_indicator.indices[node_indicator.indptr[0]:\n",
        "                                      node_indicator.indptr[1]]\n",
        "  neuron_ids = []\n",
        "  for node_id in node_index:\n",
        "    if leaf_id[0] == node_id:\n",
        "        continue\n",
        "    neuron_ids.append(feature[node_id])\n",
        "  return neuron_ids\n",
        "\n",
        "def get_suffix_cluster(neuron_ids, neuron_sig,suffixes=train_suffixes):\n",
        "  # Get the cluster of inputs that such that all inputs in the cluster\n",
        "  # have provided on/off signature for the provided neurons.\n",
        "  #\n",
        "  # The returned cluster is an array of indices (into mnist.train.images).\n",
        "  return np.where((suffixes[:, neuron_ids] == neuron_sig).all(axis=1))[0]\n",
        "\n",
        "def is_consistent_cluster(cluster, predictions):\n",
        "  # Check if all inputs within the cluster have the same prediction.\n",
        "  # 'cluster' is an array of input ids.\n",
        "  pred = predictions[cluster[0]]\n",
        "  for i in cluster:\n",
        "    if predictions[i] != pred:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def is_misclassified(i):\n",
        "  return train_predictions[i] != mnist.train.labels[i]\n",
        "\n",
        "def visualize_conductances(img, label, neuron_ids, only_on=False):\n",
        "  # Visualize the conductances for the provided image.\n",
        "  # Args:\n",
        "  # - img: the provided mnist image\n",
        "  # - label: prediction label w.r.t. conductance must be computed\n",
        "  # - neuron_ids: list of neurons indices from the suffix tensor for which\n",
        "  #    conductances must be computed.\n",
        "  # - only_on: If True then conductance is computed only for those neurons\n",
        "  #    that are on for the given image. \n",
        "  vis = [mnist_to_pil_img(img)]\n",
        "  suffix = fingerprint_signature([img],curr_lay)\n",
        "  sumigc = 0.0\n",
        "  for i, id in enumerate(neuron_ids):\n",
        "    if only_on and suffix[i] != 1:\n",
        "      continue  \n",
        "    igc = conductance(img, label, neuron_id=id)\n",
        "    for indx in range(0,len(igc)):\n",
        "      print(indx,igc[indx]) ## a -ve gradient indicates the pixel value decreases the value of the neuron, making it go towards zero and negative\n",
        "      if ((suffix[i] == 0) and (igc[indx] > 0.0)): ## in a property if we want the o/p of a neuron to be zero, pixels which increase the neurons output should be given negative weightage\n",
        "        igc[indx] = -(igc[indx])\n",
        "\n",
        "    sumigc = sumigc + igc \n",
        "  \n",
        "  avgigc = sumigc / len(neuron_ids) ## gradient of each pixel w.r.t entire property - HIGHER VALUE INDICATES THE PIXEL HAS HIGHER CHANCE OF MAINTAINING THE PROPERTY\n",
        "  maxval = abs(max(avgigc, key=abs))\n",
        "  minval = abs(min(avgigc, key=abs))\n",
        "  threshold = (maxval - minval)/2.0\n",
        "  print(\"MAX ATR:\", maxval, \"MIN ATR:\", minval, \"THRESH:\", threshold)\n",
        "  avgigc = 1.0 * avgigc * (abs(avgigc) >= threshold) ## pixels with less significance in either SAT or DIS-SAT the property get blacked out\n",
        "  \n",
        "  \n",
        "  vis.append(visualize_attrs2(255*mnist_to_rgb(img), mnist_to_rgb(avgigc)))\n",
        "  return combine(vis)\n",
        "\n",
        "def get_invariant_inp(estimator, ref_id, suffixes):\n",
        "  # Returns an invariant (property) found w.r.t. the provided reference input\n",
        "  # Args\n",
        "  #  - inp: reference input, shape <784,>\n",
        "  # Returns:\n",
        "  #  - cluster: Indices of training inputs that satisfy the invariant\n",
        "  #  - neuron_id: A list of neurons such that all inputs that agree with\n",
        "  #    the reference input on the on/off status of these neurons have the\n",
        "  #    same prediction as the reference input.\n",
        "  ref_img = mnist_inp_images[ref_id]\n",
        "  ref_suffix = suffixes[ref_id]\n",
        "  print('PREFIX',ref_suffix)\n",
        "  neuron_ids = get_decision_path(estimator, ref_suffix)\n",
        "  print('NEURON IDS',neuron_ids)\n",
        "  neuron_sig = ref_suffix[neuron_ids]\n",
        "  print('NEURON SIGNATURE',neuron_sig)\n",
        "  cluster = get_suffix_cluster(neuron_ids, neuron_sig,suffixes)\n",
        "  imgs = []\n",
        "  cnt = 0\n",
        "  for indx1 in range(0,len(cluster)):\n",
        "    img = mnist.train.images(cluster[indx1])\n",
        "    fnd = 1\n",
        "    for i in range(0,len(img)):\n",
        "      if (ref_img[i] != img[i]):\n",
        "        fnd = 0\n",
        "        break\n",
        "    if (fnd == 1):\n",
        "        ref_id = cnt\n",
        "    cnt = cnt + 1\n",
        "    imgs.append(img)\n",
        "    \n",
        "  imgs_suffixes = fingerprint_signature(imgs,t_fc2)\n",
        "  ref_suffix = imgs_suffixes[ref_id]\n",
        "  print('PREFIX',ref_suffix)\n",
        "  neuron_ids = get_decision_path(estimator, ref_suffix)\n",
        "  print('NEURON IDS',neuron_ids)\n",
        "  neuron_sig = ref_suffix[neuron_ids]\n",
        "  print('NEURON SIGNATURE',neuron_sig)\n",
        "  cluster = get_suffix_cluster(neuron_ids, neuron_sig,imgs_suffixes)\n",
        "    \n",
        "  return cluster, neuron_ids, neuron_sig\n",
        "\n",
        "def get_invariant(estimator, ref_id):\n",
        "  # Returns an invariant found w.r.t. the provided reference input\n",
        "  # Args\n",
        "  #  - ref_id: Index (into mnist.train.images) of the reference input\n",
        "  # Returns:\n",
        "  #  - cluster: Indices of training inputs that satisfy the invariant\n",
        "  #  - neuron_id: A list of neurons such that all inputs that agree with\n",
        "  #    the reference input on the on/off status of these neurons have the\n",
        "  #    same prediction as the reference input.\n",
        "  ref_img = mnist.train.images[ref_id]\n",
        "  ref_suffix = train_suffixes[ref_id]\n",
        "  neuron_ids = get_decision_path(estimator, ref_suffix)\n",
        "  neuron_sig = ref_suffix[neuron_ids]\n",
        "  cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "  return cluster, neuron_ids, neuron_sig\n",
        "\n",
        "\n",
        "def get_all_invariants(estimator):\n",
        "  # Returns a dictionary mapping each decision tree prediction class\n",
        "  # to a list of invariants. Each invariant is specified as a triple:\n",
        "  # - neuron ids\n",
        "  # - neuron signature (for the neuron ids)\n",
        "  # - number of training samples that hit it\n",
        "  # The neuron ids and neuron signature can be supplied to get_suffix_cluster\n",
        "  # to obtain the cluster of training instances that hit the invariant.\n",
        "  def is_leaf(node):\n",
        "    return estimator.tree_.children_left[node] == estimator.tree_.children_right[node]\n",
        "\n",
        "  def left_child(node):\n",
        "    return estimator.tree_.children_left[node]\n",
        "\n",
        "  def right_child(node):\n",
        "    return estimator.tree_.children_right[node]\n",
        "  \n",
        "  def get_all_paths_rec(node):\n",
        "    # Returns a list of triples corresponding to paths\n",
        "    # in the decision tree. Each triple consists of\n",
        "    # - neurons encountered along the path\n",
        "    # - signature along the path\n",
        "    # - prediction class at the leaf\n",
        "    # - number of training samples that hit the path\n",
        "    # The prediction class and number of training samples\n",
        "    # are set to -1 when the leaf is \"impure\".\n",
        "    feature = estimator.tree_.feature\n",
        "    if is_leaf(node):\n",
        "      values = estimator.tree_.value[node][0]\n",
        "      if len(np.where(values != 0)[0]) == 1:\n",
        "        cl = estimator.classes_[np.where(values != 0)[0][0]]\n",
        "        nsamples = estimator.tree_.n_node_samples[node]\n",
        "      else:\n",
        "        # impure node\n",
        "        cl = -1\n",
        "        nsamples = -1\n",
        "      return [[[], [], cl, nsamples]]\n",
        "    # If it is not a leaf both left and right childs must exist\n",
        "    paths = [[[feature[node]] + p[0], [0] + p[1], p[2], p[3]] for p in get_all_paths_rec(left_child(node))]\n",
        "    paths += [[[feature[node]] + p[0], [1] + p[1], p[2], p[3]] for p in get_all_paths_rec(right_child(node))]\n",
        "    return paths\n",
        "  paths =  get_all_paths_rec(0)\n",
        "  print(\"Obtained all paths\")\n",
        "  invariants = {}\n",
        "  for p in tqdm(paths):\n",
        "    neuron_ids, neuron_sig, cl, nsamples = p\n",
        "    if cl not in invariants:\n",
        "      invariants[cl] = []\n",
        "    # cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "    invariants[cl].append([neuron_ids, neuron_sig, nsamples])\n",
        "  for cl in invariants.keys():\n",
        "    invariants[cl] = sorted(invariants[cl], key=operator.itemgetter(2), reverse=True)\n",
        "  return invariants\n",
        "\n",
        "\n",
        "def describe_cluster(cluster, neuron_ids, show_samples=False):\n",
        "  neuron_sig = train_suffixes[cluster[0]][neuron_ids]\n",
        "  print(\"Num neurons in invariant\", len(neuron_ids))\n",
        "  print(\"Neuron id and signature\")\n",
        "  \n",
        "  for i in range(0,len(neuron_ids)):\n",
        "    print(\"id:\", neuron_ids[i], \"sig:\", neuron_sig[i])\n",
        "  \n",
        "  print(\"Cluster size: \", len(cluster))\n",
        "  print(\"Num misclassified\", len([i for i in cluster if is_misclassified(i)]))\n",
        "  if show_samples:\n",
        "    for i in range(10):\n",
        "      images = []\n",
        "      for j in range(10):\n",
        "        if 10*i + j >= len(cluster):\n",
        "          break\n",
        "        images.append(mnist_to_pil_img(mnist.train.images[cluster[10*i+j]]))\n",
        "      if len(images) > 0:\n",
        "        show_img(combine(images))\n",
        "  \n",
        "\n",
        "def describe_invariants_all_labels(all_invariants,prevlayer = t_fc1,layer = t_fc2,suffixes=train_suffixes,COMMON=False, DEC_PREFX= False):\n",
        "  \n",
        "  print(\"PRINTING PURE RULES WITH SUPPORT MORE THAN 50 FOR EVERY LABEL:\");\n",
        "  for cl, invs in all_invariants.items():\n",
        "    if (cl == -1):\n",
        "      continue\n",
        "    \n",
        "    for indx in range (0, len(invs)):\n",
        "      inv = invs[indx]\n",
        "      cls = get_suffix_cluster(inv[0],inv[1],suffixes)\n",
        "      \n",
        "      neurons = inv[0]\n",
        "      signature = inv[1]\n",
        "\n",
        "      if (len(cls) <= 50):\n",
        "        continue\n",
        "      print(\"Class:\", cl, \", Rule:(neurons:\",inv[0],\",signature:\",inv[1],\"), Support:\",inv[2],\", Num misclassified\", len([i for i in cls if is_misclassified(i)]));\n",
        "\n",
        "      #print(\"PIXELS IMPACTING PROPERTY (conductance) for 10 inputs satisfy the property\")\n",
        "      interval = int(len(cls)/10)\n",
        "      #print(\"INTERVAL:\", interval)\n",
        "      i = 0\n",
        "      while (i < len(cls)):\n",
        "        ref_id = cls[i]\n",
        "        #if (is_misclassified(ref_id)):\n",
        "          #print(\"MISCLASSIFIED\")\n",
        "        #else:\n",
        "          #print(\"CORRECTLY CLASSIFIED\")\n",
        "        #show_img(visualize_conductances(mnist.train.images[ref_id], train_predictions[ref_id], inv[0], only_on=False))\n",
        "        i = i + interval\n",
        "      \n",
        "      print(\"INVOKE MARABOU:\",LAYER,\", \",cl); \n",
        "      invoke_marabou_chk(LAYER,neurons,signature,cl)\n",
        "\n",
        "      if (COMMON == True):\n",
        "          common_nodes(cls,suffixes)\n",
        "\n",
        "      if (DEC_PREFX == True):\n",
        "          decision_prefs(cls,suffixes)\n",
        "\n",
        "  return\n",
        "  \n",
        "def common_nodes(cls,suffixes):\n",
        "    cnt = 0\n",
        "    common = np.zeros(10,dtype=int)\n",
        "    prev = np.zeros(10,dtype=int)\n",
        "    \n",
        "    for indx in range(0, len(cls)):\n",
        "        i = cls[indx]\n",
        "        cnt = cnt + 1\n",
        "        for j in range(0,len(suffixes[i])):\n",
        "          if (common[j] == -1):\n",
        "             continue\n",
        "          if ((indx != 0) and (suffixes[i][j] != prev[j])):\n",
        "             common[j] = -1\n",
        "          else:\n",
        "             common[j] = suffixes[i][j]\n",
        "          prev[j] = suffixes[i][j]\n",
        "\n",
        "\n",
        "    print('COMMON NODES IN CLUSTER for CLASS:',cl,cnt)\n",
        "    com = []\n",
        "    for k in range(0,len(common)):\n",
        "        if (common[k] != -1):\n",
        "           com.append((k,common[k]))\n",
        "    print(com)\n",
        "\n",
        "    return\n",
        "    \n",
        "def decision_prefs(cls,suffixes):\n",
        "    images = mnist.train.images\n",
        "    imgsCom = []\n",
        "    imgs = []\n",
        "    for indx in range(0, len(cls)):\n",
        "        print('IMG:')\n",
        "        print(list(zip(images[cls[indx]])))\n",
        "        imgs.append(images[cls[indx]])\n",
        "        imgsCom.append(images[cls[indx]])\n",
        "            \n",
        "    dec_prefixes= fingerprint_signature(imgs,layer)\n",
        "    prefixes = []\n",
        "    for indx in range(0,len(dec_prefixes)):\n",
        "       dec_pref = dec_prefixes[indx]\n",
        "    \n",
        "       match = 0\n",
        "       for indx1 in range(0, len(prefixes)):\n",
        "          match = 1\n",
        "          for i in range(0,len(prefixes[indx1])):\n",
        "             if (dec_pref[i] != prefixes[indx1][i]):\n",
        "                match = 0\n",
        "                break\n",
        "          if (match == 1):\n",
        "             break\n",
        "    \n",
        "       if (match == 0):\n",
        "          prefixes.append(dec_pref)\n",
        "    \n",
        "    print('DECISION PREFIXES IN CLUSTER for CLASS:',cl,cnt)\n",
        "    for k in range(0,len(prefixes)):\n",
        "      print(prefixes[k])\n",
        "\n",
        "    return\n",
        "    \n",
        "  \n",
        "  #print('LAYER INPS:')\n",
        "  #min = np.zeros(10)\n",
        "  #max = np.zeros(10)\n",
        "  #for dim in range(0,10):\n",
        "  #    min[dim] = 1000\n",
        "  #    max[dim] = -1000\n",
        "          \n",
        "  #prevlayer_vals = get_prediction(imgsCom,prevlayer)      \n",
        "  #print('MIN, MAX LAYER INPS:',len(prevlayer_vals))\n",
        "  #for i in range(0,len(prevlayer_vals)):\n",
        "  #    if (i == 0):\n",
        "  #      print(zip(prevlayer_vals[i]))\n",
        "  #    for dim in range(0,10):\n",
        "  #        if ( prevlayer_vals[i][dim] < min[dim]):\n",
        "  #            min[dim] = prevlayer_vals[i][dim]\n",
        "  #        if ( prevlayer_vals[i][dim] > max[dim]):\n",
        "  #            max[dim] = prevlayer_vals[i][dim]\n",
        "    \n",
        "  #print('MIN')\n",
        "  #print(zip(min))\n",
        "  #print('MAX')\n",
        "  #print(zip(max))    \n",
        "    \n",
        "  #df = pd.DataFrame(df, columns=['Prediction Class', 'Num Instances', 'Num Invariants', 'Num Invariants with cluster size >= 10', 'Size of largest invariant cluster'])\n",
        "  #df = pd.DataFrame(df,columns=['Pred Class','Total #Neurons','# Invariants'])\n",
        "  #return df\n",
        "\n",
        "\n",
        "def describe_all_invariants(all_invariants):\n",
        "  df = []\n",
        "  for cl, invs in all_invariants.iteritems(): \n",
        "    inv = invs[0]\n",
        "    clus = get_suffix_cluster(inv[0],inv[1])\n",
        "    #print(len(clus))\n",
        "    misCl = 0\n",
        "    for i in range(0,len(clus)):\n",
        "      indx = clus[i]\n",
        "      if (is_misclassified(indx) == True):\n",
        "        misCl = misCl + 1\n",
        "    print('class:',cl,',masSup:',inv[2],',#misCl:',misCl)\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOUniL-T5-Sa",
        "colab_type": "code",
        "outputId": "95cd6112-88d1-4f43-d998-b0fb2537cffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"layer:\", LAYER)\n",
        "#\",label:\", LABEL)\n",
        "invariants = get_all_invariants(basic_estimator)\n",
        "describe_invariants_all_labels(invariants,prev_lay,curr_lay)\n",
        "\n",
        "\n",
        "#ref_id = 3\n",
        "#print(\"### Cluster ###\")\n",
        "#cluster, neuron_ids, neuron_sig = get_invariant(basic_estimator, ref_id)\n",
        "#describe_cluster(cluster, neuron_ids, show_samples=True)\n",
        "\n",
        "# Visualize  10 inputs in the cluster\n",
        "#for i in cluster[:10]:\n",
        "#show_img(visualize_conductances(mnist.train.images[ref_id], train_predictions[ref_id], neuron_ids, only_on=False))\n",
        "    \n",
        "#print \"###  BASIC DECISION TREE ###\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Print invariant stats\n",
        "#describe_all_invariants(invariants,t_fc1,t_fc1)\n",
        "#describe_all_invariantsFull(invariants,True,t_fc2)\n",
        "#print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "#print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "\n",
        "### OTHER INPUTS ####\n",
        "#for indxi in range(0,len(mnist_inp_images)):\n",
        "#    inp = mnist_inp_images[indxi]\n",
        "#    lab = mnist_inp_labels[indxi] \n",
        "#    print(indxi, lab)\n",
        "#    describe_input_INP(indxi)\n",
        "##    print \"###  BASIC DECISION TREE Cluster ###\"\n",
        "#    if (indxi == 0):\n",
        "#       cluster, neuron_ids, neuron_sig = get_invariant_inp(basic_estimator,indxi )\n",
        "#       print('LEN CLUSTER:',len(cluster),is_consistent_cluster(cluster,train_predictions))\n",
        " #  # describe_cluster(cluster, neuron_ids)\n",
        "\n",
        "#    print \"###  RELATIVE DECISION TREE Cluster ###\"\n",
        "#    cluster, neuron_ids, neuron_sig = get_invariant(relative_estimators[lab], indxi)\n",
        "#    describe_cluster(cluster, neuron_ids)\n",
        "   \n",
        "    \n",
        "#print \"###  BASIC DECISION TREE ###\"\n",
        "#invariants = get_all_invariants(basic_estimator)\n",
        "## Print invariant stats\n",
        "#describe_all_invariants1(invariants)\n",
        "#describe_all_invariantsFull(invariants,True,t_fc2)\n",
        "#print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "#print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "\n",
        "\n",
        "#for cl in invariants.keys():\n",
        "#  print(cl,invariants[cl])\n",
        "  \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 0\"\n",
        "#invariants = get_all_invariants(relative_estimators[0])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "\n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 1\"\n",
        "#invariants = get_all_invariants(relative_estimators[1])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "    \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 2\"\n",
        "#invariants = get_all_invariants(relative_estimators[2])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "    \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 3\"\n",
        "#invariants = get_all_invariants(relative_estimators[3])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "    \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 4\"\n",
        "#invariants = get_all_invariants(relative_estimators[4])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "    \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 5\"\n",
        "#invariants = get_all_invariants(relative_estimators[5])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "\n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 6\"\n",
        "#invariants = get_all_invariants(relative_estimators[6])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "    \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 7\"\n",
        "#invariants = get_all_invariants(relative_estimators[7])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "    \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 8\"\n",
        "#invariants = get_all_invariants(relative_estimators[8])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "#for cl in invariants.keys():\n",
        "#   print(cl,invariants[cl])\n",
        "    \n",
        "#print \"###  RELATIVE DECISION TREE for CLASS 9\"\n",
        "#invariants = get_all_invariants(relative_estimators[9])\n",
        "#df = describe_all_invariants(invariants)\n",
        "##print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "##print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "##for cl in invariants.keys():\n",
        "##   print(cl,invariants[cl])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 59/59 [00:00<00:00, 158732.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "layer: 7\n",
            "Obtained all paths\n",
            "PRINTING PURE RULES WITH SUPPORT MORE THAN 50 FOR EVERY LABEL:\n",
            "Class: 6 , Rule:(neurons: [9, 3, 2, 7, 1, 4] ,signature: [1, 1, 0, 1, 0, 1] ), Support: 4708 , Num misclassified 31\n",
            "INVOKE MARABOU: 7 ,  6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Network: ./mnist_10_layer.nnet\n",
            "Number of layers: 12. Input layer size: 784. Output layer size: 10. Number of ReLUs: 100\n",
            "Total number of variables: 994\n",
            "Property: ./property.txt\n",
            "\n",
            "Engine::processInputQuery: Input query (before preprocessing): 111 equations, 994 variables\n",
            "Engine::processInputQuery: Input query (after preprocessing): 211 equations, 1089 variables\n",
            "\n",
            "Input bounds:\n",
            "\tx0: [  0.0000,   1.0000] \n",
            "\tx1: [  0.0000,   1.0000] \n",
            "\tx2: [  0.0000,   1.0000] \n",
            "\tx3: [  0.0000,   1.0000] \n",
            "\tx4: [  0.0000,   1.0000] \n",
            "\tx5: [  0.0000,   1.0000] \n",
            "\tx6: [  0.0000,   1.0000] \n",
            "\tx7: [  0.0000,   1.0000] \n",
            "\tx8: [  0.0000,   1.0000] \n",
            "\tx9: [  0.0000,   1.0000] \n",
            "\tx10: [  0.0000,   1.0000] \n",
            "\tx11: [  0.0000,   1.0000] \n",
            "\tx12: [  0.0000,   1.0000] \n",
            "\tx13: [  0.0000,   1.0000] \n",
            "\tx14: [  0.0000,   1.0000] \n",
            "\tx15: [  0.0000,   1.0000] \n",
            "\tx16: [  0.0000,   1.0000] \n",
            "\tx17: [  0.0000,   1.0000] \n",
            "\tx18: [  0.0000,   1.0000] \n",
            "\tx19: [  0.0000,   1.0000] \n",
            "\tx20: [  0.0000,   1.0000] \n",
            "\tx21: [  0.0000,   1.0000] \n",
            "\tx22: [  0.0000,   1.0000] \n",
            "\tx23: [  0.0000,   1.0000] \n",
            "\tx24: [  0.0000,   1.0000] \n",
            "\tx25: [  0.0000,   1.0000] \n",
            "\tx26: [  0.0000,   1.0000] \n",
            "\tx27: [  0.0000,   1.0000] \n",
            "\tx28: [  0.0000,   1.0000] \n",
            "\tx29: [  0.0000,   1.0000] \n",
            "\tx30: [  0.0000,   1.0000] \n",
            "\tx31: [  0.0000,   1.0000] \n",
            "\tx32: [  0.0000,   1.0000] \n",
            "\tx33: [  0.0000,   1.0000] \n",
            "\tx34: [  0.0000,   1.0000] \n",
            "\tx35: [  0.0000,   1.0000] \n",
            "\tx36: [  0.0000,   1.0000] \n",
            "\tx37: [  0.0000,   1.0000] \n",
            "\tx38: [  0.0000,   1.0000] \n",
            "\tx39: [  0.0000,   1.0000] \n",
            "\tx40: [  0.0000,   1.0000] \n",
            "\tx41: [  0.0000,   1.0000] \n",
            "\tx42: [  0.0000,   1.0000] \n",
            "\tx43: [  0.0000,   1.0000] \n",
            "\tx44: [  0.0000,   1.0000] \n",
            "\tx45: [  0.0000,   1.0000] \n",
            "\tx46: [  0.0000,   1.0000] \n",
            "\tx47: [  0.0000,   1.0000] \n",
            "\tx48: [  0.0000,   1.0000] \n",
            "\tx49: [  0.0000,   1.0000] \n",
            "\tx50: [  0.0000,   1.0000] \n",
            "\tx51: [  0.0000,   1.0000] \n",
            "\tx52: [  0.0000,   1.0000] \n",
            "\tx53: [  0.0000,   1.0000] \n",
            "\tx54: [  0.0000,   1.0000] \n",
            "\tx55: [  0.0000,   1.0000] \n",
            "\tx56: [  0.0000,   1.0000] \n",
            "\tx57: [  0.0000,   1.0000] \n",
            "\tx58: [  0.0000,   1.0000] \n",
            "\tx59: [  0.0000,   1.0000] \n",
            "\tx60: [  0.0000,   1.0000] \n",
            "\tx61: [  0.0000,   1.0000] \n",
            "\tx62: [  0.0000,   1.0000] \n",
            "\tx63: [  0.0000,   1.0000] \n",
            "\tx64: [  0.0000,   1.0000] \n",
            "\tx65: [  0.0000,   1.0000] \n",
            "\tx66: [  0.0000,   1.0000] \n",
            "\tx67: [  0.0000,   1.0000] \n",
            "\tx68: [  0.0000,   1.0000] \n",
            "\tx69: [  0.0000,   1.0000] \n",
            "\tx70: [  0.0000,   1.0000] \n",
            "\tx71: [  0.0000,   1.0000] \n",
            "\tx72: [  0.0000,   1.0000] \n",
            "\tx73: [  0.0000,   1.0000] \n",
            "\tx74: [  0.0000,   1.0000] \n",
            "\tx75: [  0.0000,   1.0000] \n",
            "\tx76: [  0.0000,   1.0000] \n",
            "\tx77: [  0.0000,   1.0000] \n",
            "\tx78: [  0.0000,   1.0000] \n",
            "\tx79: [  0.0000,   1.0000] \n",
            "\tx80: [  0.0000,   1.0000] \n",
            "\tx81: [  0.0000,   1.0000] \n",
            "\tx82: [  0.0000,   1.0000] \n",
            "\tx83: [  0.0000,   1.0000] \n",
            "\tx84: [  0.0000,   1.0000] \n",
            "\tx85: [  0.0000,   1.0000] \n",
            "\tx86: [  0.0000,   1.0000] \n",
            "\tx87: [  0.0000,   1.0000] \n",
            "\tx88: [  0.0000,   1.0000] \n",
            "\tx89: [  0.0000,   1.0000] \n",
            "\tx90: [  0.0000,   1.0000] \n",
            "\tx91: [  0.0000,   1.0000] \n",
            "\tx92: [  0.0000,   1.0000] \n",
            "\tx93: [  0.0000,   1.0000] \n",
            "\tx94: [  0.0000,   1.0000] \n",
            "\tx95: [  0.0000,   1.0000] \n",
            "\tx96: [  0.0000,   1.0000] \n",
            "\tx97: [  0.0000,   1.0000] \n",
            "\tx98: [  0.0000,   1.0000] \n",
            "\tx99: [  0.0000,   1.0000] \n",
            "\tx100: [  0.0000,   1.0000] \n",
            "\tx101: [  0.0000,   1.0000] \n",
            "\tx102: [  0.0000,   1.0000] \n",
            "\tx103: [  0.0000,   1.0000] \n",
            "\tx104: [  0.0000,   1.0000] \n",
            "\tx105: [  0.0000,   1.0000] \n",
            "\tx106: [  0.0000,   1.0000] \n",
            "\tx107: [  0.0000,   1.0000] \n",
            "\tx108: [  0.0000,   1.0000] \n",
            "\tx109: [  0.0000,   1.0000] \n",
            "\tx110: [  0.0000,   1.0000] \n",
            "\tx111: [  0.0000,   1.0000] \n",
            "\tx112: [  0.0000,   1.0000] \n",
            "\tx113: [  0.0000,   1.0000] \n",
            "\tx114: [  0.0000,   1.0000] \n",
            "\tx115: [  0.0000,   1.0000] \n",
            "\tx116: [  0.0000,   1.0000] \n",
            "\tx117: [  0.0000,   1.0000] \n",
            "\tx118: [  0.0000,   1.0000] \n",
            "\tx119: [  0.0000,   1.0000] \n",
            "\tx120: [  0.0000,   1.0000] \n",
            "\tx121: [  0.0000,   1.0000] \n",
            "\tx122: [  0.0000,   1.0000] \n",
            "\tx123: [  0.0000,   1.0000] \n",
            "\tx124: [  0.0000,   1.0000] \n",
            "\tx125: [  0.0000,   1.0000] \n",
            "\tx126: [  0.0000,   1.0000] \n",
            "\tx127: [  0.0000,   1.0000] \n",
            "\tx128: [  0.0000,   1.0000] \n",
            "\tx129: [  0.0000,   1.0000] \n",
            "\tx130: [  0.0000,   1.0000] \n",
            "\tx131: [  0.0000,   1.0000] \n",
            "\tx132: [  0.0000,   1.0000] \n",
            "\tx133: [  0.0000,   1.0000] \n",
            "\tx134: [  0.0000,   1.0000] \n",
            "\tx135: [  0.0000,   1.0000] \n",
            "\tx136: [  0.0000,   1.0000] \n",
            "\tx137: [  0.0000,   1.0000] \n",
            "\tx138: [  0.0000,   1.0000] \n",
            "\tx139: [  0.0000,   1.0000] \n",
            "\tx140: [  0.0000,   1.0000] \n",
            "\tx141: [  0.0000,   1.0000] \n",
            "\tx142: [  0.0000,   1.0000] \n",
            "\tx143: [  0.0000,   1.0000] \n",
            "\tx144: [  0.0000,   1.0000] \n",
            "\tx145: [  0.0000,   1.0000] \n",
            "\tx146: [  0.0000,   1.0000] \n",
            "\tx147: [  0.0000,   1.0000] \n",
            "\tx148: [  0.0000,   1.0000] \n",
            "\tx149: [  0.0000,   1.0000] \n",
            "\tx150: [  0.0000,   1.0000] \n",
            "\tx151: [  0.0000,   1.0000] \n",
            "\tx152: [  0.0000,   1.0000] \n",
            "\tx153: [  0.0000,   1.0000] \n",
            "\tx154: [  0.0000,   1.0000] \n",
            "\tx155: [  0.0000,   1.0000] \n",
            "\tx156: [  0.0000,   1.0000] \n",
            "\tx157: [  0.0000,   1.0000] \n",
            "\tx158: [  0.0000,   1.0000] \n",
            "\tx159: [  0.0000,   1.0000] \n",
            "\tx160: [  0.0000,   1.0000] \n",
            "\tx161: [  0.0000,   1.0000] \n",
            "\tx162: [  0.0000,   1.0000] \n",
            "\tx163: [  0.0000,   1.0000] \n",
            "\tx164: [  0.0000,   1.0000] \n",
            "\tx165: [  0.0000,   1.0000] \n",
            "\tx166: [  0.0000,   1.0000] \n",
            "\tx167: [  0.0000,   1.0000] \n",
            "\tx168: [  0.0000,   1.0000] \n",
            "\tx169: [  0.0000,   1.0000] \n",
            "\tx170: [  0.0000,   1.0000] \n",
            "\tx171: [  0.0000,   1.0000] \n",
            "\tx172: [  0.0000,   1.0000] \n",
            "\tx173: [  0.0000,   1.0000] \n",
            "\tx174: [  0.0000,   1.0000] \n",
            "\tx175: [  0.0000,   1.0000] \n",
            "\tx176: [  0.0000,   1.0000] \n",
            "\tx177: [  0.0000,   1.0000] \n",
            "\tx178: [  0.0000,   1.0000] \n",
            "\tx179: [  0.0000,   1.0000] \n",
            "\tx180: [  0.0000,   1.0000] \n",
            "\tx181: [  0.0000,   1.0000] \n",
            "\tx182: [  0.0000,   1.0000] \n",
            "\tx183: [  0.0000,   1.0000] \n",
            "\tx184: [  0.0000,   1.0000] \n",
            "\tx185: [  0.0000,   1.0000] \n",
            "\tx186: [  0.0000,   1.0000] \n",
            "\tx187: [  0.0000,   1.0000] \n",
            "\tx188: [  0.0000,   1.0000] \n",
            "\tx189: [  0.0000,   1.0000] \n",
            "\tx190: [  0.0000,   1.0000] \n",
            "\tx191: [  0.0000,   1.0000] \n",
            "\tx192: [  0.0000,   1.0000] \n",
            "\tx193: [  0.0000,   1.0000] \n",
            "\tx194: [  0.0000,   1.0000] \n",
            "\tx195: [  0.0000,   1.0000] \n",
            "\tx196: [  0.0000,   1.0000] \n",
            "\tx197: [  0.0000,   1.0000] \n",
            "\tx198: [  0.0000,   1.0000] \n",
            "\tx199: [  0.0000,   1.0000] \n",
            "\tx200: [  0.0000,   1.0000] \n",
            "\tx201: [  0.0000,   1.0000] \n",
            "\tx202: [  0.0000,   1.0000] \n",
            "\tx203: [  0.0000,   1.0000] \n",
            "\tx204: [  0.0000,   1.0000] \n",
            "\tx205: [  0.0000,   1.0000] \n",
            "\tx206: [  0.0000,   1.0000] \n",
            "\tx207: [  0.0000,   1.0000] \n",
            "\tx208: [  0.0000,   1.0000] \n",
            "\tx209: [  0.0000,   1.0000] \n",
            "\tx210: [  0.0000,   1.0000] \n",
            "\tx211: [  0.0000,   1.0000] \n",
            "\tx212: [  0.0000,   1.0000] \n",
            "\tx213: [  0.0000,   1.0000] \n",
            "\tx214: [  0.0000,   1.0000] \n",
            "\tx215: [  0.0000,   1.0000] \n",
            "\tx216: [  0.0000,   1.0000] \n",
            "\tx217: [  0.0000,   1.0000] \n",
            "\tx218: [  0.0000,   1.0000] \n",
            "\tx219: [  0.0000,   1.0000] \n",
            "\tx220: [  0.0000,   1.0000] \n",
            "\tx221: [  0.0000,   1.0000] \n",
            "\tx222: [  0.0000,   1.0000] \n",
            "\tx223: [  0.0000,   1.0000] \n",
            "\tx224: [  0.0000,   1.0000] \n",
            "\tx225: [  0.0000,   1.0000] \n",
            "\tx226: [  0.0000,   1.0000] \n",
            "\tx227: [  0.0000,   1.0000] \n",
            "\tx228: [  0.0000,   1.0000] \n",
            "\tx229: [  0.0000,   1.0000] \n",
            "\tx230: [  0.0000,   1.0000] \n",
            "\tx231: [  0.0000,   1.0000] \n",
            "\tx232: [  0.0000,   1.0000] \n",
            "\tx233: [  0.0000,   1.0000] \n",
            "\tx234: [  0.0000,   1.0000] \n",
            "\tx235: [  0.0000,   1.0000] \n",
            "\tx236: [  0.0000,   1.0000] \n",
            "\tx237: [  0.0000,   1.0000] \n",
            "\tx238: [  0.0000,   1.0000] \n",
            "\tx239: [  0.0000,   1.0000] \n",
            "\tx240: [  0.0000,   1.0000] \n",
            "\tx241: [  0.0000,   1.0000] \n",
            "\tx242: [  0.0000,   1.0000] \n",
            "\tx243: [  0.0000,   1.0000] \n",
            "\tx244: [  0.0000,   1.0000] \n",
            "\tx245: [  0.0000,   1.0000] \n",
            "\tx246: [  0.0000,   1.0000] \n",
            "\tx247: [  0.0000,   1.0000] \n",
            "\tx248: [  0.0000,   1.0000] \n",
            "\tx249: [  0.0000,   1.0000] \n",
            "\tx250: [  0.0000,   1.0000] \n",
            "\tx251: [  0.0000,   1.0000] \n",
            "\tx252: [  0.0000,   1.0000] \n",
            "\tx253: [  0.0000,   1.0000] \n",
            "\tx254: [  0.0000,   1.0000] \n",
            "\tx255: [  0.0000,   1.0000] \n",
            "\tx256: [  0.0000,   1.0000] \n",
            "\tx257: [  0.0000,   1.0000] \n",
            "\tx258: [  0.0000,   1.0000] \n",
            "\tx259: [  0.0000,   1.0000] \n",
            "\tx260: [  0.0000,   1.0000] \n",
            "\tx261: [  0.0000,   1.0000] \n",
            "\tx262: [  0.0000,   1.0000] \n",
            "\tx263: [  0.0000,   1.0000] \n",
            "\tx264: [  0.0000,   1.0000] \n",
            "\tx265: [  0.0000,   1.0000] \n",
            "\tx266: [  0.0000,   1.0000] \n",
            "\tx267: [  0.0000,   1.0000] \n",
            "\tx268: [  0.0000,   1.0000] \n",
            "\tx269: [  0.0000,   1.0000] \n",
            "\tx270: [  0.0000,   1.0000] \n",
            "\tx271: [  0.0000,   1.0000] \n",
            "\tx272: [  0.0000,   1.0000] \n",
            "\tx273: [  0.0000,   1.0000] \n",
            "\tx274: [  0.0000,   1.0000] \n",
            "\tx275: [  0.0000,   1.0000] \n",
            "\tx276: [  0.0000,   1.0000] \n",
            "\tx277: [  0.0000,   1.0000] \n",
            "\tx278: [  0.0000,   1.0000] \n",
            "\tx279: [  0.0000,   1.0000] \n",
            "\tx280: [  0.0000,   1.0000] \n",
            "\tx281: [  0.0000,   1.0000] \n",
            "\tx282: [  0.0000,   1.0000] \n",
            "\tx283: [  0.0000,   1.0000] \n",
            "\tx284: [  0.0000,   1.0000] \n",
            "\tx285: [  0.0000,   1.0000] \n",
            "\tx286: [  0.0000,   1.0000] \n",
            "\tx287: [  0.0000,   1.0000] \n",
            "\tx288: [  0.0000,   1.0000] \n",
            "\tx289: [  0.0000,   1.0000] \n",
            "\tx290: [  0.0000,   1.0000] \n",
            "\tx291: [  0.0000,   1.0000] \n",
            "\tx292: [  0.0000,   1.0000] \n",
            "\tx293: [  0.0000,   1.0000] \n",
            "\tx294: [  0.0000,   1.0000] \n",
            "\tx295: [  0.0000,   1.0000] \n",
            "\tx296: [  0.0000,   1.0000] \n",
            "\tx297: [  0.0000,   1.0000] \n",
            "\tx298: [  0.0000,   1.0000] \n",
            "\tx299: [  0.0000,   1.0000] \n",
            "\tx300: [  0.0000,   1.0000] \n",
            "\tx301: [  0.0000,   1.0000] \n",
            "\tx302: [  0.0000,   1.0000] \n",
            "\tx303: [  0.0000,   1.0000] \n",
            "\tx304: [  0.0000,   1.0000] \n",
            "\tx305: [  0.0000,   1.0000] \n",
            "\tx306: [  0.0000,   1.0000] \n",
            "\tx307: [  0.0000,   1.0000] \n",
            "\tx308: [  0.0000,   1.0000] \n",
            "\tx309: [  0.0000,   1.0000] \n",
            "\tx310: [  0.0000,   1.0000] \n",
            "\tx311: [  0.0000,   1.0000] \n",
            "\tx312: [  0.0000,   1.0000] \n",
            "\tx313: [  0.0000,   1.0000] \n",
            "\tx314: [  0.0000,   1.0000] \n",
            "\tx315: [  0.0000,   1.0000] \n",
            "\tx316: [  0.0000,   1.0000] \n",
            "\tx317: [  0.0000,   1.0000] \n",
            "\tx318: [  0.0000,   1.0000] \n",
            "\tx319: [  0.0000,   1.0000] \n",
            "\tx320: [  0.0000,   1.0000] \n",
            "\tx321: [  0.0000,   1.0000] \n",
            "\tx322: [  0.0000,   1.0000] \n",
            "\tx323: [  0.0000,   1.0000] \n",
            "\tx324: [  0.0000,   1.0000] \n",
            "\tx325: [  0.0000,   1.0000] \n",
            "\tx326: [  0.0000,   1.0000] \n",
            "\tx327: [  0.0000,   1.0000] \n",
            "\tx328: [  0.0000,   1.0000] \n",
            "\tx329: [  0.0000,   1.0000] \n",
            "\tx330: [  0.0000,   1.0000] \n",
            "\tx331: [  0.0000,   1.0000] \n",
            "\tx332: [  0.0000,   1.0000] \n",
            "\tx333: [  0.0000,   1.0000] \n",
            "\tx334: [  0.0000,   1.0000] \n",
            "\tx335: [  0.0000,   1.0000] \n",
            "\tx336: [  0.0000,   1.0000] \n",
            "\tx337: [  0.0000,   1.0000] \n",
            "\tx338: [  0.0000,   1.0000] \n",
            "\tx339: [  0.0000,   1.0000] \n",
            "\tx340: [  0.0000,   1.0000] \n",
            "\tx341: [  0.0000,   1.0000] \n",
            "\tx342: [  0.0000,   1.0000] \n",
            "\tx343: [  0.0000,   1.0000] \n",
            "\tx344: [  0.0000,   1.0000] \n",
            "\tx345: [  0.0000,   1.0000] \n",
            "\tx346: [  0.0000,   1.0000] \n",
            "\tx347: [  0.0000,   1.0000] \n",
            "\tx348: [  0.0000,   1.0000] \n",
            "\tx349: [  0.0000,   1.0000] \n",
            "\tx350: [  0.0000,   1.0000] \n",
            "\tx351: [  0.0000,   1.0000] \n",
            "\tx352: [  0.0000,   1.0000] \n",
            "\tx353: [  0.0000,   1.0000] \n",
            "\tx354: [  0.0000,   1.0000] \n",
            "\tx355: [  0.0000,   1.0000] \n",
            "\tx356: [  0.0000,   1.0000] \n",
            "\tx357: [  0.0000,   1.0000] \n",
            "\tx358: [  0.0000,   1.0000] \n",
            "\tx359: [  0.0000,   1.0000] \n",
            "\tx360: [  0.0000,   1.0000] \n",
            "\tx361: [  0.0000,   1.0000] \n",
            "\tx362: [  0.0000,   1.0000] \n",
            "\tx363: [  0.0000,   1.0000] \n",
            "\tx364: [  0.0000,   1.0000] \n",
            "\tx365: [  0.0000,   1.0000] \n",
            "\tx366: [  0.0000,   1.0000] \n",
            "\tx367: [  0.0000,   1.0000] \n",
            "\tx368: [  0.0000,   1.0000] \n",
            "\tx369: [  0.0000,   1.0000] \n",
            "\tx370: [  0.0000,   1.0000] \n",
            "\tx371: [  0.0000,   1.0000] \n",
            "\tx372: [  0.0000,   1.0000] \n",
            "\tx373: [  0.0000,   1.0000] \n",
            "\tx374: [  0.0000,   1.0000] \n",
            "\tx375: [  0.0000,   1.0000] \n",
            "\tx376: [  0.0000,   1.0000] \n",
            "\tx377: [  0.0000,   1.0000] \n",
            "\tx378: [  0.0000,   1.0000] \n",
            "\tx379: [  0.0000,   1.0000] \n",
            "\tx380: [  0.0000,   1.0000] \n",
            "\tx381: [  0.0000,   1.0000] \n",
            "\tx382: [  0.0000,   1.0000] \n",
            "\tx383: [  0.0000,   1.0000] \n",
            "\tx384: [  0.0000,   1.0000] \n",
            "\tx385: [  0.0000,   1.0000] \n",
            "\tx386: [  0.0000,   1.0000] \n",
            "\tx387: [  0.0000,   1.0000] \n",
            "\tx388: [  0.0000,   1.0000] \n",
            "\tx389: [  0.0000,   1.0000] \n",
            "\tx390: [  0.0000,   1.0000] \n",
            "\tx391: [  0.0000,   1.0000] \n",
            "\tx392: [  0.0000,   1.0000] \n",
            "\tx393: [  0.0000,   1.0000] \n",
            "\tx394: [  0.0000,   1.0000] \n",
            "\tx395: [  0.0000,   1.0000] \n",
            "\tx396: [  0.0000,   1.0000] \n",
            "\tx397: [  0.0000,   1.0000] \n",
            "\tx398: [  0.0000,   1.0000] \n",
            "\tx399: [  0.0000,   1.0000] \n",
            "\tx400: [  0.0000,   1.0000] \n",
            "\tx401: [  0.0000,   1.0000] \n",
            "\tx402: [  0.0000,   1.0000] \n",
            "\tx403: [  0.0000,   1.0000] \n",
            "\tx404: [  0.0000,   1.0000] \n",
            "\tx405: [  0.0000,   1.0000] \n",
            "\tx406: [  0.0000,   1.0000] \n",
            "\tx407: [  0.0000,   1.0000] \n",
            "\tx408: [  0.0000,   1.0000] \n",
            "\tx409: [  0.0000,   1.0000] \n",
            "\tx410: [  0.0000,   1.0000] \n",
            "\tx411: [  0.0000,   1.0000] \n",
            "\tx412: [  0.0000,   1.0000] \n",
            "\tx413: [  0.0000,   1.0000] \n",
            "\tx414: [  0.0000,   1.0000] \n",
            "\tx415: [  0.0000,   1.0000] \n",
            "\tx416: [  0.0000,   1.0000] \n",
            "\tx417: [  0.0000,   1.0000] \n",
            "\tx418: [  0.0000,   1.0000] \n",
            "\tx419: [  0.0000,   1.0000] \n",
            "\tx420: [  0.0000,   1.0000] \n",
            "\tx421: [  0.0000,   1.0000] \n",
            "\tx422: [  0.0000,   1.0000] \n",
            "\tx423: [  0.0000,   1.0000] \n",
            "\tx424: [  0.0000,   1.0000] \n",
            "\tx425: [  0.0000,   1.0000] \n",
            "\tx426: [  0.0000,   1.0000] \n",
            "\tx427: [  0.0000,   1.0000] \n",
            "\tx428: [  0.0000,   1.0000] \n",
            "\tx429: [  0.0000,   1.0000] \n",
            "\tx430: [  0.0000,   1.0000] \n",
            "\tx431: [  0.0000,   1.0000] \n",
            "\tx432: [  0.0000,   1.0000] \n",
            "\tx433: [  0.0000,   1.0000] \n",
            "\tx434: [  0.0000,   1.0000] \n",
            "\tx435: [  0.0000,   1.0000] \n",
            "\tx436: [  0.0000,   1.0000] \n",
            "\tx437: [  0.0000,   1.0000] \n",
            "\tx438: [  0.0000,   1.0000] \n",
            "\tx439: [  0.0000,   1.0000] \n",
            "\tx440: [  0.0000,   1.0000] \n",
            "\tx441: [  0.0000,   1.0000] \n",
            "\tx442: [  0.0000,   1.0000] \n",
            "\tx443: [  0.0000,   1.0000] \n",
            "\tx444: [  0.0000,   1.0000] \n",
            "\tx445: [  0.0000,   1.0000] \n",
            "\tx446: [  0.0000,   1.0000] \n",
            "\tx447: [  0.0000,   1.0000] \n",
            "\tx448: [  0.0000,   1.0000] \n",
            "\tx449: [  0.0000,   1.0000] \n",
            "\tx450: [  0.0000,   1.0000] \n",
            "\tx451: [  0.0000,   1.0000] \n",
            "\tx452: [  0.0000,   1.0000] \n",
            "\tx453: [  0.0000,   1.0000] \n",
            "\tx454: [  0.0000,   1.0000] \n",
            "\tx455: [  0.0000,   1.0000] \n",
            "\tx456: [  0.0000,   1.0000] \n",
            "\tx457: [  0.0000,   1.0000] \n",
            "\tx458: [  0.0000,   1.0000] \n",
            "\tx459: [  0.0000,   1.0000] \n",
            "\tx460: [  0.0000,   1.0000] \n",
            "\tx461: [  0.0000,   1.0000] \n",
            "\tx462: [  0.0000,   1.0000] \n",
            "\tx463: [  0.0000,   1.0000] \n",
            "\tx464: [  0.0000,   1.0000] \n",
            "\tx465: [  0.0000,   1.0000] \n",
            "\tx466: [  0.0000,   1.0000] \n",
            "\tx467: [  0.0000,   1.0000] \n",
            "\tx468: [  0.0000,   1.0000] \n",
            "\tx469: [  0.0000,   1.0000] \n",
            "\tx470: [  0.0000,   1.0000] \n",
            "\tx471: [  0.0000,   1.0000] \n",
            "\tx472: [  0.0000,   1.0000] \n",
            "\tx473: [  0.0000,   1.0000] \n",
            "\tx474: [  0.0000,   1.0000] \n",
            "\tx475: [  0.0000,   1.0000] \n",
            "\tx476: [  0.0000,   1.0000] \n",
            "\tx477: [  0.0000,   1.0000] \n",
            "\tx478: [  0.0000,   1.0000] \n",
            "\tx479: [  0.0000,   1.0000] \n",
            "\tx480: [  0.0000,   1.0000] \n",
            "\tx481: [  0.0000,   1.0000] \n",
            "\tx482: [  0.0000,   1.0000] \n",
            "\tx483: [  0.0000,   1.0000] \n",
            "\tx484: [  0.0000,   1.0000] \n",
            "\tx485: [  0.0000,   1.0000] \n",
            "\tx486: [  0.0000,   1.0000] \n",
            "\tx487: [  0.0000,   1.0000] \n",
            "\tx488: [  0.0000,   1.0000] \n",
            "\tx489: [  0.0000,   1.0000] \n",
            "\tx490: [  0.0000,   1.0000] \n",
            "\tx491: [  0.0000,   1.0000] \n",
            "\tx492: [  0.0000,   1.0000] \n",
            "\tx493: [  0.0000,   1.0000] \n",
            "\tx494: [  0.0000,   1.0000] \n",
            "\tx495: [  0.0000,   1.0000] \n",
            "\tx496: [  0.0000,   1.0000] \n",
            "\tx497: [  0.0000,   1.0000] \n",
            "\tx498: [  0.0000,   1.0000] \n",
            "\tx499: [  0.0000,   1.0000] \n",
            "\tx500: [  0.0000,   1.0000] \n",
            "\tx501: [  0.0000,   1.0000] \n",
            "\tx502: [  0.0000,   1.0000] \n",
            "\tx503: [  0.0000,   1.0000] \n",
            "\tx504: [  0.0000,   1.0000] \n",
            "\tx505: [  0.0000,   1.0000] \n",
            "\tx506: [  0.0000,   1.0000] \n",
            "\tx507: [  0.0000,   1.0000] \n",
            "\tx508: [  0.0000,   1.0000] \n",
            "\tx509: [  0.0000,   1.0000] \n",
            "\tx510: [  0.0000,   1.0000] \n",
            "\tx511: [  0.0000,   1.0000] \n",
            "\tx512: [  0.0000,   1.0000] \n",
            "\tx513: [  0.0000,   1.0000] \n",
            "\tx514: [  0.0000,   1.0000] \n",
            "\tx515: [  0.0000,   1.0000] \n",
            "\tx516: [  0.0000,   1.0000] \n",
            "\tx517: [  0.0000,   1.0000] \n",
            "\tx518: [  0.0000,   1.0000] \n",
            "\tx519: [  0.0000,   1.0000] \n",
            "\tx520: [  0.0000,   1.0000] \n",
            "\tx521: [  0.0000,   1.0000] \n",
            "\tx522: [  0.0000,   1.0000] \n",
            "\tx523: [  0.0000,   1.0000] \n",
            "\tx524: [  0.0000,   1.0000] \n",
            "\tx525: [  0.0000,   1.0000] \n",
            "\tx526: [  0.0000,   1.0000] \n",
            "\tx527: [  0.0000,   1.0000] \n",
            "\tx528: [  0.0000,   1.0000] \n",
            "\tx529: [  0.0000,   1.0000] \n",
            "\tx530: [  0.0000,   1.0000] \n",
            "\tx531: [  0.0000,   1.0000] \n",
            "\tx532: [  0.0000,   1.0000] \n",
            "\tx533: [  0.0000,   1.0000] \n",
            "\tx534: [  0.0000,   1.0000] \n",
            "\tx535: [  0.0000,   1.0000] \n",
            "\tx536: [  0.0000,   1.0000] \n",
            "\tx537: [  0.0000,   1.0000] \n",
            "\tx538: [  0.0000,   1.0000] \n",
            "\tx539: [  0.0000,   1.0000] \n",
            "\tx540: [  0.0000,   1.0000] \n",
            "\tx541: [  0.0000,   1.0000] \n",
            "\tx542: [  0.0000,   1.0000] \n",
            "\tx543: [  0.0000,   1.0000] \n",
            "\tx544: [  0.0000,   1.0000] \n",
            "\tx545: [  0.0000,   1.0000] \n",
            "\tx546: [  0.0000,   1.0000] \n",
            "\tx547: [  0.0000,   1.0000] \n",
            "\tx548: [  0.0000,   1.0000] \n",
            "\tx549: [  0.0000,   1.0000] \n",
            "\tx550: [  0.0000,   1.0000] \n",
            "\tx551: [  0.0000,   1.0000] \n",
            "\tx552: [  0.0000,   1.0000] \n",
            "\tx553: [  0.0000,   1.0000] \n",
            "\tx554: [  0.0000,   1.0000] \n",
            "\tx555: [  0.0000,   1.0000] \n",
            "\tx556: [  0.0000,   1.0000] \n",
            "\tx557: [  0.0000,   1.0000] \n",
            "\tx558: [  0.0000,   1.0000] \n",
            "\tx559: [  0.0000,   1.0000] \n",
            "\tx560: [  0.0000,   1.0000] \n",
            "\tx561: [  0.0000,   1.0000] \n",
            "\tx562: [  0.0000,   1.0000] \n",
            "\tx563: [  0.0000,   1.0000] \n",
            "\tx564: [  0.0000,   1.0000] \n",
            "\tx565: [  0.0000,   1.0000] \n",
            "\tx566: [  0.0000,   1.0000] \n",
            "\tx567: [  0.0000,   1.0000] \n",
            "\tx568: [  0.0000,   1.0000] \n",
            "\tx569: [  0.0000,   1.0000] \n",
            "\tx570: [  0.0000,   1.0000] \n",
            "\tx571: [  0.0000,   1.0000] \n",
            "\tx572: [  0.0000,   1.0000] \n",
            "\tx573: [  0.0000,   1.0000] \n",
            "\tx574: [  0.0000,   1.0000] \n",
            "\tx575: [  0.0000,   1.0000] \n",
            "\tx576: [  0.0000,   1.0000] \n",
            "\tx577: [  0.0000,   1.0000] \n",
            "\tx578: [  0.0000,   1.0000] \n",
            "\tx579: [  0.0000,   1.0000] \n",
            "\tx580: [  0.0000,   1.0000] \n",
            "\tx581: [  0.0000,   1.0000] \n",
            "\tx582: [  0.0000,   1.0000] \n",
            "\tx583: [  0.0000,   1.0000] \n",
            "\tx584: [  0.0000,   1.0000] \n",
            "\tx585: [  0.0000,   1.0000] \n",
            "\tx586: [  0.0000,   1.0000] \n",
            "\tx587: [  0.0000,   1.0000] \n",
            "\tx588: [  0.0000,   1.0000] \n",
            "\tx589: [  0.0000,   1.0000] \n",
            "\tx590: [  0.0000,   1.0000] \n",
            "\tx591: [  0.0000,   1.0000] \n",
            "\tx592: [  0.0000,   1.0000] \n",
            "\tx593: [  0.0000,   1.0000] \n",
            "\tx594: [  0.0000,   1.0000] \n",
            "\tx595: [  0.0000,   1.0000] \n",
            "\tx596: [  0.0000,   1.0000] \n",
            "\tx597: [  0.0000,   1.0000] \n",
            "\tx598: [  0.0000,   1.0000] \n",
            "\tx599: [  0.0000,   1.0000] \n",
            "\tx600: [  0.0000,   1.0000] \n",
            "\tx601: [  0.0000,   1.0000] \n",
            "\tx602: [  0.0000,   1.0000] \n",
            "\tx603: [  0.0000,   1.0000] \n",
            "\tx604: [  0.0000,   1.0000] \n",
            "\tx605: [  0.0000,   1.0000] \n",
            "\tx606: [  0.0000,   1.0000] \n",
            "\tx607: [  0.0000,   1.0000] \n",
            "\tx608: [  0.0000,   1.0000] \n",
            "\tx609: [  0.0000,   1.0000] \n",
            "\tx610: [  0.0000,   1.0000] \n",
            "\tx611: [  0.0000,   1.0000] \n",
            "\tx612: [  0.0000,   1.0000] \n",
            "\tx613: [  0.0000,   1.0000] \n",
            "\tx614: [  0.0000,   1.0000] \n",
            "\tx615: [  0.0000,   1.0000] \n",
            "\tx616: [  0.0000,   1.0000] \n",
            "\tx617: [  0.0000,   1.0000] \n",
            "\tx618: [  0.0000,   1.0000] \n",
            "\tx619: [  0.0000,   1.0000] \n",
            "\tx620: [  0.0000,   1.0000] \n",
            "\tx621: [  0.0000,   1.0000] \n",
            "\tx622: [  0.0000,   1.0000] \n",
            "\tx623: [  0.0000,   1.0000] \n",
            "\tx624: [  0.0000,   1.0000] \n",
            "\tx625: [  0.0000,   1.0000] \n",
            "\tx626: [  0.0000,   1.0000] \n",
            "\tx627: [  0.0000,   1.0000] \n",
            "\tx628: [  0.0000,   1.0000] \n",
            "\tx629: [  0.0000,   1.0000] \n",
            "\tx630: [  0.0000,   1.0000] \n",
            "\tx631: [  0.0000,   1.0000] \n",
            "\tx632: [  0.0000,   1.0000] \n",
            "\tx633: [  0.0000,   1.0000] \n",
            "\tx634: [  0.0000,   1.0000] \n",
            "\tx635: [  0.0000,   1.0000] \n",
            "\tx636: [  0.0000,   1.0000] \n",
            "\tx637: [  0.0000,   1.0000] \n",
            "\tx638: [  0.0000,   1.0000] \n",
            "\tx639: [  0.0000,   1.0000] \n",
            "\tx640: [  0.0000,   1.0000] \n",
            "\tx641: [  0.0000,   1.0000] \n",
            "\tx642: [  0.0000,   1.0000] \n",
            "\tx643: [  0.0000,   1.0000] \n",
            "\tx644: [  0.0000,   1.0000] \n",
            "\tx645: [  0.0000,   1.0000] \n",
            "\tx646: [  0.0000,   1.0000] \n",
            "\tx647: [  0.0000,   1.0000] \n",
            "\tx648: [  0.0000,   1.0000] \n",
            "\tx649: [  0.0000,   1.0000] \n",
            "\tx650: [  0.0000,   1.0000] \n",
            "\tx651: [  0.0000,   1.0000] \n",
            "\tx652: [  0.0000,   1.0000] \n",
            "\tx653: [  0.0000,   1.0000] \n",
            "\tx654: [  0.0000,   1.0000] \n",
            "\tx655: [  0.0000,   1.0000] \n",
            "\tx656: [  0.0000,   1.0000] \n",
            "\tx657: [  0.0000,   1.0000] \n",
            "\tx658: [  0.0000,   1.0000] \n",
            "\tx659: [  0.0000,   1.0000] \n",
            "\tx660: [  0.0000,   1.0000] \n",
            "\tx661: [  0.0000,   1.0000] \n",
            "\tx662: [  0.0000,   1.0000] \n",
            "\tx663: [  0.0000,   1.0000] \n",
            "\tx664: [  0.0000,   1.0000] \n",
            "\tx665: [  0.0000,   1.0000] \n",
            "\tx666: [  0.0000,   1.0000] \n",
            "\tx667: [  0.0000,   1.0000] \n",
            "\tx668: [  0.0000,   1.0000] \n",
            "\tx669: [  0.0000,   1.0000] \n",
            "\tx670: [  0.0000,   1.0000] \n",
            "\tx671: [  0.0000,   1.0000] \n",
            "\tx672: [  0.0000,   1.0000] \n",
            "\tx673: [  0.0000,   1.0000] \n",
            "\tx674: [  0.0000,   1.0000] \n",
            "\tx675: [  0.0000,   1.0000] \n",
            "\tx676: [  0.0000,   1.0000] \n",
            "\tx677: [  0.0000,   1.0000] \n",
            "\tx678: [  0.0000,   1.0000] \n",
            "\tx679: [  0.0000,   1.0000] \n",
            "\tx680: [  0.0000,   1.0000] \n",
            "\tx681: [  0.0000,   1.0000] \n",
            "\tx682: [  0.0000,   1.0000] \n",
            "\tx683: [  0.0000,   1.0000] \n",
            "\tx684: [  0.0000,   1.0000] \n",
            "\tx685: [  0.0000,   1.0000] \n",
            "\tx686: [  0.0000,   1.0000] \n",
            "\tx687: [  0.0000,   1.0000] \n",
            "\tx688: [  0.0000,   1.0000] \n",
            "\tx689: [  0.0000,   1.0000] \n",
            "\tx690: [  0.0000,   1.0000] \n",
            "\tx691: [  0.0000,   1.0000] \n",
            "\tx692: [  0.0000,   1.0000] \n",
            "\tx693: [  0.0000,   1.0000] \n",
            "\tx694: [  0.0000,   1.0000] \n",
            "\tx695: [  0.0000,   1.0000] \n",
            "\tx696: [  0.0000,   1.0000] \n",
            "\tx697: [  0.0000,   1.0000] \n",
            "\tx698: [  0.0000,   1.0000] \n",
            "\tx699: [  0.0000,   1.0000] \n",
            "\tx700: [  0.0000,   1.0000] \n",
            "\tx701: [  0.0000,   1.0000] \n",
            "\tx702: [  0.0000,   1.0000] \n",
            "\tx703: [  0.0000,   1.0000] \n",
            "\tx704: [  0.0000,   1.0000] \n",
            "\tx705: [  0.0000,   1.0000] \n",
            "\tx706: [  0.0000,   1.0000] \n",
            "\tx707: [  0.0000,   1.0000] \n",
            "\tx708: [  0.0000,   1.0000] \n",
            "\tx709: [  0.0000,   1.0000] \n",
            "\tx710: [  0.0000,   1.0000] \n",
            "\tx711: [  0.0000,   1.0000] \n",
            "\tx712: [  0.0000,   1.0000] \n",
            "\tx713: [  0.0000,   1.0000] \n",
            "\tx714: [  0.0000,   1.0000] \n",
            "\tx715: [  0.0000,   1.0000] \n",
            "\tx716: [  0.0000,   1.0000] \n",
            "\tx717: [  0.0000,   1.0000] \n",
            "\tx718: [  0.0000,   1.0000] \n",
            "\tx719: [  0.0000,   1.0000] \n",
            "\tx720: [  0.0000,   1.0000] \n",
            "\tx721: [  0.0000,   1.0000] \n",
            "\tx722: [  0.0000,   1.0000] \n",
            "\tx723: [  0.0000,   1.0000] \n",
            "\tx724: [  0.0000,   1.0000] \n",
            "\tx725: [  0.0000,   1.0000] \n",
            "\tx726: [  0.0000,   1.0000] \n",
            "\tx727: [  0.0000,   1.0000] \n",
            "\tx728: [  0.0000,   1.0000] \n",
            "\tx729: [  0.0000,   1.0000] \n",
            "\tx730: [  0.0000,   1.0000] \n",
            "\tx731: [  0.0000,   1.0000] \n",
            "\tx732: [  0.0000,   1.0000] \n",
            "\tx733: [  0.0000,   1.0000] \n",
            "\tx734: [  0.0000,   1.0000] \n",
            "\tx735: [  0.0000,   1.0000] \n",
            "\tx736: [  0.0000,   1.0000] \n",
            "\tx737: [  0.0000,   1.0000] \n",
            "\tx738: [  0.0000,   1.0000] \n",
            "\tx739: [  0.0000,   1.0000] \n",
            "\tx740: [  0.0000,   1.0000] \n",
            "\tx741: [  0.0000,   1.0000] \n",
            "\tx742: [  0.0000,   1.0000] \n",
            "\tx743: [  0.0000,   1.0000] \n",
            "\tx744: [  0.0000,   1.0000] \n",
            "\tx745: [  0.0000,   1.0000] \n",
            "\tx746: [  0.0000,   1.0000] \n",
            "\tx747: [  0.0000,   1.0000] \n",
            "\tx748: [  0.0000,   1.0000] \n",
            "\tx749: [  0.0000,   1.0000] \n",
            "\tx750: [  0.0000,   1.0000] \n",
            "\tx751: [  0.0000,   1.0000] \n",
            "\tx752: [  0.0000,   1.0000] \n",
            "\tx753: [  0.0000,   1.0000] \n",
            "\tx754: [  0.0000,   1.0000] \n",
            "\tx755: [  0.0000,   1.0000] \n",
            "\tx756: [  0.0000,   1.0000] \n",
            "\tx757: [  0.0000,   1.0000] \n",
            "\tx758: [  0.0000,   1.0000] \n",
            "\tx759: [  0.0000,   1.0000] \n",
            "\tx760: [  0.0000,   1.0000] \n",
            "\tx761: [  0.0000,   1.0000] \n",
            "\tx762: [  0.0000,   1.0000] \n",
            "\tx763: [  0.0000,   1.0000] \n",
            "\tx764: [  0.0000,   1.0000] \n",
            "\tx765: [  0.0000,   1.0000] \n",
            "\tx766: [  0.0000,   1.0000] \n",
            "\tx767: [  0.0000,   1.0000] \n",
            "\tx768: [  0.0000,   1.0000] \n",
            "\tx769: [  0.0000,   1.0000] \n",
            "\tx770: [  0.0000,   1.0000] \n",
            "\tx771: [  0.0000,   1.0000] \n",
            "\tx772: [  0.0000,   1.0000] \n",
            "\tx773: [  0.0000,   1.0000] \n",
            "\tx774: [  0.0000,   1.0000] \n",
            "\tx775: [  0.0000,   1.0000] \n",
            "\tx776: [  0.0000,   1.0000] \n",
            "\tx777: [  0.0000,   1.0000] \n",
            "\tx778: [  0.0000,   1.0000] \n",
            "\tx779: [  0.0000,   1.0000] \n",
            "\tx780: [  0.0000,   1.0000] \n",
            "\tx781: [  0.0000,   1.0000] \n",
            "\tx782: [  0.0000,   1.0000] \n",
            "\tx783: [  0.0000,   1.0000] \n",
            "\n",
            "^C\n",
            "Summary Results from Marabou:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-a3c386602abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#\",label:\", LABEL)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minvariants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_invariants\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdescribe_invariants_all_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvariants\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev_lay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurr_lay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-13c2794104d1>\u001b[0m in \u001b[0;36mdescribe_invariants_all_labels\u001b[0;34m(all_invariants, prevlayer, layer, suffixes, COMMON, DEC_PREFX)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INVOKE MARABOU:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLAYER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m       \u001b[0minvoke_marabou_chk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLAYER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCOMMON\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-e9c6d4ac1a96>\u001b[0m in \u001b[0;36minvoke_marabou_chk\u001b[0;34m(layer, neurons, signature, label)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./marabou1.elf ./mnist_10_layer.nnet ./property.txt --summary-file=summary1.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Summary Results from Marabou:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'summary1.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mfile_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m#print (file_contents)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'summary1.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isr62nPPlROL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Get all fine_grained_estimator invariants\n",
        "#fge_all_invariants = get_all_invariants(fine_grained_estimator)\n",
        "## Print invariant stats\n",
        "#df = describe_all_invariants(fge_all_invariants)\n",
        "#print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "#print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "#print df.to_string(index=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU7SHWilyZjc",
        "colab_type": "text"
      },
      "source": [
        "### Analyzing clusters of misclassified inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GPZ8nMyYCMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Examine the cluster for a misclasification (Groundtruth: 4, Prediction: 49)\n",
        "#invs = fge_all_invariants[49]\n",
        "#neuron_ids, neuron_sig, _ = invs[0]\n",
        "#cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "#describe_cluster(cluster, neuron_ids)\n",
        "\n",
        "## Visualize  10 inputs in the cluster\n",
        "#for i in cluster[:10]:\n",
        "#  describe_input(i)\n",
        " # # show_img(visualize_conductances(mnist.train.images[i], train_predictions[i], neuron_ids, only_on=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-yR4qbD44Qr",
        "colab_type": "text"
      },
      "source": [
        "### Test Accuracy Improvements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBZDfuH42u42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We use the fine_grained_estimator to check if an input belongs\n",
        "## to a pure cluster (i.e., prediction id of the form 10*label + label).\n",
        "## If so, we declare the network's prediction as a \"condident prediction\".\n",
        "## We measure the accuracy of confident_predictions.\n",
        "#fine_grained_estimator_test_predictions = fine_grained_estimator.predict(test_suffixes)\n",
        "#fine_grained_estimator_leaf_nodes = fine_grained_estimator.apply(test_suffixes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HT-Fqrp7Kt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_accuracy_for_label(label):\n",
        "  def get_confidence():\n",
        "    is_confident = (fine_grained_estimator_test_predictions == 10*label + label)\n",
        "    sufficient_samples = fine_grained_estimator.tree_.n_node_samples[fine_grained_estimator_leaf_nodes] >= 10\n",
        "    is_confident *= sufficient_samples\n",
        "    return is_confident\n",
        "  # Following are boolean array. For e.g., with_label[i] is True if\n",
        "  # image i has the given label\n",
        "  with_label = (mnist.test.labels == label)\n",
        "  is_correct = (test_predictions == mnist.test.labels)\n",
        "  with_label_and_correct = with_label*is_correct\n",
        "  is_confident = get_confidence()\n",
        "  with_label_and_correct_and_confident = with_label_and_correct*is_confident\n",
        "  with_label_and_confident = with_label*is_confident\n",
        "\n",
        "  total = np.sum(with_label)\n",
        "  num_conf = np.sum(with_label_and_confident) \n",
        "  num_correct = np.sum(with_label_and_correct)\n",
        "  num_correct_conf = np.sum(with_label_and_correct_and_confident)\n",
        "  return total, num_conf, num_correct, num_correct_conf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1ed8-tT82wW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df = []\n",
        "#grand_total = 0\n",
        "#grand_correct = 0\n",
        "#grand_conf = 0\n",
        "#grand_correct_conf = 0\n",
        "#for i in range(10):\n",
        "#  total, num_conf, num_correct, num_correct_conf = test_accuracy_for_label(i)\n",
        "#  grand_total += total\n",
        "#  grand_conf += num_conf\n",
        "#  grand_correct += num_correct\n",
        "#  grand_correct_conf += num_correct_conf\n",
        "#  acc = 1.0*num_correct/total\n",
        "#  conf_acc = 1.0*num_correct_conf/num_conf\n",
        "#  df.append([i, total, num_conf, acc, conf_acc])\n",
        "#df = pd.DataFrame(df, columns=['Label', 'Instances', 'ConfidentInstances',  'Acc', 'ConfidentAcc',])\n",
        "#display(df)\n",
        "#print \"Total Instances\", grand_total\n",
        "#print \"Num Confident Instances\", grand_conf\n",
        "#print \"Orig Accuracy\", 1.0*grand_correct/grand_total\n",
        "#print \"Confident Accuracy\", 1.0*grand_correct_conf/grand_conf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhKV-FYLv-Eb",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing the Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEZ8ZeuMo_9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!apt-get install graphviz\n",
        "#!pip install graphviz\n",
        "#import graphviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaOsPspkjuOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dot_data = tree.export_graphviz(basic_estimator, out_file=None) \n",
        "#graph = graphviz.Source(dot_data)  \n",
        "#graph "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}